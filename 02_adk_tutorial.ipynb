{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ADK Features\n",
        "\n",
        "- This notebook contains the key features of ADK that can help you build your custom agent.\n",
        "- Credit to lavinigam@"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:red\">**YOU NEED TO UPDATE YOUR PROJECT_ID AND LOCATION**</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:red\">**Verify google-adk version is 0.5.0**</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip show google-adk | grep Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Temp - to supress WARNING:google_genai.types:\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='google.generativeai.types.content_types') # Suppress harmless warning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H9y2tcqvOVWt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"hello-world-418507\"\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
        "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\" # Use Vertex AI API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73qeFgix6d0x"
      },
      "source": [
        "### LLM Agent with a Single Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp2U_FdrIl_d",
        "outputId": "f7c78468-2a5f-46ca-9bbf-3fcafcb92aec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  The capital of France is Paris.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import LlmAgent\n",
        "from google.genai import types\n",
        "from pydantic import BaseModel\n",
        "from google.adk.agents import Agent\n",
        "from google.genai import types\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"capital_city_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"capital_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-exp\"\n",
        "\n",
        "\n",
        "# Define a simple tool\n",
        "def get_capital_city(country: str) -> str:\n",
        "    \"\"\"Retrieves the capital city of a given country.\n",
        "\n",
        "    Args:\n",
        "        country: The name of the country.\n",
        "\n",
        "    Returns:\n",
        "        The capital city of the country.\n",
        "    \"\"\"\n",
        "    country_capitals = {\n",
        "        \"united states\": \"washington, d.c.\",\n",
        "        \"canada\": \"ottawa\",\n",
        "        \"france\": \"paris\",\n",
        "    }\n",
        "    return country_capitals.get(country.lower(), \"Capital not found\")\n",
        "\n",
        "\n",
        "# Agent\n",
        "capital_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=\"capital_agent\",\n",
        "    description=\"An agent that can retrieve the capital city of a country.\",\n",
        "    instruction=\"\"\"You are an agent that can retrieve the capital city of a country.\n",
        "    When a user provides a prompt, extract the country name.\n",
        "    Then, use the `get_capital_city` tool to retrieve the capital city for that country.\n",
        "    Finally, present the capital city to the user in a clear and concise manner.\n",
        "    \"\"\",\n",
        "    tools=[get_capital_city],\n",
        "    generate_content_config=types.GenerateContentConfig(\n",
        "        max_output_tokens=100,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=capital_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "  for event in events:\n",
        "      if event.is_final_response():\n",
        "          final_response = event.content.parts[0].text\n",
        "          print(\"Agent Response: \", final_response)\n",
        "\n",
        "call_agent(\"What is the capital of france?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SugbbRPE6QYL"
      },
      "source": [
        "### LLM Agent with a Input/Output Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K0gMttqXgn3",
        "outputId": "a9534934-f503-4477-cafc-0a5150b4a7ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  {\n",
            "  \"capital\": \"Paris\"\n",
            "}\n",
            "Agent Response:  {\n",
            "  \"capital\": \"Berlin\"\n",
            "}\n",
            "Agent Response:  {\n",
            "  \"capital\": \"Tokyo\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "from pydantic import Field\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.genai import types\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"capital_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"capital_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "\n",
        "class InputSchema(BaseModel):\n",
        "    country: str = Field(description=\"The country to find the capital of.\")\n",
        "\n",
        "\n",
        "class OutputSchema(BaseModel):\n",
        "    capital: str = Field(description=\"The capital of the country.\")\n",
        "\n",
        "\n",
        "# Agent\n",
        "capital_agent = Agent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Capital Information Agent. Your task is to provide the capital of a given country.\n",
        "\n",
        "    When a user provides a prompt, extract the country name.\n",
        "    Then, respond with the capital of that country in the following JSON format:\n",
        "\n",
        "    \"\"\",\n",
        "    description=\"\"\"You are an agent who can tell the capital of a country.\"\"\",\n",
        "    disallow_transfer_to_peers=True,\n",
        "    disallow_transfer_to_parent=True,\n",
        "    input_schema=InputSchema,\n",
        "    output_schema=OutputSchema,\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(\n",
        "    agent=capital_agent, app_name=APP_NAME, session_service=session_service\n",
        ")\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run(\n",
        "        user_id=USER_ID, session_id=SESSION_ID, new_message=content\n",
        "    )\n",
        "\n",
        "    for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n",
        "\n",
        "call_agent('{\"country\": \"France\"}')\n",
        "call_agent('{\"country\": \"Germany\"}')\n",
        "call_agent('{\"country\": \"Japan\"}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXOPByf86GYm"
      },
      "source": [
        "### LLM Agent with a output_key\n",
        "\n",
        "- You set `output_key` to store the key value pair into session state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2mV97Ctf_jF",
        "outputId": "4baa9e80-4754-4c97-9809-809661d471ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Paris is the capital of France.\n",
            "\n",
            "Session State: Paris is the capital of France.\n",
            "\n",
            "Agent Response:  Berlin is the capital of Germany.\n",
            "\n",
            "Session State: Berlin is the capital of Germany.\n",
            "\n",
            "Agent Response:  Tokyo is the capital of Japan.\n",
            "\n",
            "Session State: Tokyo is the capital of Japan.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "import json\n",
        "\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"capital_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"capital_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# Agent\n",
        "capital_agent = Agent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Capital Information Agent. Your task is to provide the capital of a given country.\n",
        "\n",
        "    When a user provides a prompt, extract the country name.\n",
        "\n",
        "    \"\"\",\n",
        "    description=\"\"\"You are an agent who can tell the capital of a country.\"\"\",\n",
        "    output_key=\"capital_output\",\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(\n",
        "    agent=capital_agent, app_name=APP_NAME, session_service=session_service\n",
        ")\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run(\n",
        "        user_id=USER_ID, session_id=SESSION_ID, new_message=content\n",
        "    )\n",
        "\n",
        "    for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n",
        "            print(\n",
        "                \"Session State:\",\n",
        "                session_service.get_session(\n",
        "                    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        "                ).state.get(\"capital_output\"),\n",
        "            )\n",
        "\n",
        "\n",
        "call_agent(\"What's the capital of France?\")\n",
        "call_agent(\"What's the capital of Germany?\")\n",
        "call_agent(\"What's the capital of Japan?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCUGzcWD59A3"
      },
      "source": [
        "### LLM Agent with a built_in_code_execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVXqH7O5jDTa",
        "outputId": "6bc8f1e9-dcae-4a96-d2fe-fec8c4909c76"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['executable_code', 'code_execution_result'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Agent Response ---\n",
            "111 + 222 = 333\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "import json\n",
        "from google.adk.tools import built_in_code_execution\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"capital_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"capital_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "code_agent = LlmAgent(\n",
        "    name=\"code_execution_agent\",\n",
        "    model=GEMINI_2_FLASH,\n",
        "    tools=[built_in_code_execution],\n",
        "    instruction=\"Generate python code to solve the user's request. \"\n",
        "    \"If the user asks for a specific output, return the output of the code execution.\",\n",
        ")\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(agent=code_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "    for event in events:\n",
        "        if event.content and event.content.parts:\n",
        "            for part in event.content.parts:\n",
        "                if part.text:\n",
        "                    print(f\"\\n--- Agent Response ---\\n{part.text}\\n\")\n",
        "        else:\n",
        "            print(f\"Event: {event}\")\n",
        "\n",
        "call_agent(\"what is 111 + 222\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E17fqZuaH1BM"
      },
      "source": [
        "### LLM Agent with a Multiple Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtFi_2ANHUN4",
        "outputId": "f4e8707c-d3a6-434f-e5e4-4de38e78adc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  I found one recipe with chicken: chicken tikka masala.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  No, pasta carbonara is not suitable for a vegan diet.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  The ingredients for vegan lentil soup are: lentils, carrots, celery, onion, vegetable broth.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "\n",
        "# Constants\n",
        "APP_NAME = \"recipe_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"recipe_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# --- Mock Data ---\n",
        "recipes = {\n",
        "    \"pasta carbonara\": {\n",
        "        \"ingredients\": [\"pasta\", \"eggs\", \"guanciale\", \"pecorino romano\", \"black pepper\"],\n",
        "        \"dietary_restrictions\": [\"none\"],\n",
        "    },\n",
        "    \"chicken tikka masala\": {\n",
        "        \"ingredients\": [\"chicken\", \"yogurt\", \"ginger\", \"garlic\", \"masala blend\"],\n",
        "        \"dietary_restrictions\": [\"none\"],\n",
        "    },\n",
        "    \"vegan lentil soup\": {\n",
        "        \"ingredients\": [\"lentils\", \"carrots\", \"celery\", \"onion\", \"vegetable broth\"],\n",
        "        \"dietary_restrictions\": [\"vegan\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "# --- Tools ---\n",
        "def search_recipes(keyword: str) -> str:\n",
        "    \"\"\"Searches for recipes based on a keyword.\n",
        "\n",
        "    Args:\n",
        "        keyword: The keyword to search for (e.g., ingredient).\n",
        "\n",
        "    Returns:\n",
        "        A string containing the names of matching recipes, or a message if no recipes are found.\n",
        "    \"\"\"\n",
        "    matching_recipes = [\n",
        "        recipe_name\n",
        "        for recipe_name, recipe_data in recipes.items()\n",
        "        if keyword.lower() in recipe_name.lower() or keyword.lower() in recipe_data[\"ingredients\"]\n",
        "    ]\n",
        "    if matching_recipes:\n",
        "        return f\"Recipes matching '{keyword}': {', '.join(matching_recipes)}.\"\n",
        "    else:\n",
        "        return f\"No recipes found matching '{keyword}'.\"\n",
        "\n",
        "\n",
        "def check_dietary_restrictions(recipe_name: str, dietary_restriction: str) -> str:\n",
        "    \"\"\"Checks if a recipe is suitable for a given dietary restriction.\n",
        "\n",
        "    Args:\n",
        "        recipe_name: The name of the recipe to check.\n",
        "        dietary_restriction: The dietary restriction to check for (e.g., \"vegan\").\n",
        "\n",
        "    Returns:\n",
        "        A string indicating if the recipe is suitable or not.\n",
        "    \"\"\"\n",
        "    recipe_data = recipes.get(recipe_name.lower())\n",
        "    if recipe_data:\n",
        "        if dietary_restriction.lower() in recipe_data[\"dietary_restrictions\"]:\n",
        "            return f\"'{recipe_name}' is suitable for a '{dietary_restriction}' diet.\"\n",
        "        else:\n",
        "            return f\"'{recipe_name}' is not suitable for a '{dietary_restriction}' diet.\"\n",
        "    else:\n",
        "        return f\"Recipe '{recipe_name}' not found.\"\n",
        "\n",
        "\n",
        "def get_ingredient_list(recipe_name: str) -> str:\n",
        "    \"\"\"Returns a list of ingredients for a given recipe.\n",
        "\n",
        "    Args:\n",
        "        recipe_name: The name of the recipe.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the list of ingredients, or a message if the recipe is not found.\n",
        "    \"\"\"\n",
        "    recipe_data = recipes.get(recipe_name.lower())\n",
        "    if recipe_data:\n",
        "        return f\"Ingredients for '{recipe_name}': {', '.join(recipe_data['ingredients'])}.\"\n",
        "    else:\n",
        "        return f\"Recipe '{recipe_name}' not found.\"\n",
        "\n",
        "\n",
        "# --- Agent ---\n",
        "recipe_agent = Agent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Recipe Agent. Your task is to help users find recipes and check their suitability for dietary restrictions.\n",
        "\n",
        "    You have access to three tools:\n",
        "    1. `search_recipes`: Use this tool to find recipes based on a keyword (e.g., ingredient).\n",
        "    2. `check_dietary_restrictions`: Use this tool to check if a recipe is suitable for a given dietary restriction.\n",
        "    3. `get_ingredient_list`: Use this tool to get a list of ingredients for a given recipe.\n",
        "\n",
        "    When a user provides a prompt, first determine what they are asking for.\n",
        "    - If they are asking for recipes based on a keyword, use the `search_recipes` tool.\n",
        "    - If they are asking if a recipe is suitable for a dietary restriction, use the `check_dietary_restrictions` tool.\n",
        "    - If they are asking for a list of ingredients for a recipe, use the `get_ingredient_list` tool.\n",
        "    Finally, present the information to the user in a clear and concise manner.\n",
        "    \"\"\",\n",
        "    description=\"\"\"An agent that can find recipes, check dietary restrictions, and list ingredients.\n",
        "    It has access to the `search_recipes`, `check_dietary_restrictions`, and `get_ingredient_list` tools.\"\"\",\n",
        "    tools=[search_recipes, check_dietary_restrictions, get_ingredient_list,],\n",
        ")\n",
        "\n",
        "# --- Session and Runner ---\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=recipe_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# --- Agent Interaction ---\n",
        "def call_agent(query):\n",
        "    content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "    for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n",
        "\n",
        "\n",
        "call_agent(\"Find recipes with chicken.\")\n",
        "call_agent(\"Is pasta carbonara suitable for a vegan diet?\")\n",
        "call_agent(\"What are the ingredients in vegan lentil soup?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G6tlJhHR3FM"
      },
      "source": [
        "### LLM Agent with a Async Run\n",
        "\n",
        "- It is as simple as using `runner.run_async` instead of `runner.run`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urYEdu2MRvWg",
        "outputId": "ab6398ba-dd9f-4a31-f63b-fffe2a830aa1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  OK. The weather in Toronto is 30 degrees Celsius, partly cloudy with an overcast sky.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"weather_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"weather_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# Tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Retrieves weather information for the given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city for which to retrieve weather information.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the weather information for the specified city,\n",
        "        or a message indicating that the weather information was not found.\n",
        "    \"\"\"\n",
        "    cities = {\n",
        "        'chicago': {'temperature': 25, 'condition': 'sunny', 'sky': 'clear'},\n",
        "        'toronto': {'temperature': 30, 'condition': 'partly cloudy', 'sky': 'overcast'},\n",
        "        'chennai': {'temperature': 15, 'condition': 'rainy', 'sky': 'cloudy'},  # Fixed typo: 'tempeerature' to 'temperature'\n",
        "    }\n",
        "\n",
        "    city_lower = city.lower()\n",
        "    if city_lower in cities:\n",
        "        weather_data = cities[city_lower]\n",
        "        return f\"Weather in {city} is {weather_data['temperature']} degrees Celsius, {weather_data['condition']} with a {weather_data['sky']} sky.\"\n",
        "    else:\n",
        "        return f\"Weather information for {city} not found.\"\n",
        "\n",
        "# Agent\n",
        "root_agent = Agent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Weather Information Agent. Your task is to provide weather information for a given city.\n",
        "\n",
        "    When a user provides a prompt, extract the city name.\n",
        "    Then, use the `get_weather` tool to retrieve the weather information for that city.\n",
        "    Finally, present the weather information to the user in a clear and concise manner.\"\"\",\n",
        "    description=\"\"\"You are an agent who can fetch weather information for a city.\n",
        "    You have access to the `get_weather` tool to accomplish this task.\"\"\",\n",
        "    tools=[get_weather],\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=root_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "async def call_agent_async(query):\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
        "      if event.is_final_response():\n",
        "          final_response = event.content.parts[0].text\n",
        "          print(\"Agent Response: \", final_response)\n",
        "\n",
        "await call_agent_async(\"What's the weather in toronto?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbq5jAVSVCUM"
      },
      "source": [
        "### LLM Agent with single sub-agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "U9RE0gb0TFkg"
      },
      "outputs": [],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "import asyncio\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"weather_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"weather_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "\n",
        "# Tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Retrieves weather information for the given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city for which to retrieve weather information.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the weather information for the specified city,\n",
        "        or a message indicating that the weather information was not found.\n",
        "    \"\"\"\n",
        "    cities = {\n",
        "        'chicago': {'temperature': 25, 'condition': 'sunny', 'sky': 'clear'},\n",
        "        'toronto': {'temperature': 30, 'condition': 'partly cloudy', 'sky': 'overcast'},\n",
        "        'chennai': {'temperature': 15, 'condition': 'rainy', 'sky': 'cloudy'},\n",
        "    }\n",
        "\n",
        "    city_lower = city.lower()\n",
        "    if city_lower in cities:\n",
        "        weather_data = cities[city_lower]\n",
        "        return f\"Weather in {city} is {weather_data['temperature']} degrees Celsius, {weather_data['condition']} with a {weather_data['sky']} sky.\"\n",
        "    else:\n",
        "        return f\"Weather information for {city} not found.\"\n",
        "\n",
        "\n",
        "def get_greeting(name: str) -> str:\n",
        "    \"\"\"Greets the given name.\n",
        "\n",
        "    Args:\n",
        "        name: The name to greet.\n",
        "\n",
        "    Returns:\n",
        "        A greeting message.\n",
        "    \"\"\"\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "\n",
        "# Agent\n",
        "root_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=\"root_agent\",\n",
        "    instruction=\"\"\"You are a helpful agent with tool and sub-agents.\n",
        "    - When user ask about weather, extract the city name, then use the `get_weather` tool to retrieve the weather information for that city.\n",
        "    - If the user asks for a greeting, transfer to the greeting_agent.\"\"\",\n",
        "    description=\"\"\"You are an agent who can fetch weather information for a city, and also greet a user.\"\"\",\n",
        "    tools=[get_weather],\n",
        "    # allow_transfer=True,\n",
        ")\n",
        "\n",
        "greeting_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=\"greeting_agent\",\n",
        "    instruction=\"\"\"You are a Greeting Agent. Your task is to greet the user.\n",
        "\n",
        "    When a user provides a prompt, extract the name.\n",
        "    Then, use the `get_greeting` tool to greet the user.\n",
        "    Finally, present the greeting to the user in a clear and concise manner.\n",
        "    If the user asks for weather information, transfer to the weather agent.\"\"\",\n",
        "    description=\"\"\"You are an agent who can greet a user.\n",
        "    You have access to the `get_greeting` tool to accomplish this task.\"\"\",\n",
        "    tools=[get_greeting],\n",
        "    disallow_transfer_to_parent=True,\n",
        "    disallow_transfer_to_peers=True,\n",
        ")\n",
        "\n",
        "# Set parent-child relationship\n",
        "root_agent.sub_agents = [greeting_agent]\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=root_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "async def call_agent_async(query):\n",
        "    content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "    async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_70TiFhAfP3Z",
        "outputId": "9cfeb22e-a07d-4bf0-91e0-182a662b6699"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  OK. The weather in toronto is 30 degrees Celsius, partly cloudy with a overcast sky.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "await call_agent_async(\"What's the weather in toronto?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7YvXK5afP0C",
        "outputId": "0411cab1-c4fe-40ea-a5c2-e49b17758513"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n",
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Hello, Jimmy!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "await call_agent_async(\"Hello, Jimmy!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  OK. The weather in chennai is 15 degrees Celsius, rainy with a cloudy sky.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "await call_agent_async(\"What's the weather in chennai?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUiHdzpATx2H"
      },
      "source": [
        "### Weather Agent with multiple sub-agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rOIYc49jTyHj"
      },
      "outputs": [],
      "source": [
        "# Tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Retrieves weather information for the given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city for which to retrieve weather information.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the weather information for the specified city,\n",
        "        or a message indicating that the weather information was not found.\n",
        "    \"\"\"\n",
        "    cities = {\n",
        "        'chicago': {'temperature': 25, 'condition': 'sunny', 'sky': 'clear'},\n",
        "        'toronto': {'temperature': 30, 'condition': 'partly cloudy', 'sky': 'overcast'},\n",
        "        'chennai': {'temperature': 15, 'condition': 'rainy', 'sky': 'cloudy'},\n",
        "    }\n",
        "\n",
        "    city_lower = city.lower()\n",
        "    if city_lower in cities:\n",
        "        weather_data = cities[city_lower]\n",
        "        return f\"Weather in {city} is {weather_data['temperature']} degrees Celsius, {weather_data['condition']} with a {weather_data['sky']} sky.\"\n",
        "    else:\n",
        "        return f\"Weather information for {city} not found.\"\n",
        "\n",
        "\n",
        "# Tool for the Greeting Agent\n",
        "def say_hello(name: str = \"there\") -> str:\n",
        "    \"\"\"Provides a simple greeting.\"\"\"\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "# Tool for the Farewell Agent\n",
        "def say_goodbye() -> str:\n",
        "    \"\"\"Provides a simple farewell message.\"\"\"\n",
        "    return \"Goodbye! Have a great day.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX-ZKdrgT09N",
        "outputId": "c8602e47-5e64-4a79-8e72-1dc276face1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defined parent agent: weather_agent\n",
            "Defined weather_agent (parent), greeting_agent (child), and farewell_agent (child).\n",
            "weather_agent children: ['greeting_agent', 'farewell_agent']\n"
          ]
        }
      ],
      "source": [
        "# --- Import LlmAgent ---\n",
        "from google.adk.agents import LlmAgent\n",
        "import google.genai.types as types # For Content/Part later\n",
        "\n",
        "# --- Agent Definitions ---\n",
        "AGENT_NAME_WEATHER = \"weather_agent\"\n",
        "AGENT_NAME_GREETING = \"greeting_agent\"\n",
        "AGENT_NAME_FAREWELL = \"farewell_agent\"\n",
        "MODEL_NAME = \"gemini-2.0-flash-001\" # Use a recent flash model\n",
        "\n",
        "# --- Parent Agent: Weather ---\n",
        "root_agent = LlmAgent(\n",
        "    model=MODEL_NAME,\n",
        "    name=AGENT_NAME_WEATHER,\n",
        "    instruction=f\"\"\"You are the main Weather Agent in charge. Your primary responsibility is providing weather information.\n",
        "    - **IF** the user asks specifically about the weather (e.g., 'weather in city', 'forecast'), use the 'get_weather' tool YOURSELF. **DO NOT transfer weather requests.**\n",
        "    - **ONLY IF** the user gives a simple greeting (like 'Hi', 'Hello') with NO other request, transfer to the '{AGENT_NAME_GREETING}'.\n",
        "    - **ONLY IF** the user explicitly says goodbye (like 'Bye', 'See you'), transfer to the '{AGENT_NAME_FAREWELL}'.\n",
        "    - Handle only weather requests directly.\n",
        "    \"\"\",\n",
        "    description=\"Provides weather forecasts using 'get_weather'. Delegates greetings/farewells.\",\n",
        "    tools=[get_weather],\n",
        ")\n",
        "print(f\"Defined parent agent: {root_agent.name}\")\n",
        "\n",
        "\n",
        "# --- Child Agent 1: Greeting ---\n",
        "greeting_agent = LlmAgent(\n",
        "    model=MODEL_NAME,\n",
        "    name=AGENT_NAME_GREETING,\n",
        "    instruction=\"You are the Greeting Agent. Use the 'say_hello' tool to greet the user. Do nothing else.\",\n",
        "    description=\"Handles simple greetings using the 'say_hello' tool.\",\n",
        "    tools=[say_hello],\n",
        "    disallow_transfer_to_parent=True,\n",
        "    disallow_transfer_to_peers=True,\n",
        ")\n",
        "\n",
        "# --- Child Agent 2: Farewell ---\n",
        "farewell_agent = LlmAgent(\n",
        "    model=MODEL_NAME,\n",
        "    name=AGENT_NAME_FAREWELL,\n",
        "    instruction=\"You are the Farewell Agent. Use the 'say_goodbye' tool when the user indicates they are leaving. Do nothing else.\",\n",
        "    description=\"Handles simple farewells using the 'say_goodbye' tool.\",\n",
        "    tools=[say_goodbye],\n",
        "    disallow_transfer_to_parent=True,\n",
        "    disallow_transfer_to_peers=True,\n",
        ")\n",
        "\n",
        "# --- Define the Parent-Child Relationship ---\n",
        "# This tells the framework how the agents are structured.\n",
        "root_agent.sub_agents = [greeting_agent, farewell_agent]\n",
        "# The framework automatically sets the .parent_agent attribute on the children\n",
        "\n",
        "print(\"Defined weather_agent (parent), greeting_agent (child), and farewell_agent (child).\")\n",
        "print(f\"{root_agent.name} children: {[child.name for child in root_agent.sub_agents]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PkWevBdBaK_G"
      },
      "outputs": [],
      "source": [
        "from google.adk.sessions import InMemorySessionService\n",
        "import uuid\n",
        "from google.adk.runners import Runner\n",
        "\n",
        "session_service = InMemorySessionService()\n",
        "\n",
        "# Required. Unique identifier for the application.\n",
        "APP_NAME = \"weather_app\"\n",
        "# Required. Identifier for the user interacting with the agent. This is a dynamic variable.\n",
        "USER_ID = \"12345\"\n",
        "\n",
        "SESSION_ID = f\"session_{uuid.uuid4()}\" # Use a dynamic session ID\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "\n",
        "\n",
        "runner = Runner(\n",
        "    agent=root_agent,\n",
        "    app_name=APP_NAME,\n",
        "    session_service=session_service,\n",
        ")\n",
        "\n",
        "def call_agent(user_query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=user_query)])\n",
        "    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "    for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrFV75DHaK3G",
        "outputId": "d396c486-668a-46e8-8276-a7d1d3c4ed0f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Weather in Chennai is 15 degrees Celsius, rainy with a cloudy sky.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "call_agent(user_query = \"What's the weather like in Chennai?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIQWRq2qbPMk",
        "outputId": "b48e3fee-aa65-4764-f7b7-91e38e55163f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Weather in toronto is 30 degrees Celsius, partly cloudy with a overcast sky.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "call_agent(user_query = \"What about the weather in toronto?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-OvXlb4bYYT",
        "outputId": "db985692-3d37-4ba7-9ca8-78606603d274"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n",
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Goodbye! Have a great day.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "call_agent(user_query = \"Okay, thanks, bye!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n",
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Hello, my friend!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "call_agent(user_query = \"Hi, my friend\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDl005Qnog6j"
      },
      "source": [
        "### LLM Agents with Callbacks (Agent, Model & Tool)\n",
        "\n",
        "- This is useful when you want to inspect the messege exchange agents\n",
        "- This cell shows the available functio parameters that you can use in callback\n",
        "\n",
        "\n",
        "This is how the sequence looks like:\n",
        "- Before Agent Callback > Before Model Callback > After Model Callback > Before Tool Callback > After Tool Callback > Before Model Callback > After Model Callback > After Agent Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wPVFcZ84sfLe"
      },
      "outputs": [],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "import asyncio\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"weather_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"weather_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "\n",
        "# Tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Retrieves weather information for the given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city for which to retrieve weather information.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the weather information for the specified city,\n",
        "        or a message indicating that the weather information was not found.\n",
        "    \"\"\"\n",
        "    cities = {\n",
        "        'chicago': {'temperature': 25, 'condition': 'sunny', 'sky': 'clear'},\n",
        "        'toronto': {'temperature': 30, 'condition': 'partly cloudy', 'sky': 'overcast'},\n",
        "        'chennai': {'temperature': 15, 'condition': 'rainy', 'sky': 'cloudy'},\n",
        "    }\n",
        "\n",
        "    city_lower = city.lower()\n",
        "    if city_lower in cities:\n",
        "        weather_data = cities[city_lower]\n",
        "        return f\"Weather in {city} is {weather_data['temperature']} degrees Celsius, {weather_data['condition']} with a {weather_data['sky']} sky.\"\n",
        "    else:\n",
        "        return f\"Weather information for {city} not found.\"\n",
        "\n",
        "\n",
        "def get_greeting(name: str) -> str:\n",
        "    \"\"\"Greets the given name.\n",
        "\n",
        "    Args:\n",
        "        name: The name to greet.\n",
        "\n",
        "    Returns:\n",
        "        A greeting message.\n",
        "    \"\"\"\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "\n",
        "# Callbacks\n",
        "\n",
        "def before_model_callback(callback_context, llm_request):\n",
        "    print(f\"Before Model Callback: Agent {callback_context._invocation_context.agent.name}, Request: {llm_request.contents}\")\n",
        "    return None\n",
        "\n",
        "def after_model_callback(callback_context, llm_response):\n",
        "    print(f\"After Model Callback: Agent {callback_context._invocation_context.agent.name}, Response: {llm_response.content}\")\n",
        "    return None\n",
        "\n",
        "def before_tool_callback(tool, args, tool_context):\n",
        "    print(f\"Before Tool Callback: Tool {tool.name}, Args: {args}\")\n",
        "    return None\n",
        "\n",
        "def after_tool_callback(tool, args, tool_context, tool_response):\n",
        "    print(f\"After Tool Callback: Tool {tool.name}, Response: {tool_response}\")\n",
        "    return None\n",
        "\n",
        "def before_agent_callback(callback_context):\n",
        "    print(f\"Before Agent Callback: Agent {callback_context._invocation_context.agent.name}\")\n",
        "    return None\n",
        "\n",
        "def after_agent_callback(callback_context):\n",
        "    print(f\"After Agent Callback: Agent {callback_context._invocation_context.agent.name}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "root_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=\"weather_agent\",\n",
        "    instruction=\"\"\"You are a Weather Information Agent. Your task is to provide weather information for a given city.\n",
        "\n",
        "    When a user provides a prompt, extract the city name.\n",
        "    Then, use the `get_weather` tool to retrieve the weather information for that city.\n",
        "    Finally, present the weather information to the user in a clear and concise manner.\n",
        "    If the user asks for a greeting, transfer to the greeting agent.\"\"\",\n",
        "    description=\"\"\"You are an agent who can fetch weather information for a city.\n",
        "    You have access to the `get_weather` tool to accomplish this task.\"\"\",\n",
        "    tools=[get_weather],\n",
        "    before_model_callback=before_model_callback,\n",
        "    after_model_callback=after_model_callback,\n",
        "    before_tool_callback=before_tool_callback,\n",
        "    after_tool_callback=after_tool_callback,\n",
        "    before_agent_callback=before_agent_callback,\n",
        "    after_agent_callback=after_agent_callback,\n",
        ")\n",
        "\n",
        "greeting_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=\"greeting_agent\",\n",
        "    instruction=\"\"\"You are a Greeting Agent. Your task is to greet the user.\n",
        "\n",
        "    When a user provides a prompt, extract the name.\n",
        "    Then, use the `get_greeting` tool to greet the user.\n",
        "    Finally, present the greeting to the user in a clear and concise manner.\n",
        "    If the user asks for weather information, transfer to the weather agent.\"\"\",\n",
        "    description=\"\"\"You are an agent who can greet a user.\n",
        "    You have access to the `get_greeting` tool to accomplish this task.\"\"\",\n",
        "    tools=[get_greeting],\n",
        "    disallow_transfer_to_parent=True,\n",
        "    disallow_transfer_to_peers=True,\n",
        "    before_model_callback=before_model_callback,\n",
        "    after_model_callback=after_model_callback,\n",
        "    before_tool_callback=before_tool_callback,\n",
        "    after_tool_callback=after_tool_callback,\n",
        "    before_agent_callback=before_agent_callback,\n",
        "    after_agent_callback=after_agent_callback,\n",
        ")\n",
        "\n",
        "# Set parent-child relationship\n",
        "root_agent.sub_agents = [greeting_agent]\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=root_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "async def call_agent_async(query):\n",
        "    content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "    async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUPvRHL6k4nx",
        "outputId": "d409e78d-e0e6-4577-ff28-31232b200773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before Agent Callback: Agent weather_agent\n",
            "Before Model Callback: Agent weather_agent, Request: [Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"What's the weather in Chicago?\")], role='user')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After Model Callback: Agent weather_agent, Response: parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'city': 'Chicago'}, name='get_weather'), function_response=None, inline_data=None, text=None)] role='model'\n",
            "Before Tool Callback: Tool get_weather, Args: {'city': 'Chicago'}\n",
            "After Tool Callback: Tool get_weather, Response: Weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\n",
            "Before Model Callback: Agent weather_agent, Request: [Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"What's the weather in Chicago?\")], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'city': 'Chicago'}, name='get_weather'), function_response=None, inline_data=None, text=None)], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='get_weather', response={'result': 'Weather in Chicago is 25 degrees Celsius, sunny with a clear sky.'}), inline_data=None, text=None)], role='user')]\n",
            "After Model Callback: Agent weather_agent, Response: parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\\n')] role='model'\n",
            "Agent Response:  The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\n",
            "\n",
            "After Agent Callback: Agent weather_agent\n"
          ]
        }
      ],
      "source": [
        "await call_agent_async(\"What's the weather in Chicago?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLmmHTi3ChKE"
      },
      "source": [
        "### LLM Agent with before_agent_callback and state\n",
        "\n",
        "- In this example, we check the session state `skip_agent`, and decide how the agent will response.\n",
        "- If condition is met, we skip LLM response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2X8Fh4BChmN",
        "outputId": "f7d3487e-96d9-4d73-9d08-a9e2f49ea407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running LLM Agent Normally ---\n",
            "[Callback] Entering agent: SimpleLlmAgent (Invocation: e-a8773a31-bf88-4266-bd0a-3592df502551)\n",
            "[Callback] Condition not met: Proceeding with agent SimpleLlmAgent.\n",
            "Event Output: SimpleLlmAgent: Hello!\n",
            "\n",
            "--- Running LLM Agent with Skip Condition ---\n",
            "[Callback] Entering agent: SimpleLlmAgent (Invocation: e-1b029b21-8886-4a5f-9b63-26493a71adb4)\n",
            "[Callback] Condition met: Skipping agent SimpleLlmAgent.\n",
            "Event Output: SimpleLlmAgent: Agent SimpleLlmAgent was skipped by callback.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\" # Or your preferred model\n",
        "\n",
        "# --- Define the Callback Function (Same as before) ---\n",
        "def simple_before_agent_logger(callback_context: CallbackContext) -> Optional[types.Content]:\n",
        "    \"\"\"Logs entry into an agent and checks a condition.\"\"\"\n",
        "    agent_name = callback_context.agent_name\n",
        "    invocation_id = callback_context.invocation_id\n",
        "    print(f\"[Callback] Entering agent: {agent_name} (Invocation: {invocation_id})\")\n",
        "\n",
        "    # Example: Check a condition in state\n",
        "    if callback_context.state.get(\"skip_agent\", False):\n",
        "        print(f\"[Callback] Condition met: Skipping agent {agent_name}.\")\n",
        "        # Return Content to skip the agent's run\n",
        "        return types.Content(parts=[types.Part(text=f\"Agent {agent_name} was skipped by callback.\")])\n",
        "    else:\n",
        "        print(f\"[Callback] Condition not met: Proceeding with agent {agent_name}.\")\n",
        "        # Return None to allow the agent's run to execute\n",
        "        return None\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent and Assign Callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"SimpleLlmAgent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are a simple agent. Just say 'Hello!'\",\n",
        "        description=\"An LLM agent demonstrating before_agent_callback\",\n",
        "        before_agent_callback=simple_before_agent_logger\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(agent=my_llm_agent, app_name=\"llm_demo_app\", session_service=session_service)\n",
        "    session_id_run = \"llm_session_run_1\"\n",
        "    session_id_skip = \"llm_session_skip_1\"\n",
        "    user_id = \"llm_test_user\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(app_name=\"llm_demo_app\", user_id=user_id, session_id=session_id_run)\n",
        "    session_service.create_session(app_name=\"llm_demo_app\", user_id=user_id, session_id=session_id_skip,\n",
        "                                  state={\"skip_agent\": True}) # Set state to trigger skip condition\n",
        "\n",
        "    print(\"--- Running LLM Agent Normally ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_run,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"Run normally\")])):\n",
        "        # Only print final LLM response or callback override\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\") # Added strip() for cleaner output\n",
        "\n",
        "    print(\"\\n--- Running LLM Agent with Skip Condition ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_skip, new_message=types.Content(parts=[types.Part(text=\"Skip this agent\")])):\n",
        "         # Only print final LLM response or callback override\n",
        "         if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhpRxvkuFoIt"
      },
      "source": [
        "### LLM Agent with after_agent_callback and state\n",
        "\n",
        "- In this use case, we check the `add_concluding_note` session state.\n",
        "- If condition is met, we append extra message to LLM response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2hWWDwNFodQ",
        "outputId": "e766ace8-bb30-40a8-f2f4-2215f38c771b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running LLM Agent Normally (with after_agent_callback) ---\n",
            "Event Output: SimpleLlmAgentWithAfter: Processing complete!\n",
            "[Callback] Exiting agent: SimpleLlmAgentWithAfter (Invocation: e-66895a8c-1509-46ea-9591-40c31cc044e7)\n",
            "[Callback] No concluding note added for agent SimpleLlmAgentWithAfter.\n",
            "\n",
            "--- Running LLM Agent with Concluding Note Condition ---\n",
            "Event Output: SimpleLlmAgentWithAfter: Processing complete!\n",
            "[Callback] Exiting agent: SimpleLlmAgentWithAfter (Invocation: e-3e3859d1-be23-41c2-887c-d16fe78f2190)\n",
            "[Callback] Adding concluding note for agent SimpleLlmAgentWithAfter.\n",
            "Event Output: SimpleLlmAgentWithAfter: Concluding note added by after_agent_callback.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\" # Or your preferred model\n",
        "\n",
        "# --- Define the Callback Function ---\n",
        "def simple_after_agent_logger(callback_context: CallbackContext) -> Optional[types.Content]:\n",
        "    \"\"\"Logs exit from an agent and optionally appends a message.\"\"\"\n",
        "    agent_name = callback_context.agent_name\n",
        "    invocation_id = callback_context.invocation_id\n",
        "    print(f\"[Callback] Exiting agent: {agent_name} (Invocation: {invocation_id})\")\n",
        "\n",
        "    # Example: Optionally return Content to append a message\n",
        "    if callback_context.state.get(\"add_concluding_note\", False):\n",
        "        print(f\"[Callback] Adding concluding note for agent {agent_name}.\")\n",
        "        # Return Content to append after the agent's own output\n",
        "        return types.Content(parts=[types.Part(text=f\"Concluding note added by after_agent_callback.\")])\n",
        "    else:\n",
        "        print(f\"[Callback] No concluding note added for agent {agent_name}.\")\n",
        "        # Return None - no additional message appended\n",
        "        return None\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent and Assign Callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"SimpleLlmAgentWithAfter\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are a simple agent. Just say 'Processing complete!'\",\n",
        "        description=\"An LLM agent demonstrating after_agent_callback\",\n",
        "        after_agent_callback=simple_after_agent_logger # Assign the function here\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(agent=my_llm_agent, app_name=\"llm_demo_app_after\", session_service=session_service)\n",
        "    session_id_run = \"llm_session_run_after_1\"\n",
        "    session_id_conclude = \"llm_session_conclude_1\"\n",
        "    user_id = \"llm_test_user_after\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(app_name=\"llm_demo_app_after\", user_id=user_id, session_id=session_id_run)\n",
        "    # Session where the callback will add a note\n",
        "    session_service.create_session(app_name=\"llm_demo_app_after\", user_id=user_id, session_id=session_id_conclude,\n",
        "                                  state={\"add_concluding_note\": True})\n",
        "\n",
        "\n",
        "    # Test with different session_id\n",
        "    print(\"--- Running LLM Agent Normally (with after_agent_callback) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_run,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"Run normally\")])):\n",
        "        # Print any event content from agent or callback\n",
        "        if event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "    print(\"\\n--- Running LLM Agent with Concluding Note Condition ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_conclude,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"Run and conclude\")])):\n",
        "        # Print any event content from agent or callback\n",
        "         if event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGs43srmLq7y"
      },
      "source": [
        "### LLM Agent with before_model_callback and state\n",
        "\n",
        "In this example, we demonstrate how to\n",
        "\n",
        "- modify system instruction\n",
        "- block model response based on keyword in query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCxIFM8CLrNx",
        "outputId": "a277150a-8394-4eb9-f2d1-16729331f7c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running LLM Agent Normally (with before_model_callback modification) ---\n",
            "[Callback] Before model call for agent: ModelCallbackAgent\n",
            "[Callback] Inspecting last user message: 'Tell me a short joke.'\n",
            "[Callback] Modified system instruction to: '[Modified by Callback] You are a helpful assistant.\n",
            "\n",
            "You are an agent. Your internal name is \"ModelCallbackAgent\".\n",
            "\n",
            " The description about you is \"An LLM agent demonstrating before_model_callback\"'\n",
            "[Callback] Proceeding with LLM call.\n",
            "Event Output: ModelCallbackAgent: Why don't scientists trust atoms? \n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "--- Running LLM Agent with BLOCK Keyword (triggering skip) ---\n",
            "[Callback] Before model call for agent: ModelCallbackAgent\n",
            "[Callback] Inspecting last user message: 'BLOCK this request.'\n",
            "[Callback] Modified system instruction to: '[Modified by Callback] You are a helpful assistant.\n",
            "\n",
            "You are an agent. Your internal name is \"ModelCallbackAgent\".\n",
            "\n",
            " The description about you is \"An LLM agent demonstrating before_model_callback\"'\n",
            "[Callback] 'BLOCK' keyword found. Skipping LLM call.\n",
            "Event Output: ModelCallbackAgent: LLM call was blocked by before_model_callback.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "# Need LlmRequest and LlmResponse for the callback signature and return type\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\" # Or your preferred model\n",
        "\n",
        "# --- Define the Callback Function ---\n",
        "def simple_before_model_modifier(\n",
        "    callback_context: CallbackContext, llm_request: LlmRequest\n",
        ") -> Optional[LlmResponse]:\n",
        "    \"\"\"Inspects/modifies the LLM request or skips the call.\"\"\"\n",
        "    agent_name = callback_context.agent_name\n",
        "    print(f\"[Callback] Before model call for agent: {agent_name}\")\n",
        "\n",
        "    # Inspect the last user message in the request contents\n",
        "    last_user_message = \"\"\n",
        "    if llm_request.contents and llm_request.contents[-1].role == 'user':\n",
        "         if llm_request.contents[-1].parts:\n",
        "            last_user_message = llm_request.contents[-1].parts[0].text\n",
        "    print(f\"[Callback] Inspecting last user message: '{last_user_message}'\")\n",
        "\n",
        "    # --- Modification Example ---\n",
        "    # Add a prefix to the system instruction\n",
        "    original_instruction = llm_request.config.system_instruction or types.Content(role=\"system\", parts=[])\n",
        "    prefix = \"[Modified by Callback] \"\n",
        "    # Ensure system_instruction is Content and parts list exists\n",
        "    if not isinstance(original_instruction, types.Content):\n",
        "         # Handle case where it might be a string (though config expects Content)\n",
        "         original_instruction = types.Content(role=\"system\", parts=[types.Part(text=str(original_instruction))])\n",
        "    if not original_instruction.parts:\n",
        "        original_instruction.parts.append(types.Part(text=\"\")) # Add an empty part if none exist\n",
        "\n",
        "    # Modify the text of the first part\n",
        "    modified_text = prefix + (original_instruction.parts[0].text or \"\")\n",
        "    original_instruction.parts[0].text = modified_text\n",
        "    llm_request.config.system_instruction = original_instruction\n",
        "    print(f\"[Callback] Modified system instruction to: '{modified_text}'\")\n",
        "\n",
        "\n",
        "    # --- Skip Example ---\n",
        "    # Check if the last user message contains \"BLOCK\"\n",
        "    if \"BLOCK\" in last_user_message.upper():\n",
        "        print(\"[Callback] 'BLOCK' keyword found. Skipping LLM call.\")\n",
        "        # Return an LlmResponse to skip the actual LLM call\n",
        "        return LlmResponse(\n",
        "            content=types.Content(\n",
        "                role=\"model\",\n",
        "                parts=[types.Part(text=\"LLM call was blocked by before_model_callback.\")],\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        print(\"[Callback] Proceeding with LLM call.\")\n",
        "        # Return None to allow the (modified) request to go to the LLM\n",
        "        return None\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent and Assign Callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"ModelCallbackAgent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are a helpful assistant.\", # Base instruction\n",
        "        description=\"An LLM agent demonstrating before_model_callback\",\n",
        "        before_model_callback=simple_before_model_modifier # Assign the function here\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(agent=my_llm_agent, app_name=\"llm_model_cb_app\", session_service=session_service)\n",
        "    session_id_run = \"model_cb_run_1\"\n",
        "    session_id_block = \"model_cb_block_1\"\n",
        "    user_id = \"model_cb_user\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(app_name=\"llm_model_cb_app\", user_id=user_id, session_id=session_id_run)\n",
        "    session_service.create_session(app_name=\"llm_model_cb_app\", user_id=user_id, session_id=session_id_block)\n",
        "\n",
        "    print(\"--- Running LLM Agent Normally (with before_model_callback modification) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_run,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"Tell me a short joke.\")])):\n",
        "        # Only print final LLM response or callback override\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "    print(\"\\n--- Running LLM Agent with BLOCK Keyword (triggering skip) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_block,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"BLOCK this request.\")])):\n",
        "         # Only print final LLM response or callback override\n",
        "         if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qhO2G_wLrgl"
      },
      "source": [
        "### LLM Agent with after_model_callback and state\n",
        "\n",
        "This use case demonstract\n",
        "- overwriting model response with custom text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n38CnCEfLrw_",
        "outputId": "c17c46d0-06ea-4d12-8153-899eab7bbe6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running LLM Agent Normally (Callback passes response through) ---\n",
            "[Callback] After model call for agent: AfterModelCallbackAgent\n",
            "[Callback] Inspected original response text: 'Hello! I'm AfterModelCallbackAgent, an LLM agent designed to demonstrate the use of the `after_model...'\n",
            "[Callback] 'scientists' not found. Passing original response through.\n",
            "Event Output: AfterModelCallbackAgent: Hello! I'm AfterModelCallbackAgent, an LLM agent designed to demonstrate the use of the `after_model_callback`. I'm here to help and answer your questions.\n",
            "\n",
            "--- Running LLM Agent with Input Triggering Modification ---\n",
            "[Callback] After model call for agent: AfterModelCallbackAgent\n",
            "[Callback] Inspected original response text: 'This is a funny question that plays on words! Here's why the joke works:\n",
            "\n",
            "*   **\"Atom\" sounds like \"...'\n",
            "[Callback] Found 'scientists'. Modifying response.\n",
            "[Callback] Returning modified response.\n",
            "Event Output: AfterModelCallbackAgent: This is a funny question that plays on words! Here's why the joke works:\n",
            "\n",
            "*   **\"Atom\" sounds like \"Adam\":** The joke relies on the similarity between the word \"atom\" (a basic unit of matter) and the name \"Adam\" (a common male name, often associated with the biblical figure).\n",
            "*   **Trust issues:** The humor comes from the idea that CUSTOM scientists wouldn't trust someone named Adam, or that atoms are inherently untrustworthy. In reality, CUSTOM scientists rely heavily on our understanding of atoms and their behavior!\n",
            "*   **No basis in reality:** It's funny because there's no logical reason for CUSTOM scientists to distrust atoms. The joke creates an absurd scenario.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional\n",
        "import copy # Needed to safely modify response content\n",
        "\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\" # Or your preferred model\n",
        "\n",
        "# --- Define the Callback Function ---\n",
        "def simple_after_model_modifier(\n",
        "    callback_context: CallbackContext, llm_response: LlmResponse\n",
        ") -> Optional[LlmResponse]:\n",
        "    \"\"\"Inspects/modifies the LLM response after it's received.\"\"\"\n",
        "    agent_name = callback_context.agent_name\n",
        "    print(f\"[Callback] After model call for agent: {agent_name}\")\n",
        "\n",
        "    # --- Inspection ---\n",
        "    original_text = \"\"\n",
        "    if llm_response.content and llm_response.content.parts:\n",
        "        # Assuming simple text response for this example\n",
        "        if llm_response.content.parts[0].text:\n",
        "            original_text = llm_response.content.parts[0].text\n",
        "            print(f\"[Callback] Inspected original response text: '{original_text[:100]}...'\") # Log snippet\n",
        "        elif llm_response.content.parts[0].function_call:\n",
        "             print(f\"[Callback] Inspected response: Contains function call '{llm_response.content.parts[0].function_call.name}'. No text modification.\")\n",
        "             return None # Don't modify tool calls in this example\n",
        "        else:\n",
        "             print(\"[Callback] Inspected response: No text content found.\")\n",
        "             return None\n",
        "    elif llm_response.error_message:\n",
        "        print(f\"[Callback] Inspected response: Contains error '{llm_response.error_message}'. No modification.\")\n",
        "        return None\n",
        "    else:\n",
        "        print(\"[Callback] Inspected response: Empty LlmResponse.\")\n",
        "        return None # Nothing to modify\n",
        "\n",
        "    # --- Modification Example ---\n",
        "    # Replace \"scientists\" with \"funny scientists\" (case-insensitive)\n",
        "    search_term = \"scientists\"\n",
        "    replace_term = \"CUSTOM scientists\"\n",
        "    if search_term in original_text.lower():\n",
        "        print(f\"[Callback] Found '{search_term}'. Modifying response.\")\n",
        "        modified_text = original_text.replace(search_term, replace_term)\n",
        "        modified_text = modified_text.replace(search_term.capitalize(), replace_term.capitalize()) # Handle capitalization\n",
        "\n",
        "        # Create a NEW LlmResponse with the modified content\n",
        "        # Deep copy parts to avoid modifying original if other callbacks exist\n",
        "        modified_parts = [copy.deepcopy(part) for part in llm_response.content.parts]\n",
        "        modified_parts[0].text = modified_text # Update the text in the copied part\n",
        "\n",
        "        new_response = LlmResponse(\n",
        "             content=types.Content(role=\"model\", parts=modified_parts),\n",
        "             # Copy other relevant fields if necessary, e.g., grounding_metadata\n",
        "             grounding_metadata=llm_response.grounding_metadata\n",
        "             )\n",
        "        print(f\"[Callback] Returning modified response.\")\n",
        "        return new_response # Return the modified response\n",
        "    else:\n",
        "        print(f\"[Callback] '{search_term}' not found. Passing original response through.\")\n",
        "        # Return None to use the original llm_response\n",
        "        return None\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent and Assign Callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"AfterModelCallbackAgent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are a helpful assistant.\",\n",
        "        description=\"An LLM agent demonstrating after_model_callback\",\n",
        "        after_model_callback=simple_after_model_modifier # Assign the function here\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(agent=my_llm_agent, app_name=\"llm_after_model_cb_app\", session_service=session_service)\n",
        "    session_id_run = \"after_model_cb_run_1\"\n",
        "    session_id_modify = \"after_model_cb_modify_1\"\n",
        "    user_id = \"after_model_cb_user\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(app_name=\"llm_after_model_cb_app\", user_id=user_id, session_id=session_id_run)\n",
        "    session_service.create_session(app_name=\"llm_after_model_cb_app\", user_id=user_id, session_id=session_id_modify)\n",
        "\n",
        "    print(\"--- Running LLM Agent Normally (Callback passes response through) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_run,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"Say hello.\")])):\n",
        "        # Only print final LLM response\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "    print(\"\\n--- Running LLM Agent with Input Triggering Modification ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_modify,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"Why don't scientists trust atoms\")])):\n",
        "         # Only print final LLM response\n",
        "         if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNdkfIVGQ3Af"
      },
      "source": [
        "### LLM Agent with before_tool_callback and state\n",
        "\n",
        "This cell demonstract:\n",
        "- inspection of tool arguments, and overwriting of argument and response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zsGbxXwQ3Po",
        "outputId": "0b14f3a7-0107-42ed-c5db-3b45e8d91d7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running Agent (Normal Tool Call - Germany) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Callback] Before tool call for tool 'get_capital_city' in agent 'ToolCallbackAgent'\n",
            "[Callback] Original args: {'country': 'Germany'}\n",
            "[Callback] Proceeding with original or previously modified args.\n",
            "--- Tool 'get_capital_city' executing with country: Germany ---\n",
            "Event Output: ToolCallbackAgent: Berlin\n",
            "\n",
            "--- Running Agent (Tool Call Triggering Modification - Canada -> France) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Callback] Before tool call for tool 'get_capital_city' in agent 'ToolCallbackAgent'\n",
            "[Callback] Original args: {'country': 'Canada'}\n",
            "[Callback] Detected 'Canada'. Modifying args to 'France'.\n",
            "[Callback] Modified args: {'country': 'France'}\n",
            "--- Tool 'get_capital_city' executing with country: France ---\n",
            "Event Output: ToolCallbackAgent: I am sorry, I made a mistake and looked up the capital of France instead of Canada. I am unable to answer this question.\n",
            "\n",
            "--- Running Agent (Tool Call Triggering Skip - BLOCK) ---\n",
            "Event Output: ToolCallbackAgent: I cannot provide an answer as 'BLOCK' is not a valid country name. Please provide a valid country name to find the capital city.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional, Dict, Any\n",
        "import copy\n",
        "\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "# Need LlmRequest, LlmResponse for context\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "# Need BaseTool, ToolContext for the callback signature\n",
        "from google.adk.tools.base_tool import BaseTool\n",
        "from google.adk.tools.tool_context import ToolContext\n",
        "# Using FunctionTool to easily create a tool\n",
        "from google.adk.tools.function_tool import FunctionTool\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\" # Or your preferred model\n",
        "\n",
        "# --- Define a Simple Tool Function ---\n",
        "def get_capital_city(country: str) -> str:\n",
        "    \"\"\"Retrieves the capital city of a given country.\"\"\"\n",
        "    print(f\"--- Tool 'get_capital_city' executing with country: {country} ---\")\n",
        "    country_capitals = {\n",
        "        \"united states\": \"Washington, D.C.\",\n",
        "        \"canada\": \"Ottawa\", # Intentionally correct here\n",
        "        \"france\": \"Paris\",\n",
        "        \"germany\": \"Berlin\",\n",
        "    }\n",
        "    return country_capitals.get(country.lower(), f\"Capital not found for {country}\")\n",
        "\n",
        "# --- Wrap the function into a Tool ---\n",
        "capital_tool = FunctionTool(func=get_capital_city)\n",
        "\n",
        "# --- Define the Callback Function ---\n",
        "def simple_before_tool_modifier(\n",
        "    tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext\n",
        ") -> Optional[Dict]:\n",
        "    \"\"\"Inspects/modifies tool args or skips the tool call.\"\"\"\n",
        "    agent_name = tool_context.agent_name\n",
        "    tool_name = tool.name\n",
        "    print(f\"[Callback] Before tool call for tool '{tool_name}' in agent '{agent_name}'\")\n",
        "    print(f\"[Callback] Original args: {args}\")\n",
        "\n",
        "    # --- Modification Example ---\n",
        "    # If the tool is 'get_capital_city' and country is 'Canada', change it to 'France'\n",
        "    if tool_name == 'get_capital_city' and args.get('country', '').lower() == 'canada':\n",
        "        print(\"[Callback] Detected 'Canada'. Modifying args to 'France'.\")\n",
        "        args['country'] = 'France' # Modify the args dictionary directly\n",
        "        print(f\"[Callback] Modified args: {args}\")\n",
        "        return None # Proceed with modified args\n",
        "\n",
        "    # --- Skip Example ---\n",
        "    # If the tool is 'get_capital_city' and country is 'BLOCK'\n",
        "    if tool_name == 'get_capital_city' and args.get('country', '').upper() == 'BLOCK':\n",
        "        print(\"[Callback] Detected 'BLOCK'. Skipping tool execution.\")\n",
        "        # Return a dictionary to be used as the tool result, skipping the actual tool call\n",
        "        return {\"result\": \"Tool execution was blocked by before_tool_callback.\"}\n",
        "\n",
        "    print(\"[Callback] Proceeding with original or previously modified args.\")\n",
        "    # Return None to allow the tool to execute normally (with original or modified args)\n",
        "    return None\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent with the tool and callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"ToolCallbackAgent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are an agent that can find capital cities. Use the get_capital_city tool.\",\n",
        "        description=\"An LLM agent demonstrating before_tool_callback\",\n",
        "        tools=[capital_tool], # Add the tool here\n",
        "        before_tool_callback=simple_before_tool_modifier # Assign the callback here\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(agent=my_llm_agent, app_name=\"llm_tool_cb_app\", session_service=session_service)\n",
        "    session_id_run = \"tool_cb_run_1\"\n",
        "    session_id_modify = \"tool_cb_modify_1\"\n",
        "    session_id_block = \"tool_cb_block_1\"\n",
        "    user_id = \"tool_cb_user\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(app_name=\"llm_tool_cb_app\", user_id=user_id, session_id=session_id_run)\n",
        "    session_service.create_session(app_name=\"llm_tool_cb_app\", user_id=user_id, session_id=session_id_modify)\n",
        "    session_service.create_session(app_name=\"llm_tool_cb_app\", user_id=user_id, session_id=session_id_block)\n",
        "\n",
        "    print(\"--- Running Agent (Normal Tool Call - Germany) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_run,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"What is the capital of Germany?\")])):\n",
        "        # Only print final LLM response\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "    print(\"\\n--- Running Agent (Tool Call Triggering Modification - Canada -> France) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_modify,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"What is the capital of Canada?\")])):\n",
        "         # Only print final LLM response\n",
        "         if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "    print(\"\\n--- Running Agent (Tool Call Triggering Skip - BLOCK) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_block,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"What is the capital of BLOCK?\")])):\n",
        "         # Only print final LLM response\n",
        "         if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-36dSreYQ3mG"
      },
      "source": [
        "### LLM Agent with after_tool_callback and state\n",
        "\n",
        "- Modify tool call result based on tool argument.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEFUbgV3Q352",
        "outputId": "58bb6039-c6d4-4023-e626-7f4cd5de808f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running Agent (Callback passes result through - France) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Tool 'get_capital_city' executing with country: France ---\n",
            "[Callback] After tool call for tool 'get_capital_city' in agent 'AfterToolCallbackAgent'\n",
            "[Callback] Args used: {'country': 'France'}\n",
            "[Callback] Original tool_response: {'result': 'Paris'}\n",
            "[Callback] Passing original tool response through.\n",
            "Event Output: AfterToolCallbackAgent: The capital of France is Paris.\n",
            "\n",
            "--- Running Agent (Callback modifies result - United States) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Tool 'get_capital_city' executing with country: United States ---\n",
            "[Callback] After tool call for tool 'get_capital_city' in agent 'AfterToolCallbackAgent'\n",
            "[Callback] Args used: {'country': 'United States'}\n",
            "[Callback] Original tool_response: {'result': 'Washington, D.C.'}\n",
            "[Callback] Detected 'Washington, D.C.'. Modifying tool response.\n",
            "[Callback] Modified tool_response: {'result': 'Washington, D.C. (Note: This is the capital of the USA).', 'note_added_by_callback': True}\n",
            "Event Output: AfterToolCallbackAgent: The capital of the United States is Washington, D.C. (Note: This is the capital of the USA).\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional, Dict, Any\n",
        "import copy # Good practice for modifying results\n",
        "\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.adk.tools.base_tool import BaseTool\n",
        "from google.adk.tools.tool_context import ToolContext\n",
        "from google.adk.tools.function_tool import FunctionTool\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\" # Or your preferred model\n",
        "\n",
        "# --- Define a Simple Tool Function (Same as before) ---\n",
        "def get_capital_city(country: str) -> str:\n",
        "    \"\"\"Retrieves the capital city of a given country.\"\"\"\n",
        "    print(f\"--- Tool 'get_capital_city' executing with country: {country} ---\")\n",
        "    country_capitals = {\n",
        "        \"united states\": \"Washington, D.C.\",\n",
        "        \"canada\": \"Ottawa\",\n",
        "        \"france\": \"Paris\",\n",
        "        \"germany\": \"Berlin\",\n",
        "    }\n",
        "    return {\"result\": country_capitals.get(country.lower(), f\"Capital not found for {country}\")}\n",
        "\n",
        "# --- Wrap the function into a Tool ---\n",
        "capital_tool = FunctionTool(func=get_capital_city)\n",
        "\n",
        "# --- Define the Callback Function ---\n",
        "def simple_after_tool_modifier(\n",
        "    tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext, tool_response: Dict\n",
        ") -> Optional[Dict]:\n",
        "    \"\"\"Inspects/modifies the tool result after execution.\"\"\"\n",
        "    agent_name = tool_context.agent_name\n",
        "    tool_name = tool.name\n",
        "    print(f\"[Callback] After tool call for tool '{tool_name}' in agent '{agent_name}'\")\n",
        "    print(f\"[Callback] Args used: {args}\")\n",
        "    print(f\"[Callback] Original tool_response: {tool_response}\")\n",
        "\n",
        "    # Default structure for function tool results is {\"result\": <return_value>}\n",
        "    original_result_value = tool_response.get(\"result\", \"\")\n",
        "    # original_result_value = tool_response\n",
        "\n",
        "    # --- Modification Example ---\n",
        "    # If the tool was 'get_capital_city' and result is 'Washington, D.C.'\n",
        "    if tool_name == 'get_capital_city' and original_result_value == \"Washington, D.C.\":\n",
        "        print(\"[Callback] Detected 'Washington, D.C.'. Modifying tool response.\")\n",
        "\n",
        "        # IMPORTANT: Create a new dictionary or modify a copy\n",
        "        modified_response = copy.deepcopy(tool_response)\n",
        "        modified_response[\"result\"] = f\"{original_result_value} (Note: This is the capital of the USA).\"\n",
        "        modified_response[\"note_added_by_callback\"] = True # Add extra info if needed\n",
        "\n",
        "        print(f\"[Callback] Modified tool_response: {modified_response}\")\n",
        "        return modified_response # Return the modified dictionary\n",
        "\n",
        "    print(\"[Callback] Passing original tool response through.\")\n",
        "    # Return None to use the original tool_response\n",
        "    return None\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent with the tool and callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"AfterToolCallbackAgent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are an agent that finds capital cities using the get_capital_city tool. Report the result clearly.\",\n",
        "        description=\"An LLM agent demonstrating after_tool_callback\",\n",
        "        tools=[capital_tool], # Add the tool\n",
        "        after_tool_callback=simple_after_tool_modifier # Assign the callback\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(agent=my_llm_agent, app_name=\"llm_after_tool_cb_app\", session_service=session_service)\n",
        "    session_id_run = \"after_tool_cb_run_1\"\n",
        "    session_id_modify = \"after_tool_cb_modify_1\"\n",
        "    user_id = \"after_tool_cb_user\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(app_name=\"llm_after_tool_cb_app\", user_id=user_id, session_id=session_id_run)\n",
        "    session_service.create_session(app_name=\"llm_after_tool_cb_app\", user_id=user_id, session_id=session_id_modify)\n",
        "\n",
        "    print(\"--- Running Agent (Callback passes result through - France) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_run,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"What is the capital of France?\")])):\n",
        "        # Only print final LLM response\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "    print(\"\\n--- Running Agent (Callback modifies result - United States) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_modify,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"What is the capital of the United States?\")])):\n",
        "         # Only print final LLM response\n",
        "         if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yecR7byQ01bQ"
      },
      "source": [
        "### LLM Agent with Gaurdrail (Profanity Checker with before_model callback)\n",
        "\n",
        "- Prevent model calling if there is bad word detected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xexSYqxPuSiH"
      },
      "outputs": [],
      "source": [
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.agents.llm_agent import AfterModelCallback, BeforeModelCallback\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.adk.agents.llm_agent import LlmAgent\n",
        "from typing import Any, List, Optional\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "\n",
        "\n",
        "def profanity_guardrail(\n",
        "    callback_context: CallbackContext, llm_request: LlmRequest\n",
        ") -> Optional[LlmResponse]:\n",
        "    \"\"\"Check for profanity in the model request.\"\"\"\n",
        "    profanity_list: List[str] = [\"badword1\", \"badword2\", \"badword3\"]\n",
        "    if llm_request.contents:\n",
        "        for content in llm_request.contents:\n",
        "            for part in content.parts:\n",
        "                if part.text:\n",
        "                    for profanity in profanity_list:\n",
        "                        if profanity in part.text.lower():\n",
        "                            callback_context.state[\"profanity_trigger\"] = True\n",
        "                            return LlmResponse(\n",
        "                                content=types.Content(\n",
        "                                    role=\"model\",\n",
        "                                    parts=[\n",
        "                                        types.Part(\n",
        "                                            text=(\n",
        "                                                \"No bad word allowed.\"\n",
        "                                            )\n",
        "                                        )\n",
        "                                    ],\n",
        "                                )\n",
        "                            )\n",
        "    return None\n",
        "\n",
        "\n",
        "# Tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Retrieves weather information for the given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city for which to retrieve weather information.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the weather information for the specified city,\n",
        "        or a message indicating that the weather information was not found.\n",
        "    \"\"\"\n",
        "    cities = {\n",
        "        'chicago': {'temperature': 25, 'condition': 'sunny', 'sky': 'clear'},\n",
        "        'toronto': {'temperature': 30, 'condition': 'partly cloudy', 'sky': 'overcast'},\n",
        "        'chennai': {'temperature': 15, 'condition': 'rainy', 'sky': 'cloudy'},\n",
        "    }\n",
        "\n",
        "    city_lower = city.lower()\n",
        "    if city_lower in cities:\n",
        "        weather_data = cities[city_lower]\n",
        "        return f\"Weather in {city} is {weather_data['temperature']} degrees Celsius, {weather_data['condition']} with a {weather_data['sky']} sky.\"\n",
        "    else:\n",
        "        return f\"Weather information for {city} not found.\"\n",
        "\n",
        "async def run_query(query: str):\n",
        "    weather_agent = LlmAgent(\n",
        "        model=GEMINI_2_FLASH,\n",
        "        name=\"weather_agent\",\n",
        "        instruction=\"\"\"You are a Weather Information Agent. Your task is to provide weather information for a given city.\n",
        "\n",
        "        When a user provides a prompt, extract the city name.\n",
        "        Then, use the `get_weather` tool to retrieve the weather information for that city.\n",
        "        Finally, present the weather information to the user in a clear and concise manner.\n",
        "        If the user asks for a greeting, transfer to the greeting agent.\"\"\",\n",
        "        description=\"\"\"You are an agent who can fetch weather information for a city.\n",
        "        You have access to the `get_weather` tool to accomplish this task.\"\"\",\n",
        "        tools=[get_weather],\n",
        "        before_model_callback=profanity_guardrail,\n",
        "    )\n",
        "\n",
        "    session_service = InMemorySessionService()\n",
        "    session = session_service.create_session(\n",
        "        app_name=\"weather_app\", user_id=\"12345\", session_id=\"123344\"\n",
        "    )\n",
        "    runner = Runner(\n",
        "        agent=weather_agent,\n",
        "        app_name=\"weather_app\",\n",
        "        session_service=session_service,\n",
        "    )\n",
        "\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    async for event in runner.run_async(\n",
        "        user_id=\"12345\", session_id=\"123344\", new_message=content\n",
        "    ):\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(f\"Query: {query}\")\n",
        "            print(f\"Agent Response: {final_response}\")\n",
        "            print(\"-\" * 20)\n",
        "        if \"profanity_trigger\" in event.actions.state_delta:\n",
        "            print(f\"Profanity Triggered: {event.actions.state_delta['profanity_trigger']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K2D004BzaEv",
        "outputId": "b2b789fb-166c-4392-9721-9b4ec7156e14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What is the weather in Chicago?\n",
            "Agent Response: The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\n",
            "\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "await run_query(\"What is the weather in Chicago?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3L1N4AXza-C",
        "outputId": "4efaf126-23e5-43ea-ec15-8d5173a586a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: what the badword1 is the weather in Chicago?\n",
            "Agent Response: No bad word allowed.\n",
            "--------------------\n",
            "Profanity Triggered: True\n"
          ]
        }
      ],
      "source": [
        "await run_query(\"what the badword1 is the weather in Chicago?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG4lwya4iUoX"
      },
      "source": [
        "### LlmAgent with All Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a89KSH0tiUb1",
        "outputId": "f6159277-4327-47fe-f8e5-3472906376f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Workflow for Query: 'Help! My computer screen keeps flickering constantly.' ---\n",
            "\n",
            "[User Submits Ticket: 'Help! My computer screen keeps flickering constantly.']\n",
            "\n",
            "[Callback Triggered: before_agent_callback]\n",
            "  -> Processing Ticket ID: TICKET-ABC\n",
            "  -> Agent 'support_agent' starting.\n",
            "\n",
            "[Callback Triggered: before_model_callback]\n",
            "  -> Preparing to call LLM for agent 'support_agent'.\n",
            "  -> Safety check/prompt augmentation applied (simulated).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Callback Triggered: after_model_callback]\n",
            "  -> Received LLM response for agent 'support_agent'.\n",
            "  -> LLM Raw Response (brief): Okay, I understand your computer screen is flickering. I can help you with that.\n",
            "\n",
            "...\n",
            "  -> PII check passed (simulated).\n",
            "\n",
            "[AFW: LLM decided to use Tool 'kb_search']\n",
            "\n",
            "[Callback Triggered: before_tool_callback for Tool: 'kb_search']\n",
            "  -> Attempting to call tool 'kb_search' with args: {'keywords': ['screen', 'flickering']}\n",
            "  -> Keyword validation passed.\n",
            "      [Tool Executing: kb_search with keywords: ['screen', 'flickering']]\n",
            "      [Tool Result: Found 4 steps]\n",
            "\n",
            "[Callback Triggered: after_tool_callback for Tool: 'kb_search']\n",
            "  -> Tool 'kb_search' executed with args: {'keywords': ['screen', 'flickering']}\n",
            "  -> Received tool response (brief): {'solutions': ['Check the display cable connection.', 'Try a different monitor port.', 'Update the graphics driver.', 'Adjust the screen refresh rate.']}...\n",
            "  -> Caching tool result (simulated).\n",
            "\n",
            "[AFW: Received result from Tool 'kb_search']\n",
            "  -> AFW: Sending tool result back to LLM for final response generation...\n",
            "\n",
            "[Callback Triggered: before_model_callback]\n",
            "  -> Preparing to call LLM for agent 'support_agent'.\n",
            "  -> Safety check/prompt augmentation applied (simulated).\n",
            "\n",
            "[Callback Triggered: after_model_callback]\n",
            "  -> Received LLM response for agent 'support_agent'.\n",
            "  -> LLM Raw Response (brief): Okay, I have some troubleshooting steps for you. Please try the following:\n",
            "\n",
            "1.  **Check the display ...\n",
            "  -> PII check passed (simulated).\n",
            "\n",
            "[AFW: Sending Final Response to User]\n",
            "  -> Response: Okay, I have some troubleshooting steps for you. Please try the following:\n",
            "\n",
            "1.  **Check the display cable connection:** Make sure the cable connecting your monitor to your computer is securely plugged in at both ends. Try disconnecting and reconnecting it.\n",
            "2.  **Try a different monitor port:** If you're using HDMI, try DisplayPort, or vice versa. Also, if your computer has multiple ports of the same type, try a different one.\n",
            "3.  **Update the graphics driver:** Outdated or corrupted graphics drivers can cause screen flickering. You can usually find the latest drivers on the website of your graphics card manufacturer (e.g., NVIDIA, AMD, Intel).\n",
            "4.  **Adjust the screen refresh rate:** Sometimes, the refresh rate can cause flickering. Right-click on your desktop, select \"Display settings,\" then \"Advanced display settings,\" and choose a different refresh rate from the dropdown menu. Common refresh rates are 60Hz or 75Hz.\n",
            "\n",
            "If none of these steps resolve the issue, the monitor itself might be faulty.\n",
            "\n",
            "\n",
            "[Callback Triggered: after_agent_callback]\n",
            "  -> Finished processing for Ticket ID: TICKET-ABC.\n",
            "  -> Updating external system: Ticket TICKET-ABC status set to 'Responded'.\n",
            "\n",
            "--- Workflow Finished for Query: 'Help! My computer screen keeps flickering constantly.' ---\n",
            "Final Session State (example): ticket_id='TICKET-ABC', status='completed'\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Okay, I have some troubleshooting steps for you. Please try the following:\\n\\n1.  **Check the display cable connection:** Make sure the cable connecting your monitor to your computer is securely plugged in at both ends. Try disconnecting and reconnecting it.\\n2.  **Try a different monitor port:** If you\\'re using HDMI, try DisplayPort, or vice versa. Also, if your computer has multiple ports of the same type, try a different one.\\n3.  **Update the graphics driver:** Outdated or corrupted graphics drivers can cause screen flickering. You can usually find the latest drivers on the website of your graphics card manufacturer (e.g., NVIDIA, AMD, Intel).\\n4.  **Adjust the screen refresh rate:** Sometimes, the refresh rate can cause flickering. Right-click on your desktop, select \"Display settings,\" then \"Advanced display settings,\" and choose a different refresh rate from the dropdown menu. Common refresh rates are 60Hz or 75Hz.\\n\\nIf none of these steps resolve the issue, the monitor itself might be faulty.\\n'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import asyncio\n",
        "import warnings\n",
        "import os\n",
        "from typing import Any, Optional, Dict, List, AsyncGenerator\n",
        "\n",
        "# --- ADK Imports ---\n",
        "from google.adk.agents import Agent, LlmAgent, BaseAgent # Using LlmAgent directly\n",
        "from google.adk.sessions import InMemorySessionService, Session, State\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.adk.tools.base_tool import BaseTool\n",
        "from google.adk.tools.tool_context import ToolContext\n",
        "from google.adk.tools.function_tool import FunctionTool\n",
        "from google.adk.events import Event\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.genai import types\n",
        "\n",
        "# Suppress specific UserWarning from google.generativeai if necessary\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='google.generativeai.types.content_types')\n",
        "\n",
        "# --- Constants ---\n",
        "APP_NAME = \"support_ticket_app\"\n",
        "USER_ID = \"customer_123\"\n",
        "SESSION_ID = \"ticket_session_abc\"\n",
        "AGENT_NAME = \"support_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\" # Or your preferred Gemini model\n",
        "\n",
        "# --- Simulated Knowledge Base Tool ---\n",
        "def kb_search(keywords: List[str]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Searches the knowledge base for troubleshooting steps based on keywords.\n",
        "\n",
        "    Args:\n",
        "        keywords: A list of keywords related to the issue (e.g., ['screen', 'flickering']).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing potential solutions or an empty dictionary if none found.\n",
        "    \"\"\"\n",
        "    print(f\"      [Tool Executing: kb_search with keywords: {keywords}]\")\n",
        "    # Simple mock implementation\n",
        "    mock_kb = {\n",
        "        \"screen\": [\"Check the display cable connection.\", \"Try a different monitor port.\"],\n",
        "        \"flickering\": [\"Update the graphics driver.\", \"Adjust the screen refresh rate.\"],\n",
        "        \"display\": [\"Ensure monitor power is on.\", \"Reboot the computer.\"],\n",
        "        \"keyboard\": [\"Check battery if wireless.\", \"Try a different USB port.\"]\n",
        "    }\n",
        "    results = []\n",
        "    for keyword in keywords:\n",
        "        if keyword.lower() in mock_kb:\n",
        "            results.extend(mock_kb[keyword.lower()])\n",
        "\n",
        "    if results:\n",
        "        # Remove duplicates while preserving order (if Python 3.7+)\n",
        "        unique_results = list(dict.fromkeys(results))\n",
        "        print(f\"      [Tool Result: Found {len(unique_results)} steps]\")\n",
        "        return {\"solutions\": unique_results}\n",
        "    else:\n",
        "        print(\"      [Tool Result: No relevant KB articles found]\")\n",
        "        return {\"solutions\": [\"No specific troubleshooting steps found in KB for these keywords.\"]}\n",
        "\n",
        "# Wrap the function into a FunctionTool\n",
        "kb_search_tool = FunctionTool(func=kb_search)\n",
        "\n",
        "\n",
        "# --- Callback Implementations ---\n",
        "\n",
        "def log_before_agent(callback_context: CallbackContext) -> Optional[types.Content]:\n",
        "    \"\"\"Callback executed before the agent starts processing.\"\"\"\n",
        "    ticket_id = f\"TICKET-{SESSION_ID.split('_')[-1].upper()}\" # Simulate getting ticket ID\n",
        "    print(f\"\\n[Callback Triggered: before_agent_callback]\")\n",
        "    print(f\"  -> Processing Ticket ID: {ticket_id}\")\n",
        "    # Example state modification: Store ticket ID\n",
        "    callback_context.state[\"ticket_id\"] = ticket_id\n",
        "    print(f\"  -> Agent '{callback_context.agent_name}' starting.\")\n",
        "    return None # Return None to allow agent execution\n",
        "\n",
        "def log_after_agent(callback_context: CallbackContext) -> Optional[types.Content]:\n",
        "    \"\"\"Callback executed after the agent finishes processing.\"\"\"\n",
        "    ticket_id = callback_context.state.get(\"ticket_id\", \"UNKNOWN\")\n",
        "    print(f\"\\n[Callback Triggered: after_agent_callback]\")\n",
        "    print(f\"  -> Finished processing for Ticket ID: {ticket_id}.\")\n",
        "    # Example: Simulate updating ticket status in an external system\n",
        "    print(f\"  -> Updating external system: Ticket {ticket_id} status set to 'Responded'.\")\n",
        "    # Example state modification\n",
        "    callback_context.state[\"processing_status\"] = \"completed\"\n",
        "    return None # Return None, we don't want to append extra content here\n",
        "\n",
        "def log_before_model(callback_context: CallbackContext, llm_request: LlmRequest) -> Optional[LlmResponse]:\n",
        "    \"\"\"Callback executed before sending the request to the LLM.\"\"\"\n",
        "    print(f\"\\n[Callback Triggered: before_model_callback]\")\n",
        "    print(f\"  -> Preparing to call LLM for agent '{callback_context.agent_name}'.\")\n",
        "    # Example: Log request details (be careful with PII in real scenarios)\n",
        "    # print(f\"  -> LLM Request Contents (brief): {str(llm_request.contents)[:200]}...\")\n",
        "    # Example: Add a safety reminder (Note: modifying system_instruction directly might be overwritten)\n",
        "    # llm_request.append_instructions([\"Remember to be helpful and safe.\"]) # Use append_instructions\n",
        "    print(f\"  -> Safety check/prompt augmentation applied (simulated).\")\n",
        "    return None # Return None to proceed with LLM call\n",
        "\n",
        "def check_after_model(callback_context: CallbackContext, llm_response: LlmResponse) -> Optional[LlmResponse]:\n",
        "    \"\"\"Callback executed after receiving the response from the LLM.\"\"\"\n",
        "    print(f\"\\n[Callback Triggered: after_model_callback]\")\n",
        "    print(f\"  -> Received LLM response for agent '{callback_context.agent_name}'.\")\n",
        "    # Example: Log response details (be careful with PII)\n",
        "    response_text = llm_response.content.parts[0].text if llm_response.content and llm_response.content.parts else \"[No Text]\"\n",
        "    print(f\"  -> LLM Raw Response (brief): {response_text[:100]}...\")\n",
        "    # Example: Simulate PII check\n",
        "    if \"password\" in response_text.lower() or \"credit card\" in response_text.lower():\n",
        "        print(\"  -> !! PII potentially detected in LLM response (simulated) !!\")\n",
        "        # Could modify response here, e.g., return an error or redacted text\n",
        "        # return LlmResponse(content=types.Content(parts=[types.Part(text=\"[Response redacted due to potential PII]\")]))\n",
        "    else:\n",
        "        print(\"  -> PII check passed (simulated).\")\n",
        "    return None # Return None to use the original (or potentially modified) LLM response\n",
        "\n",
        "def validate_before_tool(tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext) -> Optional[Dict]:\n",
        "    \"\"\"Callback executed before a tool is called.\"\"\"\n",
        "    print(f\"\\n[Callback Triggered: before_tool_callback for Tool: '{tool.name}']\")\n",
        "    print(f\"  -> Attempting to call tool '{tool.name}' with args: {args}\")\n",
        "    # Example: Validate arguments\n",
        "    if tool.name == \"kb_search\" and \"keywords\" in args:\n",
        "        if not isinstance(args[\"keywords\"], list) or not args[\"keywords\"]:\n",
        "            print(\"  -> !! Validation Failed: Keywords must be a non-empty list. Skipping tool call. !!\")\n",
        "            return {\"error\": \"Invalid keywords provided for KB search.\"} # Return error to LLM\n",
        "        print(\"  -> Keyword validation passed.\")\n",
        "    return None # Return None to proceed with actual tool execution\n",
        "\n",
        "def log_after_tool(tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext, tool_response: Dict) -> Optional[Dict]:\n",
        "    \"\"\"Callback executed after a tool has run.\"\"\"\n",
        "    print(f\"\\n[Callback Triggered: after_tool_callback for Tool: '{tool.name}']\")\n",
        "    print(f\"  -> Tool '{tool.name}' executed with args: {args}\")\n",
        "    print(f\"  -> Received tool response (brief): {str(tool_response)[:200]}...\")\n",
        "    # Example: Caching simulation (just log it)\n",
        "    print(f\"  -> Caching tool result (simulated).\")\n",
        "    # Example: Modify response if needed\n",
        "    # if \"solutions\" in tool_response and tool_response[\"solutions\"]:\n",
        "    #    tool_response[\"solutions\"].append(\"Also, try restarting your device.\") # Append suggestion\n",
        "    return None # Return None to use the original (or modified) tool response\n",
        "\n",
        "\n",
        "# --- Agent Definition ---\n",
        "support_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are an IT Support Agent. Your goal is to help users troubleshoot technical issues.\n",
        "1. Analyze the user's problem description (support ticket).\n",
        "2. Identify keywords related to the issue.\n",
        "3. If the issue relates to common hardware problems like 'screen', 'display', 'flickering', 'keyboard', use the `kb_search` tool with the identified keywords to find troubleshooting steps.\n",
        "4. Based on your analysis and any results from the `kb_search` tool, provide a clear, step-by-step response to the user.\n",
        "5. If the `kb_search` tool doesn't return useful information, state that and provide general troubleshooting advice (e.g., restart, check connections).\n",
        "\"\"\",\n",
        "    description=\"First-level IT support agent that analyzes issues and uses a knowledge base.\",\n",
        "    tools=[kb_search_tool],\n",
        "\n",
        "    # --- Assign Callbacks ---\n",
        "    before_agent_callback=log_before_agent,\n",
        "    after_agent_callback=log_after_agent,\n",
        "    before_model_callback=log_before_model,\n",
        "    after_model_callback=check_after_model,\n",
        "    before_tool_callback=validate_before_tool,\n",
        "    after_tool_callback=log_after_tool,\n",
        ")\n",
        "\n",
        "# --- Session and Runner Setup ---\n",
        "session_service = InMemorySessionService()\n",
        "# Ensure session is created before running\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=support_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "# --- Agent Interaction Logic ---\n",
        "async def call_agent_and_show_flow(query):\n",
        "  print(f\"\\n--- Starting Workflow for Query: '{query}' ---\")\n",
        "  print(f\"\\n[User Submits Ticket: '{query}']\")\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  final_response_text = \"[Agent did not produce a final response text]\"\n",
        "\n",
        "  async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
        "      # Print event details to trace the flow\n",
        "      # print(f\"\\nDEBUG Event: Author={event.author}, Partial={event.partial}, Final={event.is_final_response()}, Content={str(event.content)[:100]}...\")\n",
        "\n",
        "      if event.get_function_calls():\n",
        "           print(f\"\\n[AFW: LLM decided to use Tool '{event.get_function_calls()[0].name}']\")\n",
        "           # Before tool callback is triggered internally by the runner/flow\n",
        "\n",
        "      elif event.get_function_responses():\n",
        "           print(f\"\\n[AFW: Received result from Tool '{event.get_function_responses()[0].name}']\")\n",
        "           # After tool callback is triggered internally by the runner/flow\n",
        "           print(\"  -> AFW: Sending tool result back to LLM for final response generation...\")\n",
        "\n",
        "      if event.is_final_response() and event.content and event.content.parts:\n",
        "          # Check if there's text before accessing parts[0]\n",
        "          if event.content.parts[0].text:\n",
        "               final_response_text = event.content.parts[0].text\n",
        "               print(f\"\\n[AFW: Sending Final Response to User]\")\n",
        "               print(f\"  -> Response: {final_response_text}\")\n",
        "          else:\n",
        "               print(f\"\\n[AFW: Final Response - Non-text content received: {event.content.parts}]\")\n",
        "               final_response_text = \"[Non-text final response]\"\n",
        "          # After agent callback will be triggered after this loop finishes internally\n",
        "\n",
        "  print(f\"\\n--- Workflow Finished for Query: '{query}' ---\")\n",
        "  # Retrieve final state to show callback modification\n",
        "  final_session = session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "  print(f\"Final Session State (example): ticket_id='{final_session.state.get('ticket_id')}', status='{final_session.state.get('processing_status')}'\")\n",
        "  return final_response_text\n",
        "\n",
        "\n",
        "await call_agent_and_show_flow(\"Help! My computer screen keeps flickering constantly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPo0-P_AwXm6"
      },
      "source": [
        "###  Quickstart session, states and SessionService\n",
        "\n",
        "- How to retreive session state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp3LOwJN3VVH",
        "outputId": "35f7180d-5965-466d-e940-bf30da2e32f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Session State: {}\n",
            "\n",
            "Agent Response:  Paris\n",
            "\n",
            "Final Session State: {'capital_city': 'Paris\\n'}\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types\n",
        "\n",
        "# --- Constants ---\n",
        "APP_NAME = \"capital_finder_app\"\n",
        "USER_ID = \"quickstart_user\"\n",
        "SESSION_ID = \"session_abc\"\n",
        "MODEL = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# Agent\n",
        "capital_agent = LlmAgent(\n",
        "    model=MODEL,\n",
        "    name=\"CapitalFinderAgent\",\n",
        "    instruction=\"\"\"You are an agent that finds the capital of a given country.\n",
        "    When asked for the capital, respond *only* with the name of the capital city.\n",
        "    \"\"\",\n",
        "    output_key=\"capital_city\" # Save the agent's final response text to state['capital_city']\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=capital_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "  for event in events:\n",
        "      if event.is_final_response():\n",
        "          final_response = event.content.parts[0].text\n",
        "          print(\"\\nAgent Response: \", final_response)\n",
        "\n",
        "initial_session = session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "print(f\"Initial Session State: {initial_session.state}\") # Should be empty {}\n",
        "\n",
        "call_agent(\"What is the capital of france?\")\n",
        "\n",
        "final_session = session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "print(f\"Final Session State: {final_session.state}\") # Should now contain {'capital_city': 'Paris'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl5zb-k4MTak"
      },
      "source": [
        "### Session State - State Manupilation\n",
        "\n",
        "- How to manually modify session state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jtLbWd9-cqih"
      },
      "outputs": [],
      "source": [
        "from google.adk.agents import LlmAgent\n",
        "from google.genai import types\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.events import Event, EventActions\n",
        "\n",
        "# --- Constants ---\n",
        "APP_NAME = \"task_manager_app\"\n",
        "USER_ID = \"test_user\"\n",
        "AGENT_NAME = \"task_manager_agent\"\n",
        "MODEL_NAME = \"gemini-2.0-flash-001\"  # Or any suitable model\n",
        "\n",
        "# --- Agent Definition ---\n",
        "#  Simplified instruction, as we're handling logic directly\n",
        "task_agent = LlmAgent(\n",
        "    model=MODEL_NAME,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Task Management Agent. Respond to user requests to manage tasks.\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def add_task(session, task_description):\n",
        "    \"\"\"Adds a task to the task list.\"\"\"\n",
        "    tasks = session.state.get(\"user:tasks\", [])  # Get current tasks (or empty list)\n",
        "    new_task_id = len(tasks) + 1\n",
        "    new_task = {\"id\": new_task_id, \"description\": task_description, \"status\": \"pending\"}\n",
        "    tasks.append(new_task)\n",
        "    # Use EventActions to update the state (delta update)\n",
        "    add_event = Event(author=\"agent\", actions=EventActions(state_delta={\"user:tasks\": tasks}))\n",
        "    session_service.append_event(session, add_event)\n",
        "    return f\"Task '{task_description}' added with ID {new_task_id}.\"\n",
        "\n",
        "def modify_task(session, task_id, new_status):\n",
        "    \"\"\"Modifies the status of a task.\"\"\"\n",
        "    tasks = session.state.get(\"user:tasks\", [])\n",
        "    try:\n",
        "        task_id = int(task_id)  # Ensure task_id is an integer\n",
        "    except ValueError:\n",
        "        return \"Invalid task ID. Please provide a number.\"\n",
        "\n",
        "    for i, task in enumerate(tasks):\n",
        "        if task[\"id\"] == task_id:\n",
        "            tasks[i][\"status\"] = new_status\n",
        "            # Update state via EventActions\n",
        "            modify_event = Event(author='agent', actions=EventActions(state_delta={\"user:tasks\": tasks}))\n",
        "            session_service.append_event(session, modify_event)\n",
        "            return f\"Task {task_id} status updated to '{new_status}'.\"\n",
        "    return f\"Task with ID {task_id} not found.\"\n",
        "\n",
        "\n",
        "def delete_task(session, task_id):\n",
        "    \"\"\"Deletes a task from the task list.\"\"\"\n",
        "    tasks = session.state.get(\"user:tasks\", [])\n",
        "    try:\n",
        "        task_id = int(task_id)\n",
        "    except ValueError:\n",
        "        return \"Invalid task ID.  Please provide a number.\"\n",
        "\n",
        "    updated_tasks = [task for task in tasks if task[\"id\"] != task_id]\n",
        "    if len(updated_tasks) < len(tasks):\n",
        "        # Update state via EventActions\n",
        "        delete_event = Event(author='agent', actions=EventActions(state_delta={\"user:tasks\": updated_tasks}))\n",
        "        session_service.append_event(session, delete_event)\n",
        "        return f\"Task {task_id} deleted.\"\n",
        "    return f\"Task with ID {task_id} not found.\"\n",
        "\n",
        "def list_tasks(session):\n",
        "    \"\"\"Lists all tasks for the user.\"\"\"\n",
        "    tasks = session.state.get(\"user:tasks\", [])\n",
        "    if not tasks:\n",
        "        return \"You have no tasks.\"\n",
        "    task_list_str = \"\\n\".join(\n",
        "        f\"{task['id']}: {task['description']} ({task['status']})\" for task in tasks\n",
        "    )\n",
        "    return f\"Your tasks:\\n{task_list_str}\"\n",
        "\n",
        "\n",
        "def call_agent(user_input, session):\n",
        "    \"\"\"Sends user input to the agent and processes events.\"\"\"\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=user_input)])\n",
        "    events = runner.run(user_id=USER_ID, session_id=session.id, new_message=content) #session.id, not SESSION_ID\n",
        "\n",
        "    final_response_text = \"\"\n",
        "    for event in events:\n",
        "      if event.content and event.content.role == 'model':\n",
        "          final_response_text = event.content.parts[0].text\n",
        "          break  # Exit loop after getting the final response.\n",
        "\n",
        "    return final_response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgLYUT1Ycqb_",
        "outputId": "0ac9b658-9f92-48d5-86ba-4c9b1b0906b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created session with ID: da1c3fe8-60a1-4b3d-9c0f-33eeca43baef\n"
          ]
        }
      ],
      "source": [
        "# 1. Create a Session (with initial state, if any)\n",
        "#  demonstrates: create_session, InMemorySessionService, initial state\n",
        "USER_ID = \"test_user2\"\n",
        "session_service = InMemorySessionService()\n",
        "\n",
        "session = session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID\n",
        ")  #  Let the service generate the ID\n",
        "\n",
        "print(f\"Created session with ID: {session.id}\")\n",
        "runner = Runner(agent=task_agent, app_name=APP_NAME, session_service=session_service)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k5wliFueuF_U",
        "outputId": "558911ac-d631-41d0-ff5f-f63e06d39f32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'da1c3fe8-60a1-4b3d-9c0f-33eeca43baef'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "session.id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G8I0MtT4Gfy",
        "outputId": "b2dba833-1a26-423a-cbf7-27ef7cecfbba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Retrieved session state:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieved_session = session_service.get_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=session.id\n",
        ")\n",
        "print(f\"\\nRetrieved session state:\")\n",
        "retrieved_session.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "tGg1H36a2k-8",
        "outputId": "8e47f9f9-2882-4e0e-ee5b-b3e625a9f25e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Adding tasks directly to state via Function...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Task 'Buy groceries' added with ID 4.\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"\\nAdding tasks directly to state via Function...\")\n",
        "\n",
        "add_task(session, \"Buy milk\")\n",
        "add_task(session, \"Walk the dog\")\n",
        "add_task(session, \"Prepare presentation\")\n",
        "add_task(session, \"Buy groceries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSK4sovb1OWx",
        "outputId": "3ef0996b-e931-4758-e456-1d0afdf2ee37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Retrieved session state:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'user:tasks': [{'id': 1, 'description': 'Buy milk', 'status': 'pending'},\n",
              "  {'id': 2, 'description': 'Walk the dog', 'status': 'pending'},\n",
              "  {'id': 3, 'description': 'Prepare presentation', 'status': 'pending'},\n",
              "  {'id': 4, 'description': 'Buy groceries', 'status': 'pending'}]}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3. Retrieve and display the session (demonstrates get_session)\n",
        "retrieved_session = session_service.get_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=session.id\n",
        ")\n",
        "print(f\"\\nRetrieved session state:\")\n",
        "retrieved_session.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUOEucZl1VdT",
        "outputId": "24dd38a5-7f5a-4dcc-c238-917d19f45a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Listing tasks...\n",
            "Your tasks:\n",
            "1: Buy milk (pending)\n",
            "2: Walk the dog (pending)\n",
            "3: Prepare presentation (pending)\n",
            "4: Buy groceries (pending)\n"
          ]
        }
      ],
      "source": [
        "# 4. List tasks (demonstrates accessing state)\n",
        "print(\"\\nListing tasks...\")\n",
        "print(list_tasks(retrieved_session))  # Using a helper, accessing state directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3EWU1Tb14Vn",
        "outputId": "f945c5dd-c3c7-4b64-9752-136ff6a74b3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Modifying task 2...\n",
            "Task 2 status updated to 'completed'.\n",
            "Modified session state:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'user:tasks': [{'id': 1, 'description': 'Buy milk', 'status': 'pending'},\n",
              "  {'id': 2, 'description': 'Walk the dog', 'status': 'completed'},\n",
              "  {'id': 3, 'description': 'Prepare presentation', 'status': 'pending'},\n",
              "  {'id': 4, 'description': 'Buy groceries', 'status': 'pending'}]}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 5. Modify a task (demonstrates state modification)\n",
        "print(\"\\nModifying task 2...\")\n",
        "print(modify_task(retrieved_session, \"2\", \"completed\"))\n",
        "retrieved_session = session_service.get_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=session.id\n",
        ")\n",
        "print(f\"Modified session state:\")\n",
        "retrieved_session.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yHQmUMB16c4",
        "outputId": "ab0f2fd1-1d4a-4831-84b6-cdc1fac331cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Deleting task 1...\n",
            "Task 1 deleted.\n"
          ]
        }
      ],
      "source": [
        "# 6. Delete a task (demonstrates state deletion)\n",
        "print(\"\\nDeleting task 1...\")\n",
        "print(delete_task(retrieved_session, \"1\"))\n",
        "retrieved_session = session_service.get_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=session.id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFHKk3zA18nz",
        "outputId": "3d287272-a29f-4b79-a465-3661acf1845f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session state after deletion:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'user:tasks': [{'id': 2,\n",
              "   'description': 'Walk the dog',\n",
              "   'status': 'completed'},\n",
              "  {'id': 3, 'description': 'Prepare presentation', 'status': 'pending'},\n",
              "  {'id': 4, 'description': 'Buy groceries', 'status': 'pending'}]}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Session state after deletion:\")\n",
        "retrieved_session.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTsLpN4a2Ddr",
        "outputId": "481bcb3b-d3cd-4580-c90d-2b15d8998f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sessions for user test_user2:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ListSessionsResponse(sessions=[Session(id='da1c3fe8-60a1-4b3d-9c0f-33eeca43baef', app_name='task_manager_app', user_id='test_user2', state={}, events=[], last_update_time=1746677605.497526)])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 7. List sessions for the user (demonstrates list_sessions)\n",
        "# Demonstrates:  list_sessions\n",
        "print(f\"\\nSessions for user {USER_ID}:\")\n",
        "session_service.list_sessions(app_name=APP_NAME, user_id=USER_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjZI2ZgK2GDc",
        "outputId": "6a2ded3d-6d11-49eb-8ad5-39783b63c9fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Deleted session: da1c3fe8-60a1-4b3d-9c0f-33eeca43baef\n"
          ]
        }
      ],
      "source": [
        "# 8. Delete a session (demonstrates delete_session).\n",
        "session_service.delete_session(app_name=APP_NAME, user_id=USER_ID, session_id=session.id)\n",
        "print(f\"\\nDeleted session: {session.id}\")\n",
        "retrieved_session = session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=session.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p4fVhCh2Iho",
        "outputId": "e6d7a072-80e2-422b-f951-6ec993a716bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session deleted successfully, as could not be retrieved.\n"
          ]
        }
      ],
      "source": [
        "if retrieved_session: # Should be None.\n",
        "    print(retrieved_session)\n",
        "else:\n",
        "    print(\"Session deleted successfully, as could not be retrieved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "648gwMHOMNWx"
      },
      "source": [
        "### Session State - delta_states (OPTIONAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbf_eXbYHfYp",
        "outputId": "c5a07f45-7747-4a69-d868-ffd6adde2af0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'order_status': 'shipped', 'items': ['shirt', 'pants'], 'tracking_number': 'XYZ456'}\n",
            "haha {}\n",
            "haha {}\n",
            "\n",
            "State with prefixes added:\n",
            "{'order_status': 'shipped', 'items': ['shirt', 'pants'], 'tracking_number': 'XYZ456', 'temp:request_id': 'temp_value'}\n",
            "\n",
            "State after retrieval (temp should be gone):\n",
            "{'order_status': 'pending', 'items': ['shirt'], 'notes': 'Initial order', 'app:max_retries': 3, 'user:pref_contact': 'email'}\n",
            "\n",
            "App State:\n",
            "{'my_app': {'max_retries': 3}}\n",
            "\n",
            "User State:\n",
            "{'my_app': {'user1': {'pref_contact': 'email'}}}\n"
          ]
        }
      ],
      "source": [
        "from google.adk.sessions import InMemorySessionService\n",
        "\n",
        "\n",
        "# Create the session service\n",
        "session_service = InMemorySessionService()\n",
        "\n",
        "# Create a session (with initial state)\n",
        "session = session_service.create_session(\n",
        "    app_name=\"my_app\",\n",
        "    user_id=\"user1\",\n",
        "    state={\"order_status\": \"pending\", \"items\": [\"shirt\"], \"notes\": \"Initial order\"},\n",
        ")\n",
        "\n",
        "# --- Direct State Manipulation (Generally NOT Recommended) ---\n",
        "\n",
        "# 1. Retrieve the session (to get the current state)\n",
        "retrieved_session = session_service.get_session(\n",
        "    app_name=\"my_app\", user_id=\"user1\", session_id=session.id\n",
        ")\n",
        "\n",
        "# 2. Access the state dictionary directly\n",
        "current_state = retrieved_session.state\n",
        "\n",
        "# --- Simulating a Delta Update (Directly) ---\n",
        "\n",
        "# Directly modify the state dictionary as if applying a state_delta:\n",
        "current_state[\"order_status\"] = \"shipped\"  # Update existing key\n",
        "current_state[\"tracking_number\"] = \"XYZ456\"  # Add a new key\n",
        "current_state[\"items\"].append(\"pants\")      # Modify a list (append)\n",
        "del current_state[\"notes\"]                   # Delete a key\n",
        "\n",
        "# The changes are reflected *immediately* in the retrieved_session (with InMemorySessionService)\n",
        "print(retrieved_session.state)\n",
        "\n",
        "# --- Demonstrating State Prefixes (Directly) ---\n",
        "# We need to use setdefault to properly initialize nested dictionaries.\n",
        "print(\"haha\", session_service.app_state.setdefault(\"my_app\", {}))\n",
        "print(\"haha\", session_service.user_state.setdefault(\"my_app\", {}).setdefault(\"user1\", {}))\n",
        "session_service.app_state.setdefault(\"my_app\", {})[\"max_retries\"] = 3  # app-level\n",
        "session_service.user_state.setdefault(\"my_app\", {}).setdefault(\"user1\", {})[\"pref_contact\"] = \"email\"\n",
        "current_state[\"temp:request_id\"] = \"temp_value\"\n",
        "\n",
        "print(\"\\nState with prefixes added:\")\n",
        "print(retrieved_session.state) #temp wont be there\n",
        "\n",
        "retrieved_session = session_service.get_session(\n",
        "    app_name=\"my_app\", user_id=\"user1\", session_id=session.id\n",
        ")\n",
        "print(\"\\nState after retrieval (temp should be gone):\")\n",
        "\n",
        "print(retrieved_session.state)\n",
        "\n",
        "print(\"\\nApp State:\")\n",
        "print(session_service.app_state)\n",
        "print(\"\\nUser State:\")\n",
        "print(session_service.user_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU_OHcRejR6j"
      },
      "source": [
        "### Accessing Session Properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUzzDIZyjSQL",
        "outputId": "8a0695ab-755c-4397-e368-ed751758c8fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Examining Session Properties ---\n",
            "ID (`id`):                94903a3c-7bbd-4267-98b2-e2f9e1315d9e\n",
            "Application Name (`app_name`): my_app\n",
            "User ID (`user_id`):         example_user\n",
            "State (`state`):           {'initial_value': 1}\n",
            "Events (`events`):         []\n",
            "Last Update (`last_update_time`): 1746679082.11\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "from google.adk.sessions import InMemorySessionService\n",
        "\n",
        "\n",
        "# Create a simple session to examine its properties\n",
        "temp_service = InMemorySessionService()\n",
        "example_session = temp_service.create_session(\n",
        "    app_name=\"my_app\",\n",
        "    user_id=\"example_user\",\n",
        "    state={\"initial_value\": 1}\n",
        ")\n",
        "\n",
        "print(f\"--- Examining Session Properties ---\")\n",
        "print(f\"ID (`id`):                {example_session.id}\")   # Unique identifier\n",
        "print(f\"Application Name (`app_name`): {example_session.app_name}\") # Which app it belongs to\n",
        "print(f\"User ID (`user_id`):         {example_session.user_id}\")    # Who the user is\n",
        "print(f\"State (`state`):           {example_session.state}\")       # The dynamic 'notes' dictionary\n",
        "print(f\"Events (`events`):         {example_session.events}\")      # The conversation history (initially empty)\n",
        "print(f\"Last Update (`last_update_time`): {example_session.last_update_time:.2f}\") # When it was last triggered\n",
        "print(f\"---------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYc-fk34FL2h"
      },
      "source": [
        "### Using InMemorySessionService Methods\n",
        "\n",
        "- Adding event\n",
        "- Update state with `state_delta`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOOfCY_k59Bt",
        "outputId": "4742d4d6-dc91-4682-8486-8f4624b13322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Demonstrating InMemorySessionService ---\n",
            "Created Session: ID=mem_session_1, State={'counter': 0}\n",
            "Appended Event, state['counter'] should be 1\n",
            "Retrieved Session: ID=mem_session_1, State={'counter': 1}\n",
            "Events in session: 2\n",
            "List Sessions for user_mem: sessions=[Session(id='mem_session_1', app_name='memory_app', user_id='user_mem', state={}, events=[], last_update_time=1746679479.206083)]\n",
            "Deleted Session: mem_session_1\n",
            "Retrieve after delete: Session not found (correct)\n",
            "------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example: Using InMemorySessionService Methods\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.events import Event, EventActions\n",
        "from google.genai import types\n",
        "import time\n",
        "import uuid\n",
        "\n",
        "print(\"\\n--- Demonstrating InMemorySessionService ---\")\n",
        "\n",
        "# 1. Instantiate\n",
        "session_service = InMemorySessionService()\n",
        "app_name, user_id = \"memory_app\", \"user_mem\"\n",
        "session_id = \"mem_session_1\"\n",
        "\n",
        "# 2. Create Session\n",
        "current_session = session_service.create_session(\n",
        "    app_name=app_name, user_id=user_id, session_id=session_id, state={\"counter\": 0}\n",
        ")\n",
        "print(f\"Created Session: ID={current_session.id}, State={current_session.state}\")\n",
        "\n",
        "# 3. Append Event with State Delta\n",
        "user_event = Event(\n",
        "    invocation_id=\"inv_1\", author=\"user\", content=types.Content(parts=[types.Part(text=\"Increment\")])\n",
        ")\n",
        "session_service.append_event(current_session, user_event)  # No state change yet\n",
        "\n",
        "agent_event = Event(\n",
        "    invocation_id=\"inv_2\", author=\"agent\",\n",
        "    actions=EventActions(state_delta={\"counter\": 1})  # Increment counter\n",
        ")\n",
        "session_service.append_event(current_session, agent_event)\n",
        "print(f\"Appended Event, state['counter'] should be 1\")\n",
        "\n",
        "# 4. Get Session\n",
        "retrieved_session = session_service.get_session(app_name=app_name, user_id=user_id, session_id=session_id)\n",
        "print(f\"Retrieved Session: ID={retrieved_session.id}, State={retrieved_session.state}\")\n",
        "print(f\"Events in session: {len(retrieved_session.events)}\")  # Shows 2 events were added\n",
        "\n",
        "# 5. List Sessions\n",
        "session_list = session_service.list_sessions(app_name=app_name, user_id=user_id)\n",
        "print(f\"List Sessions for {user_id}: {session_list}\")\n",
        "\n",
        "# 6. Delete Session\n",
        "session_service.delete_session(app_name=app_name, user_id=user_id, session_id=session_id)\n",
        "print(f\"Deleted Session: {session_id}\")\n",
        "\n",
        "# 7. Get Session (should fail)\n",
        "deleted_session = session_service.get_session(app_name=app_name, user_id=user_id, session_id=session_id)\n",
        "print(f\"Retrieve after delete: {'Session found (unexpected!)' if deleted_session else 'Session not found (correct)'}\")\n",
        "print(\"------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQRDTzpZFIBi"
      },
      "source": [
        "###  Using DatabaseSessionService Methods (with SQLite for demo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmah-tOJBgl0",
        "outputId": "aff0690d-d516-4c2c-8f72-44dc5db1974e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Demonstrating DatabaseSessionService (SQLite) ---\n",
            "Removed existing demo DB file: ./db_sessions_demo.db\n",
            "Instantiated DatabaseSessionService. DB file './db_sessions_demo.db' ensured/created.\n",
            "Created Session: ID=db_session_1, State={'status': 'new'}\n",
            "Appended Event, state should be updated in the database.\n",
            "Retrieved Session: ID=db_session_1, State={'status': 'processing', 'db_key': 'db_val'}\n",
            "List Sessions for user_db: sessions=[Session(id='db_session_1', app_name='db_app', user_id='user_db', state={}, events=[], last_update_time=1746650887.0)]\n",
            "List Events: Received NotImplementedError (as expected in base class)\n",
            "Deleted Session: db_session_1\n",
            "Retrieve after delete: Session not found (correct)\n",
            "--------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example: Using DatabaseSessionService Methods (with SQLite for demo)\n",
        "\n",
        "# NOTE: Requires `sqlalchemy` to be installed.\n",
        "# NOTE: This creates a file 'db_sessions_demo.db' in the current directory.\n",
        "from google.adk.sessions import DatabaseSessionService\n",
        "from google.adk.events import Event, EventActions\n",
        "from google.genai import types # Make sure types is imported\n",
        "import time\n",
        "import uuid\n",
        "import os # To manage the demo database file\n",
        "\n",
        "print(\"\\n--- Demonstrating DatabaseSessionService (SQLite) ---\")\n",
        "DB_FILE = \"./db_sessions_demo.db\" # Define path for the database file\n",
        "DB_DIR = os.path.dirname(DB_FILE) # Get directory path\n",
        "\n",
        "# Ensure the directory exists (useful if DB_FILE includes subdirectories)\n",
        "if DB_DIR and not os.path.exists(DB_DIR):\n",
        "    os.makedirs(DB_DIR)\n",
        "    print(f\"Created directory: {DB_DIR}\")\n",
        "\n",
        "# Remove the database file if it exists from a previous run for a clean demo\n",
        "if os.path.exists(DB_FILE):\n",
        "    os.remove(DB_FILE)\n",
        "    print(f\"Removed existing demo DB file: {DB_FILE}\")\n",
        "\n",
        "# 1. Instantiate (using SQLite file)\n",
        "# The DatabaseSessionService's __init__ method will handle DB/table creation.\n",
        "db_service = DatabaseSessionService(db_url=f\"sqlite:///{DB_FILE}\")\n",
        "print(f\"Instantiated DatabaseSessionService. DB file '{DB_FILE}' ensured/created.\")\n",
        "\n",
        "APP_DB, USER_DB = \"db_app\", \"user_db\"\n",
        "SESSION_ID_DB = \"db_session_1\"\n",
        "\n",
        "# 2. Create Session\n",
        "session_db = db_service.create_session(\n",
        "    app_name=APP_DB, user_id=USER_DB, session_id=SESSION_ID_DB, state={\"status\": \"new\"}\n",
        ")\n",
        "print(f\"Created Session: ID={session_db.id}, State={session_db.state}\")\n",
        "\n",
        "# 3. Append Event with State Delta\n",
        "# *** FIX: Ensure event has a 'content' object, even if minimal ***\n",
        "event_db_1 = Event(\n",
        "    invocation_id=\"inv_db1\", author=\"agent\",\n",
        "    content=types.Content(parts=[types.Part(text=\"System update: Processing\")]), # Add content\n",
        "    actions=EventActions(state_delta={\"status\": \"processing\", \"db_key\": \"db_val\"})\n",
        ")\n",
        "# Note: append_event updates the state in the DB and the passed session's last_update_time\n",
        "db_service.append_event(session_db, event_db_1)\n",
        "print(f\"Appended Event, state should be updated in the database.\")\n",
        "\n",
        "# 4. Get Session (re-fetch from DB to see persisted changes)\n",
        "# Note: Must re-fetch session to see DB changes reflected in the object state\n",
        "retrieved_session_db = db_service.get_session(app_name=APP_DB, user_id=USER_DB, session_id=SESSION_ID_DB)\n",
        "print(f\"Retrieved Session: ID={retrieved_session_db.id}, State={retrieved_session_db.state}\")\n",
        "# Note: Events are not automatically loaded by get_session in this implementation by default.\n",
        "\n",
        "# 5. List Sessions\n",
        "sessions_list_db = db_service.list_sessions(app_name=APP_DB, user_id=USER_DB)\n",
        "print(f\"List Sessions for {USER_DB}: {sessions_list_db}\")\n",
        "\n",
        "# 6. List Events (Not Implemented in DatabaseSessionService base implementation)\n",
        "try:\n",
        "    db_service.list_events(app_name=APP_DB, user_id=USER_DB, session_id=SESSION_ID_DB)\n",
        "except NotImplementedError as e:\n",
        "    print(f\"List Events: Received NotImplementedError (as expected in base class)\")\n",
        "except AttributeError as e:\n",
        "     print(f\"List Events: Method not found or not implemented (as expected in base class)\")\n",
        "\n",
        "# 7. Delete Session\n",
        "db_service.delete_session(app_name=APP_DB, user_id=USER_DB, session_id=SESSION_ID_DB)\n",
        "print(f\"Deleted Session: {SESSION_ID_DB}\")\n",
        "\n",
        "# 8. Get Session (should fail)\n",
        "deleted_session_check_db = db_service.get_session(app_name=APP_DB, user_id=USER_DB, session_id=SESSION_ID_DB)\n",
        "print(f\"Retrieve after delete: {'Session found (unexpected!)' if deleted_session_check_db else 'Session not found (correct)'}\")\n",
        "\n",
        "# Cleanup demo file (optional, good practice for demos)\n",
        "# if os.path.exists(DB_FILE):\n",
        "#     os.remove(DB_FILE)\n",
        "#     print(f\"Cleaned up demo DB file: {DB_FILE}\")\n",
        "print(\"--------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtgJPaV1Qw6j"
      },
      "source": [
        "### LlMAgent with Antrhopic (3rd Party Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptc7IKdrQxLx",
        "outputId": "0a39001a-a64f-4639-b09c-34f581413e08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running Anthropic Agent Example ---\n",
            "Using model: claude-3-7-sonnet@20250219\n",
            "Make sure GCP Project 'hello-world-418507' and Location 'us-east5' are correct and the model is available there.\n",
            "Ensure you are authenticated with GCP (e.g., `gcloud auth application-default login`).\n",
            "\n",
            "User Query: What is the capital of France?\n",
            "Agent Response: Paris\n",
            "\n",
            "User Query: What's the capital of Japan?\n",
            "Agent Response: Tokyo\n",
            "\n",
            "User Query: Tell me the capital of Germany.\n",
            "Agent Response: Berlin\n",
            "--- Example Finished ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import asyncio\n",
        "import warnings\n",
        "\n",
        "# --- GCP/Vertex AI Configuration ---\n",
        "# Make sure these are set in your environment OR uncomment and set here\n",
        "# os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"your-project-id\"\n",
        "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = \"us-east5\" # Make sure its us-east5 or europe-west1\n",
        "\n",
        "# --- ADK Imports ---\n",
        "from google.adk.agents import Agent, LlmAgent # Using LlmAgent directly\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types\n",
        "from google.adk.models.anthropic_llm import Claude\n",
        "from google.adk.models.registry import LLMRegistry\n",
        "\n",
        "# Manually register the Claude model class with the registry\n",
        "# This step is crucial if the framework doesn't do it automatically\n",
        "LLMRegistry.register(Claude)\n",
        "\n",
        "\n",
        "# --- Constants ---\n",
        "APP_NAME = \"anthropic_capital_app\"\n",
        "USER_ID = \"anthropic_user\"\n",
        "SESSION_ID = \"anthropic_session_1\"\n",
        "AGENT_NAME = \"claude_capital_agent\"\n",
        "# --- Anthropic Model Name (via Vertex AI) ---\n",
        "# Use the model identifier available in your Vertex AI region.\n",
        "# Example: Claude 3.5 Sonnet. Check Vertex AI documentation for available models.\n",
        "ANTHROPIC_MODEL = \"claude-3-7-sonnet@20250219\"\n",
        "# You might need to adjust this based on availability in your specific GCP project/location.\n",
        "# Other possibilities: \"claude-3-haiku@...\", \"claude-3-opus@...\"\n",
        "\n",
        "# --- Agent Definition ---\n",
        "# Simplest agent: takes user input, uses the LLM directly to answer.\n",
        "capital_agent = LlmAgent(\n",
        "    model=ANTHROPIC_MODEL, # Specify the Anthropic model identifier\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"You are a helpful assistant. When asked for the capital of a country, provide only the name of the capital city.\",\n",
        "    description=\"An agent that provides the capital city of a country using an Anthropic model.\",\n",
        "    # No tools needed for this simple task\n",
        "    tools=[],\n",
        ")\n",
        "\n",
        "# --- Session and Runner Setup ---\n",
        "session_service = InMemorySessionService()\n",
        "# Ensure session is created before running\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=capital_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "# --- Agent Interaction Logic ---\n",
        "# Using async version as it's preferred\n",
        "async def call_agent_async(query):\n",
        "  print(f\"\\nUser Query: {query}\")\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  final_response_text = \"Agent did not produce a final response.\"\n",
        "  try:\n",
        "      async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
        "          if event.is_final_response() and event.content and event.content.parts:\n",
        "              final_response_text = event.content.parts[0].text\n",
        "              print(f\"Agent Response: {final_response_text}\")\n",
        "              # Break after final response for simplicity in this example\n",
        "              break\n",
        "          elif event.error_message:\n",
        "              final_response_text = f\"Agent Error: {event.error_message}\"\n",
        "              print(final_response_text)\n",
        "              break\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred during agent execution: {e}\")\n",
        "      final_response_text = f\"Execution Error: {e}\"\n",
        "\n",
        "  return final_response_text\n",
        "\n",
        "# --- Example Usage ---\n",
        "async def run_example():\n",
        "    print(\"--- Running Anthropic Agent Example ---\")\n",
        "    print(f\"Using model: {ANTHROPIC_MODEL}\")\n",
        "    print(f\"Make sure GCP Project '{os.environ['GOOGLE_CLOUD_PROJECT']}' and Location '{os.environ['GOOGLE_CLOUD_LOCATION']}' are correct and the model is available there.\")\n",
        "    print(\"Ensure you are authenticated with GCP (e.g., `gcloud auth application-default login`).\")\n",
        "\n",
        "    await call_agent_async(\"What is the capital of France?\")\n",
        "    await call_agent_async(\"What's the capital of Japan?\")\n",
        "    await call_agent_async(\"Tell me the capital of Germany.\")\n",
        "    print(\"--- Example Finished ---\")\n",
        "\n",
        "await run_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFLdX7tcIRuz"
      },
      "source": [
        "### Artifact Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.adk.runners import Runner\n",
        "from google.adk.artifacts import InMemoryArtifactService  # Or GcsArtifactService\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.tools import ToolContext\n",
        "\n",
        "def save_dummy_report(context: ToolContext):\n",
        "    \"\"\"\n",
        "    Saves dummy PDF report bytes as an artifact.\n",
        "    \n",
        "    Creates a PDF artifact from raw bytes and stores it using the artifact service.\n",
        "    The saved artifact will be accessible via the artifact_delta in the next event.\n",
        "    \n",
        "    Args:\n",
        "        context: The callback context providing access to artifact storage methods.\n",
        "        \n",
        "    Returns:\n",
        "        str: \"Done\" if the artifact was successfully saved.\n",
        "        \n",
        "    Raises:\n",
        "        ValueError: If the artifact service is not properly configured.\n",
        "        Exception: For any other unexpected errors during storage.\n",
        "    \"\"\"\n",
        "    pdf_bytes = b'%PDF-1.4...' # Your raw PDF data\n",
        "\n",
        "    report_artifact = types.Part.from_data(\n",
        "        data=pdf_bytes, mime_type=\"application/pdf\"\n",
        "    )\n",
        "    filename = \"generated_report.pdf\"\n",
        "\n",
        "    try:\n",
        "        version = context.save_artifact(filename=filename, artifact=report_artifact)\n",
        "        print(f\"Successfully saved artifact '{filename}' as version {version}.\")\n",
        "        # The event generated after this callback will contain:\n",
        "        # event.actions.artifact_delta == {\"generated_report.pdf\": version}\n",
        "        return \"Done\"\n",
        "    except ValueError as e:\n",
        "        print(f\"Error saving artifact: {e}. Is ArtifactService configured?\")\n",
        "    except Exception as e:\n",
        "        # Handle potential storage errors (e.g., GCS permissions)\n",
        "        print(f\"An unexpected error occurred during artifact save: {e}\")\n",
        "\n",
        "\n",
        "def load_dummy_report(context: ToolContext):\n",
        "    \"\"\"\n",
        "    Loads the dummy report artifact from storage.\n",
        "    \n",
        "    Retrieves the latest version of the PDF report artifact from the artifact service.\n",
        "    Provides information about the loaded artifact including MIME type and size.\n",
        "    Returns the raw PDF bytes for further processing.\n",
        "    \n",
        "    Args:\n",
        "        context: The callback context providing access to artifact loading methods.\n",
        "        \n",
        "    Returns:\n",
        "        bytes: The raw PDF data if successfully loaded, None otherwise.\n",
        "        \n",
        "    Raises:\n",
        "        ValueError: If the artifact service is not properly configured.\n",
        "        Exception: For any other unexpected errors during retrieval.\n",
        "    \"\"\"\n",
        "    filename = \"generated_report.pdf\"\n",
        "    try:\n",
        "        # Load the latest version\n",
        "        report_artifact = context.load_artifact(filename=filename)\n",
        "\n",
        "        if report_artifact and report_artifact.inline_data:\n",
        "            print(f\"Successfully loaded latest artifact '{filename}'.\")\n",
        "            print(f\"MIME Type: {report_artifact.inline_data.mime_type}\")\n",
        "            # Process the report_artifact.inline_data.data (bytes)\n",
        "            pdf_bytes = report_artifact.inline_data.data\n",
        "            print(f\"Report size: {len(pdf_bytes)} bytes.\")\n",
        "            return report_artifact.inline_data.data\n",
        "        else:\n",
        "            print(f\"Artifact '{filename}' not found.\")\n",
        "            return None\n",
        "        # Example: Load a specific version (if version 0 exists)\n",
        "        # specific_version_artifact = context.load_artifact(filename=filename, version=0)\n",
        "        # if specific_version_artifact:\n",
        "        #     print(f\"Loaded version 0 of '{filename}'.\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Error loading artifact: {e}. Is ArtifactService configured?\")\n",
        "    except Exception as e:\n",
        "        # Handle potential storage errors\n",
        "        print(f\"An unexpected error occurred during artifact load: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-7 (_asyncio_thread_main):\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/weiyih/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/Users/weiyih/work/temp/adk-practise/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
            "    _threading_Thread_run(self)\n",
            "  File \"/Users/weiyih/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/Users/weiyih/work/temp/adk-practise/.venv/lib/python3.11/site-packages/google/adk/runners.py\", line 138, in _asyncio_thread_main\n",
            "    asyncio.run(_invoke_run_async())\n",
            "  File \"/Users/weiyih/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/asyncio/runners.py\", line 190, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/weiyih/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/weiyih/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n",
            "    return future.result()\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/Users/weiyih/work/temp/adk-practise/.venv/lib/python3.11/site-packages/google/adk/runners.py\", line 126, in _invoke_run_async\n",
            "    async for event in self.run_async(\n",
            "  File \"/Users/weiyih/work/temp/adk-practise/.venv/lib/python3.11/site-packages/google/adk/runners.py\", line 197, in run_async\n",
            "    async for event in invocation_context.agent.run_async(invocation_context):\n",
            "  File \"/Users/weiyih/work/temp/adk-practise/.venv/lib/python3.11/site-packages/google/adk/agents/base_agent.py\", line 141, in run_async\n",
            "    async for event in self._run_async_impl(ctx):\n",
            "  File \"/Users/weiyih/work/temp/adk-practise/.venv/lib/python3.11/site-packages/google/adk/agents/llm_agent.py\", line 227, in _run_async_impl\n",
            "    async for event in self._llm_flow.run_async(ctx):\n",
            "  File \"/Users/weiyih/work/temp/adk-practise/.venv/lib/python3.11/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 231, in run_async\n",
            "    async for event in self._run_one_step_async(invocation_context):\n",
            "  File \"/Users/weiyih/work/temp/adk-practise/.venv/lib/python3.11/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 245, in _run_one_step_async\n",
            "    async for event in self._preprocess_async(invocation_context, llm_request):\n",
            "  File \"/Users/weiyih/work/temp/adk-practise/.venv/lib/python3.11/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 283, in _preprocess_async\n",
            "    await tool.process_llm_request(\n",
            "  File \"/Users/weiyih/work/temp/adk-practise/.venv/lib/python3.11/site-packages/google/adk/tools/base_tool.py\", line 96, in process_llm_request\n",
            "    if (function_declaration := self._get_declaration()) is None:\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/weiyih/work/temp/adk-practise/.venv/lib/python3.11/site-packages/google/adk/tools/function_tool.py\", line 42, in _get_declaration\n",
            "    build_function_declaration(\n",
            "  File \"/Users/weiyih/work/temp/adk-practise/.venv/lib/python3.11/site-packages/google/adk/tools/_automatic_function_calling_util.py\", line 232, in build_function_declaration\n",
            "    from_function_with_options(func, variant)\n",
            "  File \"/Users/weiyih/work/temp/adk-practise/.venv/lib/python3.11/site-packages/google/adk/tools/_automatic_function_calling_util.py\", line 309, in from_function_with_options\n",
            "    schema = function_parameter_parse_util._parse_schema_from_parameter(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/weiyih/work/temp/adk-practise/.venv/lib/python3.11/site-packages/google/adk/tools/function_parameter_parse_util.py\", line 292, in _parse_schema_from_parameter\n",
            "    raise ValueError(\n",
            "ValueError: Failed to parse the parameter context: google.adk.tools.tool_context.ToolContext of function save_dummy_report for automatic function calling. Automatic function calling works best with simpler function signature schema,consider manually parse your function declaration for function save_dummy_report.\n"
          ]
        }
      ],
      "source": [
        "from google.adk.sessions import InMemorySessionService\n",
        "import uuid\n",
        "from google.adk.runners import Runner\n",
        "\n",
        "session_service = InMemorySessionService()\n",
        "artifact_service = InMemoryArtifactService()\n",
        "\n",
        "# Required. Unique identifier for the application.\n",
        "APP_NAME = \"weather_app\"\n",
        "# Required. Identifier for the user interacting with the agent. This is a dynamic variable.\n",
        "USER_ID = \"12345\"\n",
        "\n",
        "SESSION_ID = f\"session_{uuid.uuid4()}\"  # Use a dynamic session ID\n",
        "session = session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "\n",
        "# Your agent definition\n",
        "root_agent = LlmAgent(\n",
        "    name=\"my_agent\",\n",
        "    model=GEMINI_2_FLASH,\n",
        "    tools=[save_dummy_report, load_dummy_report],\n",
        "    instruction=\"You are a helpful assistant that saves a dummy PDF report as an artifact, and then read it back to user.\",\n",
        ")\n",
        "\n",
        "runner = Runner(\n",
        "    agent=root_agent,\n",
        "    app_name=APP_NAME,\n",
        "    session_service=session_service,\n",
        "    artifact_service=artifact_service\n",
        ")\n",
        "\n",
        "\n",
        "def call_agent(user_query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=user_query)])\n",
        "    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "    for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n",
        "\n",
        "\n",
        "call_agent(\"load and retrieve the dummy report\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4eum1l5eLhI"
      },
      "source": [
        "## Non-LLM Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2hOcAEld7oT"
      },
      "source": [
        "### Multi-Agent - SequenceAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XihTsvVHU8v",
        "outputId": "f40af520-c4d0-4742-dc4b-0415c19ede01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from uuid import uuid4\n",
        "from google.adk.agents.base_agent import BaseAgent\n",
        "from google.adk.agents.sequential_agent import SequentialAgent\n",
        "from google.adk.agents.invocation_context import InvocationContext, new_invocation_context_id\n",
        "from google.adk.events import Event\n",
        "from typing_extensions import override\n",
        "from google.adk.sessions.in_memory_session_service import InMemorySessionService\n",
        "from google.adk.sessions.session import Session\n",
        "from google.genai import types\n",
        "from typing import AsyncGenerator\n",
        "\n",
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "async def main():\n",
        "\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the SequentialAgent\n",
        "    sequential_agent = SequentialAgent(name=\"SequentialAgent\", sub_agents=[agent_a, agent_b])\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=sequential_agent,\n",
        "        user_content=types.Content(\n",
        "                parts=[types.Part(text=\"execute\")]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Run the SequentialAgent\n",
        "    async for event in sequential_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ZyIVLGnaPG"
      },
      "source": [
        "### Passing state between Children"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfr_cRxsXSo_",
        "outputId": "d8d20543-b2e2-4841-8958-cd7dfc16e717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Received value: Hello from Agent A!\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from uuid import uuid4\n",
        "from google.adk.agents.base_agent import BaseAgent\n",
        "from google.adk.agents.sequential_agent import SequentialAgent\n",
        "from google.adk.agents.invocation_context import InvocationContext, new_invocation_context_id\n",
        "from google.adk.events import Event\n",
        "from typing_extensions import override\n",
        "from google.adk.sessions.in_memory_session_service import InMemorySessionService\n",
        "from google.adk.sessions.session import Session\n",
        "from google.genai import types\n",
        "from typing import AsyncGenerator\n",
        "\n",
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        # Set a value in the session state\n",
        "        ctx.session.state[\"agent_a_value\"] = \"Hello from Agent A!\"\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        # Retrieve the value from the session state\n",
        "        agent_a_value = ctx.session.state.get(\"agent_a_value\")\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=f\"Agent B: Received value: {agent_a_value}\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "async def main():\n",
        "\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the SequentialAgent\n",
        "    sequential_agent = SequentialAgent(name=\"SequentialAgent\", sub_agents=[agent_a, agent_b])\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=sequential_agent,\n",
        "        user_content=types.Content(\n",
        "                parts=[types.Part(text=\"execute\")]\n",
        "            )\n",
        "\n",
        "        )\n",
        "\n",
        "    # Run the SequentialAgent\n",
        "    async for event in sequential_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDROM_P-Alpt"
      },
      "source": [
        "### Sequence with LLMAgent and simple runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPHz5Cyv_z8F",
        "outputId": "5aed0837-587e-4770-f28c-61f7c30e5518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Agent A: Starting...\n",
            "\n",
            "Agent Response:  Agent B: Starting...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents.sequential_agent import SequentialAgent\n",
        "from google.adk.agents.llm_agent import LlmAgent\n",
        "from google.genai import types\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "\n",
        "\n",
        "APP_NAME = \"sequential_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"sequential_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "\n",
        "agent_a = LlmAgent(\n",
        "        name=\"AgentA\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are Agent A. Respond with 'Agent A: Starting...'\",\n",
        "\t\toutput_key = \"agent_a\"\n",
        "    )\n",
        "\n",
        "agent_b = LlmAgent(\n",
        "        name=\"AgentB\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are Agent B. Respond with 'Agent B: Starting...'\",\n",
        "    )\n",
        "\n",
        "# Create the SequentialAgent\n",
        "sequential_agent = SequentialAgent(\n",
        "        name=\"SequentialAgent\", sub_agents=[agent_a, agent_b]\n",
        "    )\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=sequential_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "  for event in events:\n",
        "      if event.is_final_response():\n",
        "          final_response = event.content.parts[0].text\n",
        "          print(\"Agent Response: \", final_response)\n",
        "\n",
        "call_agent(\"execute\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVS5bHjqO0sD"
      },
      "source": [
        "## Multi-Agent - LoopAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrsvPMqDAu79"
      },
      "source": [
        "### LoopAgent with Simple Runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUYDIdsxAuvu",
        "outputId": "ce93c375-a026-44bb-d48f-99e434763d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Agent A: Starting...\n",
            "\n",
            "Agent Response:  Agent B: Starting...\n",
            "\n",
            "Agent Response:  Agent A: Acknowledged. Agent B is starting. I will await further instructions or input.\n",
            "\n",
            "Agent Response:  Agent B: Acknowledged. Ready to receive instructions or provide assistance.\n",
            "\n",
            "Agent Response:  Agent A: Understood. Agent B is ready. I am also ready.\n",
            "\n",
            "Agent Response:  Agent B: Understood. Awaiting further instructions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents.loop_agent import LoopAgent\n",
        "from google.adk.agents.llm_agent import LlmAgent\n",
        "from google.genai import types\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "\n",
        "\n",
        "APP_NAME = \"loop_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"loop_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "agent_a = LlmAgent(\n",
        "        name=\"AgentA\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are Agent A. Respond with 'Agent A: Starting...'\",\n",
        "    )\n",
        "\n",
        "agent_b = LlmAgent(\n",
        "        name=\"AgentB\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are Agent B. Respond with 'Agent B: Starting...'\",\n",
        "    )\n",
        "\n",
        "# Create the LoopAgent\n",
        "loop_agent = LoopAgent(\n",
        "    name=\"LoopAgent\", sub_agents=[agent_a, agent_b], max_iterations=3\n",
        ")\n",
        "\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=loop_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "  for event in events:\n",
        "      if event.is_final_response():\n",
        "          final_response = event.content.parts[0].text\n",
        "          print(\"Agent Response: \", final_response)\n",
        "\n",
        "call_agent(\"execute\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxOrUPQkO34F"
      },
      "source": [
        "### LoopAgent with InnvocationContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCaz2BtMO4Nu",
        "outputId": "9277dab0-af9e-4f5d-c038-e698e90995fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n",
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents.loop_agent import LoopAgent\n",
        "\n",
        "\n",
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the LoopAgent\n",
        "    loop_agent = LoopAgent(name=\"LoopAgent\", sub_agents=[agent_a, agent_b], max_iterations=2)\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=loop_agent,\n",
        "        user_content=types.Content(\n",
        "            parts=[types.Part(text=\"execute\")]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Run the LoopAgent\n",
        "    async for event in loop_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcBLz-NZPHvi"
      },
      "source": [
        "### Stop Condition with EventActions=Escalation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J42_PTYgPJQN",
        "outputId": "ed275827-c79a-498f-f5bc-cfcb37987704"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents.loop_agent import LoopAgent\n",
        "from google.adk.events.event_actions import EventActions\n",
        "\n",
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Finishing...\")]\n",
        "            ),\n",
        "            actions=EventActions(escalate=True),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the LoopAgent\n",
        "    loop_agent = LoopAgent(name=\"LoopAgent\", sub_agents=[agent_a, agent_b], max_iterations=10)\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=loop_agent,\n",
        "        user_content=types.Content(\n",
        "            parts=[types.Part(text=\"execute\")]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Run the LoopAgent\n",
        "    async for event in loop_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21opVatuSEME"
      },
      "source": [
        "### Escalation with Condition - defined in state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOCPRwuZSEfD",
        "outputId": "c86d0a6d-4e7b-447f-ad79-ec957d34e8d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Example condition: Escalate if session state has a key 'escalate_agent_b'\n",
        "        escalate = ctx.session.state.get('escalate_agent_b', False)\n",
        "\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Finishing...\")]\n",
        "            ),\n",
        "            actions=EventActions(escalate=escalate),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the LoopAgent\n",
        "    loop_agent = LoopAgent(name=\"LoopAgent\", sub_agents=[agent_a, agent_b], max_iterations=3)\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=loop_agent,\n",
        "        user_content=types.Content(\n",
        "            parts=[types.Part(text=\"execute\")]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Example: Set a condition to escalate\n",
        "    ctx.session.state['escalate_agent_b'] = True\n",
        "\n",
        "    # Run the LoopAgent\n",
        "    async for event in loop_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu7fJ5QIUECc"
      },
      "source": [
        "## ParallelAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3iVDqUdUGdx"
      },
      "source": [
        "### Simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgqJ9rzxUGo_",
        "outputId": "6c2db6e1-53a4-4de6-ef4a-f45c741b3929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents.parallel_agent import ParallelAgent\n",
        "\n",
        "\n",
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        await asyncio.sleep(1)\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        await asyncio.sleep(2)\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the ParallelAgent\n",
        "    parallel_agent = ParallelAgent(name=\"ParallelAgent\", sub_agents=[agent_a, agent_b])\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=parallel_agent,\n",
        "        user_content=types.Content(\n",
        "            parts=[types.Part(text=\"execute\")]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Run the ParallelAgent\n",
        "    async for event in parallel_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2iStmHFUcRj"
      },
      "source": [
        "### Shared State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1EZZfy4UdpK",
        "outputId": "f85a2373-550b-4210-b8dc-addea1823e20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Finishing... Received: Data from Agent A\n"
          ]
        }
      ],
      "source": [
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        await asyncio.sleep(1)\n",
        "        ctx.session.state['shared_data'] = \"Data from Agent A\"\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        await asyncio.sleep(2)\n",
        "        shared_data = ctx.session.state.get('shared_data', \"No data from Agent A\")\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=f\"Agent B: Finishing... Received: {shared_data}\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the ParallelAgent\n",
        "    parallel_agent = ParallelAgent(name=\"ParallelAgent\", sub_agents=[agent_a, agent_b])\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=parallel_agent,\n",
        "        user_content=types.Content(\n",
        "            parts=[types.Part(text=\"execute\")]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Run the ParallelAgent\n",
        "    async for event in parallel_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vfkz0jeeobT"
      },
      "source": [
        "## CustomAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb4lAAD4eo77",
        "outputId": "f72e46b3-fcb4-464d-9fda-0e0eee87e240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: Hello from Custom Agent!\n"
          ]
        }
      ],
      "source": [
        "class CustomAgent(BaseAgent):\n",
        "    \"\"\"A custom agent that generates a simple text message.\"\"\"\n",
        "\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            invocation_id=ctx.invocation_id,\n",
        "            author=self.name,\n",
        "            content=types.Content(\n",
        "                parts=[types.Part.from_text(text=\"Hello from Custom Agent!\")],\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a custom agent instance\n",
        "    custom_agent = CustomAgent(name=\"CustomAgent\", description=\"A custom agent\")\n",
        "\n",
        "    # Create a session service\n",
        "    session_service = InMemorySessionService()\n",
        "\n",
        "    # Create a session\n",
        "    session = session_service.create_session(\n",
        "        app_name=\"demo_app\", user_id=\"test_user\", session_id=\"test_session\"\n",
        "    )\n",
        "\n",
        "    # Create a runner instance\n",
        "    runner = Runner(\n",
        "        app_name=\"demo_app\",\n",
        "        agent=custom_agent,\n",
        "        session_service=session_service,\n",
        "    )\n",
        "\n",
        "    # Run the agent\n",
        "    async for event in runner.run_async(\n",
        "        user_id=\"test_user\",\n",
        "        session_id=\"test_session\",\n",
        "        new_message=types.Content(\n",
        "            parts=[types.Part.from_text(text=\"Hi, how are you?\")]\n",
        "        ),\n",
        "    ):\n",
        "        print(f\"Event: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
