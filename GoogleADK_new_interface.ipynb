{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKLHbmSvZabn"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xDf0wgIR_cI",
        "outputId": "02bb31bf-a982-4ea6-b6eb-b1ad19f95174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RELEASE_VERSION: google_adk-0.0.2.dev20250326+nightly740999296-py3-none-any.whl\n",
            "Copying gs://agent_framework/latest/google_adk-0.0.2.dev20250326+nightly740999296-py3-none-any.whl...\n",
            "/ [1 files][  1.2 MiB/  1.2 MiB]                                                \n",
            "Operation completed over 1 objects/1.2 MiB.                                      \n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.4/231.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "# Define the folder where latest .whl file exists\n",
        "FOLDER = \"gs://agent_framework/latest/\"\n",
        "\n",
        "# Execute gsutil command to list files in the folder\n",
        "command_output = subprocess.check_output(['gsutil', 'ls', FOLDER]).decode('utf-8')\n",
        "file_list = command_output.strip().split('\\n')\n",
        "\n",
        "# Find the .whl file\n",
        "whl_file = next((file for file in file_list if file.endswith('.whl')), None)\n",
        "\n",
        "if whl_file:\n",
        "    RELEASE_VERSION = whl_file.split('/')[-1]\n",
        "    print(f\"RELEASE_VERSION: {RELEASE_VERSION}\")\n",
        "else:\n",
        "    print(\"No .whl file found!\")\n",
        "\n",
        "!gsutil cp {FOLDER}{RELEASE_VERSION} .\n",
        "!pip3 install {RELEASE_VERSION} --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lc8ZKtaz4ATx"
      },
      "outputs": [],
      "source": [
        "# # Make sure you have .whl downlaoded from:\n",
        "# # https://drive.google.com/file/d/1WtfjKtbV66aiZP_k0ncZoY-4_ON4sP7Q/view?usp=drive_link\n",
        "# !pip3 install google_adk-0.0.2.dev20250324+739344859-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNA-Vyhu4wMk"
      },
      "source": [
        "`Make sure to restart the session`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9y2tcqvOVWt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"lavi-llm-experiment\"  # @param {type:\"string\"}\n",
        "if not PROJECT_ID:\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
        "\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
        "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\" # Use Vertex AI API\n",
        "# [your-project-id]\n",
        "\n",
        "# If you want to run Antropic Models, make sure the location is:\n",
        "# us-east5 or europe-west1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q01TsO_Uf2jp"
      },
      "outputs": [],
      "source": [
        "# # Temp - to supress WARNING:google_genai.types:\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4YxlyEOC5ki"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='google.generativeai.types.content_types') # Suppress harmless warning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7BJtAoQxGUI"
      },
      "outputs": [],
      "source": [
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rYg3X5YZhBV"
      },
      "source": [
        "## LLMAgent/Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtWWC0ILRZ7a"
      },
      "outputs": [],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.tools import built_in_google_search  # Import the tool\n",
        "\n",
        "root_agent = Agent(\n",
        "   # A unique name for the agent.\n",
        "   name=\"basic_search_agent\",\n",
        "   # The Large Language Model (LLM) that agent will use.\n",
        "   model=\"gemini-2.0-flash-001\",\n",
        "   # A short description of the agent's purpose.\n",
        "   description=\"Agent to answer questions using Google Search.\",\n",
        "   # Instructions to set the agent's behavior.\n",
        "   instruction=\"You are an expert researcher. You always stick to the facts.\",\n",
        "   # Add google_search tool to perform grounding with Google search.\n",
        "   tools=[built_in_google_search]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7imWwMPDRZey"
      },
      "outputs": [],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.tools import built_in_google_search  # Import the tool\n",
        "\n",
        "root_agent = Agent(\n",
        "   # A unique name for the agent.\n",
        "   name=\"basic_search_agent\",\n",
        "   # The Large Language Model (LLM) that agent will use.\n",
        "   model=\"gemini-2.0-flash-001\",\n",
        "   # A short description of the agent's purpose.\n",
        "   description=\"Agent to answer questions using Google Search.\",\n",
        "   # Instructions to set the agent's behavior.\n",
        "   instruction=\"You are an expert researcher. You always stick to the facts.\",\n",
        "   # Add google_search tool to perform grounding with Google search.\n",
        "   tools=[built_in_google_search]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73qeFgix6d0x"
      },
      "source": [
        "### LLM Agent with a Single Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp2U_FdrIl_d",
        "outputId": "f7c78468-2a5f-46ca-9bbf-3fcafcb92aec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  The capital of France is Paris.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import LlmAgent\n",
        "from google.genai import types\n",
        "from pydantic import BaseModel\n",
        "from google.adk.agents import Agent\n",
        "from google.genai import types\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"capital_city_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"capital_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "\n",
        "# Define a simple tool\n",
        "def get_capital_city(country: str) -> str:\n",
        "    \"\"\"Retrieves the capital city of a given country.\n",
        "\n",
        "    Args:\n",
        "        country: The name of the country.\n",
        "\n",
        "    Returns:\n",
        "        The capital city of the country.\n",
        "    \"\"\"\n",
        "    country_capitals = {\n",
        "        \"united states\": \"washington, d.c.\",\n",
        "        \"canada\": \"ottawa\",\n",
        "        \"france\": \"paris\",\n",
        "    }\n",
        "    return country_capitals.get(country.lower(), \"Capital not found\")\n",
        "\n",
        "\n",
        "# Agent\n",
        "capital_agent = LlmAgent(\n",
        "    model=\"gemini-2.0-flash-001\",\n",
        "    name=\"capital_agent\",\n",
        "    description=\"An agent that can retrieve the capital city of a country.\",\n",
        "    instruction=\"\"\"You are an agent that can retrieve the capital city of a country.\n",
        "    When a user provides a prompt, extract the country name.\n",
        "    Then, use the `get_capital_city` tool to retrieve the capital city for that country.\n",
        "    Finally, present the capital city to the user in a clear and concise manner.\n",
        "    \"\"\",\n",
        "    tools=[get_capital_city],\n",
        "    generate_content_config=types.GenerateContentConfig(\n",
        "        max_output_tokens=100,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=capital_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "  for event in events:\n",
        "      if event.is_final_response():\n",
        "          final_response = event.content.parts[0].text\n",
        "          print(\"Agent Response: \", final_response)\n",
        "\n",
        "call_agent(\"What is the capital of france?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-T0OO4K6WMn"
      },
      "source": [
        "### Simple LLM Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxPLIuhAW-eu",
        "outputId": "e1cf5c33-32bd-4151-85ae-112ad8aedcc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  The capital of France is Paris.\n",
            "\n",
            "Agent Response:  The capital of Germany is Berlin.\n",
            "\n",
            "Agent Response:  The capital of Japan is Tokyo.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import LlmAgent\n",
        "from google.genai import types\n",
        "from pydantic import BaseModel\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"capital_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"capital_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# Agent\n",
        "capital_agent = Agent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Capital Information Agent. Your task is to provide the capital of a given city.\n",
        "\n",
        "    When a user provides a prompt, extract the city name.\n",
        "    Then, respond with the capital of that city.\n",
        "    \"\"\",\n",
        "    description=\"\"\"You are an agent who can tell the capital of a city.\"\"\",\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=capital_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "  for event in events:\n",
        "      if event.is_final_response():\n",
        "          final_response = event.content.parts[0].text\n",
        "          print(\"Agent Response: \", final_response)\n",
        "\n",
        "call_agent(\"What's the capital of France?\")\n",
        "call_agent(\"What's the capital of Germany?\")\n",
        "call_agent(\"What's the capital of Japan?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SugbbRPE6QYL"
      },
      "source": [
        "### LLM Agent with a Input/Output Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K0gMttqXgn3",
        "outputId": "a9534934-f503-4477-cafc-0a5150b4a7ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  {\n",
            "\"capital\": \"Paris\"\n",
            "}\n",
            "Agent Response:  {\n",
            "\"capital\": \"Berlin\"\n",
            "}\n",
            "Agent Response:  {\n",
            "\"capital\": \"Tokyo\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "from pydantic import Field\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"capital_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"capital_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "\n",
        "class InputSchema(BaseModel):\n",
        "    country: str = Field(description=\"The country to find the capital of.\")\n",
        "\n",
        "\n",
        "class OutputSchema(BaseModel):\n",
        "    capital: str = Field(description=\"The capital of the country.\")\n",
        "\n",
        "\n",
        "# Agent\n",
        "capital_agent = Agent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Capital Information Agent. Your task is to provide the capital of a given country.\n",
        "\n",
        "    When a user provides a prompt, extract the country name.\n",
        "    Then, respond with the capital of that country in the following JSON format:\n",
        "    {\"capital\": \"capital_name\"}\n",
        "    \"\"\",\n",
        "    description=\"\"\"You are an agent who can tell the capital of a country.\"\"\",\n",
        "    allow_transfer=False,\n",
        "    input_schema=InputSchema,\n",
        "    output_schema=OutputSchema,\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(\n",
        "    agent=capital_agent, app_name=APP_NAME, session_service=session_service\n",
        ")\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run(\n",
        "        user_id=USER_ID, session_id=SESSION_ID, new_message=content\n",
        "    )\n",
        "\n",
        "    for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n",
        "\n",
        "call_agent('{\"country\": \"France\"}')\n",
        "call_agent('{\"country\": \"Germany\"}')\n",
        "call_agent('{\"country\": \"Japan\"}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXOPByf86GYm"
      },
      "source": [
        "### LLM Agent with a output_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2mV97Ctf_jF",
        "outputId": "4baa9e80-4754-4c97-9809-809661d471ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  ```json\n",
            "{\n",
            "\"capital\": \"Paris\"\n",
            "}\n",
            "```\n",
            "Session State: ```json\n",
            "{\n",
            "\"capital\": \"Paris\"\n",
            "}\n",
            "```\n",
            "Agent Response:  ```json\n",
            "{\n",
            "\"capital\": \"Berlin\"\n",
            "}\n",
            "```\n",
            "Session State: ```json\n",
            "{\n",
            "\"capital\": \"Berlin\"\n",
            "}\n",
            "```\n",
            "Agent Response:  ```json\n",
            "{\n",
            "\"capital\": \"Tokyo\"\n",
            "}\n",
            "```\n",
            "Session State: ```json\n",
            "{\n",
            "\"capital\": \"Tokyo\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "import json\n",
        "\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"capital_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"capital_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# Agent\n",
        "capital_agent = Agent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Capital Information Agent. Your task is to provide the capital of a given country.\n",
        "\n",
        "    When a user provides a prompt, extract the country name.\n",
        "    Then, respond with the capital of that country in the following JSON format:\n",
        "    {\"capital\": \"capital_name\"}\n",
        "    \"\"\",\n",
        "    description=\"\"\"You are an agent who can tell the capital of a country.\"\"\",\n",
        "    output_key=\"capital_output\",\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "runner = Runner(\n",
        "    agent=capital_agent, app_name=APP_NAME, session_service=session_service\n",
        ")\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run(\n",
        "        user_id=USER_ID, session_id=SESSION_ID, new_message=content\n",
        "    )\n",
        "\n",
        "    for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n",
        "            print(\n",
        "                \"Session State:\",\n",
        "                session_service.get_session(\n",
        "                    APP_NAME, USER_ID, SESSION_ID\n",
        "                ).state.get(\"capital_output\"),\n",
        "            )\n",
        "\n",
        "\n",
        "call_agent(\"What's the capital of France?\")\n",
        "call_agent(\"What's the capital of Germany?\")\n",
        "call_agent(\"What's the capital of Japan?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCUGzcWD59A3"
      },
      "source": [
        "### LLM Agent with a built_in_code_execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVXqH7O5jDTa",
        "outputId": "6bc8f1e9-dcae-4a96-d2fe-fec8c4909c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  ```python\n",
            "def factorial(n):\n",
            "  \"\"\"\n",
            "  Calculate the factorial of a non-negative integer.\n",
            "\n",
            "  Args:\n",
            "    n: A non-negative integer.\n",
            "\n",
            "  Returns:\n",
            "    The factorial of n, which is the product of all positive integers less than or equal to n.\n",
            "    Returns 1 if n is 0.\n",
            "    Raises ValueError if n is negative.\n",
            "  \"\"\"\n",
            "  if n < 0:\n",
            "    raise ValueError(\"Factorial is not defined for negative numbers\")\n",
            "  elif n == 0:\n",
            "    return 1\n",
            "  else:\n",
            "    result = 1\n",
            "    for i in range(1, n + 1):\n",
            "      result *= i\n",
            "    return result\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "import json\n",
        "from google.adk.tools import built_in_code_execution\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"capital_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"capital_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "code_agent = LlmAgent(\n",
        "        name=\"code_execution_agent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        tools=[built_in_code_execution],\n",
        "        instruction=\"Generate python code to solve the user's request. \"\n",
        "        \"If the user asks for a specific output, return the output of the code execution.\",\n",
        "    )\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=code_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "  for event in events:\n",
        "      if event.is_final_response():\n",
        "          final_response = event.content.parts[0].text\n",
        "          print(\"Agent Response: \", final_response)\n",
        "\n",
        "call_agent(\"Write a python function to calculate the factorial of a number and return the result\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNpg5RLocqyG"
      },
      "source": [
        "### LLM Agent with a Single Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-h0W6YVmtH7",
        "outputId": "655a8971-c357-4d69-c09a-c0a3f5b37261"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  OK. The weather in Toronto is 30 degrees Celsius, partly cloudy with an overcast sky.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"weather_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"weather_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# Tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Retrieves weather information for the given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city for which to retrieve weather information.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the weather information for the specified city,\n",
        "        or a message indicating that the weather information was not found.\n",
        "    \"\"\"\n",
        "    cities = {\n",
        "        'chicago': {'temperature': 25, 'condition': 'sunny', 'sky': 'clear'},\n",
        "        'toronto': {'temperature': 30, 'condition': 'partly cloudy', 'sky': 'overcast'},\n",
        "        'chennai': {'temperature': 15, 'condition': 'rainy', 'sky': 'cloudy'},  # Fixed typo: 'tempeerature' to 'temperature'\n",
        "    }\n",
        "\n",
        "    city_lower = city.lower()\n",
        "    if city_lower in cities:\n",
        "        weather_data = cities[city_lower]\n",
        "        return f\"Weather in {city} is {weather_data['temperature']} degrees Celsius, {weather_data['condition']} with a {weather_data['sky']} sky.\"\n",
        "    else:\n",
        "        return f\"Weather information for {city} not found.\"\n",
        "\n",
        "# Agent\n",
        "weather_agent = Agent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Weather Information Agent. Your task is to provide weather information for a given city.\n",
        "\n",
        "    When a user provides a prompt, extract the city name.\n",
        "    Then, use the `get_weather` tool to retrieve the weather information for that city.\n",
        "    Finally, present the weather information to the user in a clear and concise manner.\"\"\",\n",
        "    description=\"\"\"You are an agent who can fetch weather information for a city.\n",
        "    You have access to the `get_weather` tool to accomplish this task.\"\"\",\n",
        "    tools=[get_weather],\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=weather_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "  for event in events:\n",
        "      if event.is_final_response():\n",
        "          final_response = event.content.parts[0].text\n",
        "          print(\"Agent Response: \", final_response)\n",
        "\n",
        "call_agent(\"What's the weather in toronto?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E17fqZuaH1BM"
      },
      "source": [
        "### LLM Agent with a Multiple Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtFi_2ANHUN4",
        "outputId": "f4e8707c-d3a6-434f-e5e4-4de38e78adc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  I found one recipe matching your request: chicken tikka masala.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  No, pasta carbonara is not suitable for a vegan diet.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  The ingredients for vegan lentil soup are: lentils, carrots, celery, onion, vegetable broth.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "\n",
        "# Constants\n",
        "APP_NAME = \"recipe_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"recipe_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# --- Mock Data ---\n",
        "recipes = {\n",
        "    \"pasta carbonara\": {\n",
        "        \"ingredients\": [\"pasta\", \"eggs\", \"guanciale\", \"pecorino romano\", \"black pepper\"],\n",
        "        \"dietary_restrictions\": [\"none\"],\n",
        "    },\n",
        "    \"chicken tikka masala\": {\n",
        "        \"ingredients\": [\"chicken\", \"yogurt\", \"ginger\", \"garlic\", \"masala blend\"],\n",
        "        \"dietary_restrictions\": [\"none\"],\n",
        "    },\n",
        "    \"vegan lentil soup\": {\n",
        "        \"ingredients\": [\"lentils\", \"carrots\", \"celery\", \"onion\", \"vegetable broth\"],\n",
        "        \"dietary_restrictions\": [\"vegan\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "# --- Tools ---\n",
        "def search_recipes(keyword: str) -> str:\n",
        "    \"\"\"Searches for recipes based on a keyword.\n",
        "\n",
        "    Args:\n",
        "        keyword: The keyword to search for (e.g., ingredient).\n",
        "\n",
        "    Returns:\n",
        "        A string containing the names of matching recipes, or a message if no recipes are found.\n",
        "    \"\"\"\n",
        "    matching_recipes = [\n",
        "        recipe_name\n",
        "        for recipe_name, recipe_data in recipes.items()\n",
        "        if keyword.lower() in recipe_name.lower() or keyword.lower() in recipe_data[\"ingredients\"]\n",
        "    ]\n",
        "    if matching_recipes:\n",
        "        return f\"Recipes matching '{keyword}': {', '.join(matching_recipes)}.\"\n",
        "    else:\n",
        "        return f\"No recipes found matching '{keyword}'.\"\n",
        "\n",
        "\n",
        "def check_dietary_restrictions(recipe_name: str, dietary_restriction: str) -> str:\n",
        "    \"\"\"Checks if a recipe is suitable for a given dietary restriction.\n",
        "\n",
        "    Args:\n",
        "        recipe_name: The name of the recipe to check.\n",
        "        dietary_restriction: The dietary restriction to check for (e.g., \"vegan\").\n",
        "\n",
        "    Returns:\n",
        "        A string indicating if the recipe is suitable or not.\n",
        "    \"\"\"\n",
        "    recipe_data = recipes.get(recipe_name.lower())\n",
        "    if recipe_data:\n",
        "        if dietary_restriction.lower() in recipe_data[\"dietary_restrictions\"]:\n",
        "            return f\"'{recipe_name}' is suitable for a '{dietary_restriction}' diet.\"\n",
        "        else:\n",
        "            return f\"'{recipe_name}' is not suitable for a '{dietary_restriction}' diet.\"\n",
        "    else:\n",
        "        return f\"Recipe '{recipe_name}' not found.\"\n",
        "\n",
        "\n",
        "def get_ingredient_list(recipe_name: str) -> str:\n",
        "    \"\"\"Returns a list of ingredients for a given recipe.\n",
        "\n",
        "    Args:\n",
        "        recipe_name: The name of the recipe.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the list of ingredients, or a message if the recipe is not found.\n",
        "    \"\"\"\n",
        "    recipe_data = recipes.get(recipe_name.lower())\n",
        "    if recipe_data:\n",
        "        return f\"Ingredients for '{recipe_name}': {', '.join(recipe_data['ingredients'])}.\"\n",
        "    else:\n",
        "        return f\"Recipe '{recipe_name}' not found.\"\n",
        "\n",
        "\n",
        "# --- Agent ---\n",
        "recipe_agent = Agent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Recipe Agent. Your task is to help users find recipes and check their suitability for dietary restrictions.\n",
        "\n",
        "    You have access to three tools:\n",
        "    1. `search_recipes`: Use this tool to find recipes based on a keyword (e.g., ingredient).\n",
        "    2. `check_dietary_restrictions`: Use this tool to check if a recipe is suitable for a given dietary restriction.\n",
        "    3. `get_ingredient_list`: Use this tool to get a list of ingredients for a given recipe.\n",
        "\n",
        "    When a user provides a prompt, first determine what they are asking for.\n",
        "    - If they are asking for recipes based on a keyword, use the `search_recipes` tool.\n",
        "    - If they are asking if a recipe is suitable for a dietary restriction, use the `check_dietary_restrictions` tool.\n",
        "    - If they are asking for a list of ingredients for a recipe, use the `get_ingredient_list` tool.\n",
        "    Finally, present the information to the user in a clear and concise manner.\n",
        "    \"\"\",\n",
        "    description=\"\"\"An agent that can find recipes, check dietary restrictions, and list ingredients.\n",
        "    It has access to the `search_recipes`, `check_dietary_restrictions`, and `get_ingredient_list` tools.\"\"\",\n",
        "    tools=[search_recipes, check_dietary_restrictions, get_ingredient_list],\n",
        ")\n",
        "\n",
        "# --- Session and Runner ---\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=recipe_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# --- Agent Interaction ---\n",
        "def call_agent(query):\n",
        "    content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "    for event in events:\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n",
        "\n",
        "\n",
        "call_agent(\"Find recipes with chicken.\")\n",
        "call_agent(\"Is pasta carbonara suitable for a vegan diet?\")\n",
        "call_agent(\"What are the ingredients in vegan lentil soup?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G6tlJhHR3FM"
      },
      "source": [
        "### LLM Agent with a Async Run (run_async)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urYEdu2MRvWg",
        "outputId": "ab6398ba-dd9f-4a31-f63b-fffe2a830aa1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  OK. The weather in Toronto is 30 degrees Celsius, partly cloudy with an overcast sky.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"weather_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"weather_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# Tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Retrieves weather information for the given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city for which to retrieve weather information.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the weather information for the specified city,\n",
        "        or a message indicating that the weather information was not found.\n",
        "    \"\"\"\n",
        "    cities = {\n",
        "        'chicago': {'temperature': 25, 'condition': 'sunny', 'sky': 'clear'},\n",
        "        'toronto': {'temperature': 30, 'condition': 'partly cloudy', 'sky': 'overcast'},\n",
        "        'chennai': {'temperature': 15, 'condition': 'rainy', 'sky': 'cloudy'},  # Fixed typo: 'tempeerature' to 'temperature'\n",
        "    }\n",
        "\n",
        "    city_lower = city.lower()\n",
        "    if city_lower in cities:\n",
        "        weather_data = cities[city_lower]\n",
        "        return f\"Weather in {city} is {weather_data['temperature']} degrees Celsius, {weather_data['condition']} with a {weather_data['sky']} sky.\"\n",
        "    else:\n",
        "        return f\"Weather information for {city} not found.\"\n",
        "\n",
        "# Agent\n",
        "weather_agent = Agent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Weather Information Agent. Your task is to provide weather information for a given city.\n",
        "\n",
        "    When a user provides a prompt, extract the city name.\n",
        "    Then, use the `get_weather` tool to retrieve the weather information for that city.\n",
        "    Finally, present the weather information to the user in a clear and concise manner.\"\"\",\n",
        "    description=\"\"\"You are an agent who can fetch weather information for a city.\n",
        "    You have access to the `get_weather` tool to accomplish this task.\"\"\",\n",
        "    tools=[get_weather],\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=weather_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "async def call_agent_async(query):\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
        "      if event.is_final_response():\n",
        "          final_response = event.content.parts[0].text\n",
        "          print(\"Agent Response: \", final_response)\n",
        "\n",
        "await call_agent_async(\"What's the weather in toronto?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbq5jAVSVCUM"
      },
      "source": [
        "### LLM Agent with Auto Flow (using LLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9RE0gb0TFkg"
      },
      "outputs": [],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "import asyncio\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"weather_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"weather_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "\n",
        "# Tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Retrieves weather information for the given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city for which to retrieve weather information.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the weather information for the specified city,\n",
        "        or a message indicating that the weather information was not found.\n",
        "    \"\"\"\n",
        "    cities = {\n",
        "        'chicago': {'temperature': 25, 'condition': 'sunny', 'sky': 'clear'},\n",
        "        'toronto': {'temperature': 30, 'condition': 'partly cloudy', 'sky': 'overcast'},\n",
        "        'chennai': {'temperature': 15, 'condition': 'rainy', 'sky': 'cloudy'},\n",
        "    }\n",
        "\n",
        "    city_lower = city.lower()\n",
        "    if city_lower in cities:\n",
        "        weather_data = cities[city_lower]\n",
        "        return f\"Weather in {city} is {weather_data['temperature']} degrees Celsius, {weather_data['condition']} with a {weather_data['sky']} sky.\"\n",
        "    else:\n",
        "        return f\"Weather information for {city} not found.\"\n",
        "\n",
        "\n",
        "def get_greeting(name: str) -> str:\n",
        "    \"\"\"Greets the given name.\n",
        "\n",
        "    Args:\n",
        "        name: The name to greet.\n",
        "\n",
        "    Returns:\n",
        "        A greeting message.\n",
        "    \"\"\"\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "\n",
        "# Agent\n",
        "weather_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=\"weather_agent\",\n",
        "    instruction=\"\"\"You are a Weather Information Agent. Your task is to provide weather information for a given city.\n",
        "\n",
        "    When a user provides a prompt, extract the city name.\n",
        "    Then, use the `get_weather` tool to retrieve the weather information for that city.\n",
        "    Finally, present the weather information to the user in a clear and concise manner.\n",
        "    If the user asks for a greeting, transfer to the greeting agent.\"\"\",\n",
        "    description=\"\"\"You are an agent who can fetch weather information for a city.\n",
        "    You have access to the `get_weather` tool to accomplish this task.\"\"\",\n",
        "    tools=[get_weather],\n",
        "    allow_transfer=True,\n",
        ")\n",
        "\n",
        "greeting_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=\"greeting_agent\",\n",
        "    instruction=\"\"\"You are a Greeting Agent. Your task is to greet the user.\n",
        "\n",
        "    When a user provides a prompt, extract the name.\n",
        "    Then, use the `get_greeting` tool to greet the user.\n",
        "    Finally, present the greeting to the user in a clear and concise manner.\n",
        "    If the user asks for weather information, transfer to the weather agent.\"\"\",\n",
        "    description=\"\"\"You are an agent who can greet a user.\n",
        "    You have access to the `get_greeting` tool to accomplish this task.\"\"\",\n",
        "    tools=[get_greeting],\n",
        "    allow_transfer=True,\n",
        ")\n",
        "\n",
        "# Set parent-child relationship\n",
        "weather_agent.children = [greeting_agent]\n",
        "# greeting_agent.parent_agent = weather_agent\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=weather_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "async def call_agent_async(query):\n",
        "    content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "    async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_70TiFhAfP3Z",
        "outputId": "9cfeb22e-a07d-4bf0-91e0-182a662b6699"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  The weather in Toronto is 30 degrees Celsius, partly cloudy with an overcast sky.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "await call_agent_async(\"What's the weather in toronto?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7YvXK5afP0C",
        "outputId": "0411cab1-c4fe-40ea-a5c2-e49b17758513"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n",
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Hello, Alice!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "await call_agent_async(\"Hello, Alice!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUiHdzpATx2H"
      },
      "source": [
        "### Weather Agent QUickstart with Multi-Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOIYc49jTyHj"
      },
      "outputs": [],
      "source": [
        "# Tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Retrieves weather information for the given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city for which to retrieve weather information.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the weather information for the specified city,\n",
        "        or a message indicating that the weather information was not found.\n",
        "    \"\"\"\n",
        "    cities = {\n",
        "        'chicago': {'temperature': 25, 'condition': 'sunny', 'sky': 'clear'},\n",
        "        'toronto': {'temperature': 30, 'condition': 'partly cloudy', 'sky': 'overcast'},\n",
        "        'chennai': {'temperature': 15, 'condition': 'rainy', 'sky': 'cloudy'},\n",
        "    }\n",
        "\n",
        "    city_lower = city.lower()\n",
        "    if city_lower in cities:\n",
        "        weather_data = cities[city_lower]\n",
        "        return f\"Weather in {city} is {weather_data['temperature']} degrees Celsius, {weather_data['condition']} with a {weather_data['sky']} sky.\"\n",
        "    else:\n",
        "        return f\"Weather information for {city} not found.\"\n",
        "\n",
        "\n",
        "# Tool for the Greeting Agent\n",
        "def say_hello(name: str = \"there\") -> str:\n",
        "    \"\"\"Provides a simple greeting.\"\"\"\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "# Tool for the Farewell Agent\n",
        "def say_goodbye() -> str:\n",
        "    \"\"\"Provides a simple farewell message.\"\"\"\n",
        "    return \"Goodbye! Have a great day.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX-ZKdrgT09N",
        "outputId": "c8602e47-5e64-4a79-8e72-1dc276face1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defined parent agent: weather_agent\n",
            "Defined weather_agent (parent), greeting_agent (child), and farewell_agent (child).\n",
            "weather_agent children: ['greeting_agent', 'farewell_agent']\n"
          ]
        }
      ],
      "source": [
        "# --- Import LlmAgent ---\n",
        "from google.adk.agents import LlmAgent\n",
        "import google.genai.types as types # For Content/Part later\n",
        "\n",
        "# --- Agent Definitions ---\n",
        "AGENT_NAME_WEATHER = \"weather_agent\"\n",
        "AGENT_NAME_GREETING = \"greeting_agent\"\n",
        "AGENT_NAME_FAREWELL = \"farewell_agent\"\n",
        "MODEL_NAME = \"gemini-2.0-flash-001\" # Use a recent flash model\n",
        "\n",
        "# --- Parent Agent: Weather ---\n",
        "weather_agent = LlmAgent(\n",
        "    model=MODEL_NAME,\n",
        "    name=AGENT_NAME_WEATHER,\n",
        "    # *** REVISED INSTRUCTION ***\n",
        "    instruction=f\"\"\"You are the main Weather Agent in charge. Your primary responsibility is providing weather information.\n",
        "    - **IF** the user asks specifically about the weather (e.g., 'weather in city', 'forecast'), use the 'get_weather' tool YOURSELF. **DO NOT transfer weather requests.**\n",
        "    - **ONLY IF** the user gives a simple greeting (like 'Hi', 'Hello') with NO other request, transfer to the '{AGENT_NAME_GREETING}'.\n",
        "    - **ONLY IF** the user explicitly says goodbye (like 'Bye', 'See you'), transfer to the '{AGENT_NAME_FAREWELL}'.\n",
        "    - Handle only weather requests directly.\n",
        "    \"\"\",\n",
        "    description=\"Provides weather forecasts using 'get_weather'. Delegates greetings/farewells.\",\n",
        "    tools=[get_weather],\n",
        "    allow_transfer=True, # Still needed to enable delegation *when appropriate*\n",
        ")\n",
        "print(f\"Defined parent agent: {weather_agent.name}\")\n",
        "\n",
        "\n",
        "# --- Child Agent 1: Greeting ---\n",
        "greeting_agent = LlmAgent(\n",
        "    model=MODEL_NAME,\n",
        "    name=AGENT_NAME_GREETING,\n",
        "    instruction=\"You are the Greeting Agent. Use the 'say_hello' tool to greet the user. Do nothing else.\",\n",
        "    description=\"Handles simple greetings using the 'say_hello' tool.\",\n",
        "    tools=[say_hello],\n",
        "    allow_transfer=True,\n",
        "\n",
        ")\n",
        "\n",
        "# --- Child Agent 2: Farewell ---\n",
        "farewell_agent = LlmAgent(\n",
        "    model=MODEL_NAME,\n",
        "    name=AGENT_NAME_FAREWELL,\n",
        "    instruction=\"You are the Farewell Agent. Use the 'say_goodbye' tool when the user indicates they are leaving. Do nothing else.\",\n",
        "    description=\"Handles simple farewells using the 'say_goodbye' tool.\",\n",
        "    tools=[say_goodbye],\n",
        "    allow_transfer=True,\n",
        ")\n",
        "\n",
        "# --- Define the Parent-Child Relationship ---\n",
        "# This tells the framework how the agents are structured.\n",
        "weather_agent.children = [greeting_agent, farewell_agent]\n",
        "# The framework automatically sets the .parent_agent attribute on the children\n",
        "\n",
        "print(\"Defined weather_agent (parent), greeting_agent (child), and farewell_agent (child).\")\n",
        "print(f\"{weather_agent.name} children: {[child.name for child in weather_agent.children]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkWevBdBaK_G"
      },
      "outputs": [],
      "source": [
        "from google.adk.sessions import InMemorySessionService\n",
        "import uuid\n",
        "\n",
        "session_service = InMemorySessionService()\n",
        "\n",
        "# Required. Unique identifier for the application.\n",
        "APP_NAME = \"weather_app\"\n",
        "# Required. Identifier for the user interacting with the agent. This is a dynamic variable.\n",
        "USER_ID = \"12345\"\n",
        "\n",
        "SESSION_ID = f\"session_{uuid.uuid4()}\" # Use a dynamic session ID\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0DNBGnKaK7-"
      },
      "outputs": [],
      "source": [
        "from google.adk.runners import Runner\n",
        "\n",
        "runner = Runner(\n",
        "    agent=weather_agent,\n",
        "    app_name=APP_NAME,\n",
        "    session_service=session_service,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdhtJjVUaK41"
      },
      "outputs": [],
      "source": [
        "def call_agent(user_query):\n",
        "  content = types.Content(role='user', parts=[types.Part(text=user_query)])\n",
        "  events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "  for event in events:\n",
        "      if event.is_final_response():\n",
        "          final_response = event.content.parts[0].text\n",
        "          print(\"Agent Response: \", final_response)\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyY9FI7g3jAM",
        "outputId": "1c546feb-4383-4960-8f31-ab3b8873fd6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n",
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Hello, there!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "call_agent(user_query = \"Hello there!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrFV75DHaK3G",
        "outputId": "d396c486-668a-46e8-8276-a7d1d3c4ed0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Sorry, I am designed to only greet people. I cannot help you with the weather.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "call_agent(user_query = \"What's the weather like in Chennai?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIQWRq2qbPMk",
        "outputId": "b48e3fee-aa65-4764-f7b7-91e38e55163f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  I am here to greet you. I don't have the ability to look up weather information.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "call_agent(user_query = \"What about the weather in toronto?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-OvXlb4bYYT",
        "outputId": "db985692-3d37-4ba7-9ca8-78606603d274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Goodbye!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "call_agent(user_query = \"Okay, thanks, bye!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXymsCuyT2IT",
        "outputId": "f70ff2dc-a4b8-4d61-c6dc-f6d31d0117e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session Service setup complete. Session ID: session_ba542e33-3ec3-4a85-8c69-9d0a3544327a\n"
          ]
        }
      ],
      "source": [
        "from google.adk.sessions import InMemorySessionService\n",
        "import uuid\n",
        "\n",
        "# Required. Unique identifier for the application.\n",
        "APP_NAME = \"multi_weather_app\"\n",
        "# Required. Identifier for the user interacting with the agent.\n",
        "USER_ID = \"user_xyz\"\n",
        "SESSION_ID = f\"session_{uuid.uuid4()}\" # Use a dynamic session ID\n",
        "\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        ")\n",
        "\n",
        "print(f\"Session Service setup complete. Session ID: {session.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbMa_XpaT2Dl",
        "outputId": "d927ee3c-5121-49ec-e8ba-db04ca2a6eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Runner created, configured with root agent: weather_agent\n"
          ]
        }
      ],
      "source": [
        "from google.adk.runners import Runner\n",
        "\n",
        "runner = Runner(\n",
        "    agent=weather_agent, # Start with the main weather agent\n",
        "    app_name=APP_NAME,\n",
        "    session_service=session_service,\n",
        "    # artifact_service=... # Add if using artifacts\n",
        ")\n",
        "\n",
        "print(f\"Runner created, configured with root agent: {runner.agent.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0_QmroKT2Au",
        "outputId": "63dd9434-4a34-40d6-a715-2ef2d8ba90c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> User: What's the weather like in Chennai?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<<< weather_agent: The weather in Chennai is 15 degrees Celsius, rainy with a cloudy sky.\n",
            "\n",
            ">>> User: Hello there!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n",
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<<< greeting_agent: Hello, there!\n",
            "\n",
            ">>> User: How about the weather in Toronto?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<<< greeting_agent: Hello, there! I can greet you, but I cannot provide weather information. Would you like me to find an agent that can?\n",
            "\n",
            ">>> User: Okay, thanks, bye!\n",
            "<<< greeting_agent: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "import asyncio # Needed for async call if running directly\n",
        "\n",
        "# Helper function to call the agent and print the final response\n",
        "async def ask_agent(query: str):\n",
        "    print(f\"\\n>>> User: {query}\")\n",
        "    content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "    final_response_text = \"Agent did not provide a final text response.\"\n",
        "    agent_responder = \"System\" # Default if no agent responds clearly\n",
        "\n",
        "    # Use run_async for potential async operations within agents/tools\n",
        "    async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
        "        # You can add more detailed event logging here if desired\n",
        "        # print(f\"  Event: Author={event.author}, Content={event.content}, Actions={event.actions}\")\n",
        "        if event.is_final_response() and event.content and event.content.parts:\n",
        "             # Check if the text part exists before accessing\n",
        "             if event.content.parts[0].text:\n",
        "                  final_response_text = event.content.parts[0].text.strip()\n",
        "                  agent_responder = event.author # Capture who gave the final response\n",
        "                  break # Exit after getting the first final text response\n",
        "\n",
        "    print(f\"<<< {agent_responder}: {final_response_text}\")\n",
        "\n",
        "\n",
        "# --- Run Interactions ---\n",
        "\n",
        "# Wrapper to run async function in different environments\n",
        "async def run_interactions():\n",
        "    # Example 1: Weather query (handled by weather_agent)\n",
        "    await ask_agent(\"What's the weather like in Chennai?\")\n",
        "\n",
        "    # Example 2: Greeting (should be transferred to greeting_agent)\n",
        "    await ask_agent(\"Hello there!\")\n",
        "\n",
        "    # Example 3: Weather query after greeting (should transfer back to weather_agent)\n",
        "    await ask_agent(\"How about the weather in Toronto?\")\n",
        "\n",
        "    # Example 4: Farewell (should be transferred to farewell_agent)\n",
        "    await ask_agent(\"Okay, thanks, bye!\")\n",
        "\n",
        "# Run the interactions\n",
        "await run_interactions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDl005Qnog6j"
      },
      "source": [
        "### LLM Agents with Callbacks (Agent, Model & Tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPVFcZ84sfLe"
      },
      "outputs": [],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "import asyncio\n",
        "\n",
        "# Constant\n",
        "APP_NAME = \"weather_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"weather_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "\n",
        "# Tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Retrieves weather information for the given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city for which to retrieve weather information.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the weather information for the specified city,\n",
        "        or a message indicating that the weather information was not found.\n",
        "    \"\"\"\n",
        "    cities = {\n",
        "        'chicago': {'temperature': 25, 'condition': 'sunny', 'sky': 'clear'},\n",
        "        'toronto': {'temperature': 30, 'condition': 'partly cloudy', 'sky': 'overcast'},\n",
        "        'chennai': {'temperature': 15, 'condition': 'rainy', 'sky': 'cloudy'},\n",
        "    }\n",
        "\n",
        "    city_lower = city.lower()\n",
        "    if city_lower in cities:\n",
        "        weather_data = cities[city_lower]\n",
        "        return f\"Weather in {city} is {weather_data['temperature']} degrees Celsius, {weather_data['condition']} with a {weather_data['sky']} sky.\"\n",
        "    else:\n",
        "        return f\"Weather information for {city} not found.\"\n",
        "\n",
        "\n",
        "def get_greeting(name: str) -> str:\n",
        "    \"\"\"Greets the given name.\n",
        "\n",
        "    Args:\n",
        "        name: The name to greet.\n",
        "\n",
        "    Returns:\n",
        "        A greeting message.\n",
        "    \"\"\"\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "\n",
        "# Callbacks\n",
        "\n",
        "def before_model_callback(callback_context, llm_request):\n",
        "    print(f\"Before Model Callback: Agent {callback_context._invocation_context.agent.name}, Request: {llm_request.contents}\")\n",
        "    return None\n",
        "\n",
        "def after_model_callback(callback_context, llm_response):\n",
        "    print(f\"After Model Callback: Agent {callback_context._invocation_context.agent.name}, Response: {llm_response.content}\")\n",
        "    return None\n",
        "\n",
        "def before_tool_callback(tool, args, tool_context):\n",
        "    print(f\"Before Tool Callback: Tool {tool.name}, Args: {args}\")\n",
        "    return None\n",
        "\n",
        "def after_tool_callback(tool, args, tool_context, tool_response):\n",
        "    print(f\"After Tool Callback: Tool {tool.name}, Response: {tool_response}\")\n",
        "    return None\n",
        "\n",
        "def before_agent_callback(callback_context):\n",
        "    print(f\"Before Agent Callback: Agent {callback_context._invocation_context.agent.name}\")\n",
        "    return None\n",
        "\n",
        "def after_agent_callback(callback_context):\n",
        "    print(f\"After Agent Callback: Agent {callback_context._invocation_context.agent.name}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "weather_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=\"weather_agent\",\n",
        "    instruction=\"\"\"You are a Weather Information Agent. Your task is to provide weather information for a given city.\n",
        "\n",
        "    When a user provides a prompt, extract the city name.\n",
        "    Then, use the `get_weather` tool to retrieve the weather information for that city.\n",
        "    Finally, present the weather information to the user in a clear and concise manner.\n",
        "    If the user asks for a greeting, transfer to the greeting agent.\"\"\",\n",
        "    description=\"\"\"You are an agent who can fetch weather information for a city.\n",
        "    You have access to the `get_weather` tool to accomplish this task.\"\"\",\n",
        "    tools=[get_weather],\n",
        "    allow_transfer=True,\n",
        "    before_model_callback=before_model_callback,\n",
        "    after_model_callback=after_model_callback,\n",
        "    before_tool_callback=before_tool_callback,\n",
        "    after_tool_callback=after_tool_callback,\n",
        "    before_agent_callback=before_agent_callback,\n",
        "    after_agent_callback=after_agent_callback,\n",
        ")\n",
        "\n",
        "greeting_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=\"greeting_agent\",\n",
        "    instruction=\"\"\"You are a Greeting Agent. Your task is to greet the user.\n",
        "\n",
        "    When a user provides a prompt, extract the name.\n",
        "    Then, use the `get_greeting` tool to greet the user.\n",
        "    Finally, present the greeting to the user in a clear and concise manner.\n",
        "    If the user asks for weather information, transfer to the weather agent.\"\"\",\n",
        "    description=\"\"\"You are an agent who can greet a user.\n",
        "    You have access to the `get_greeting` tool to accomplish this task.\"\"\",\n",
        "    tools=[get_greeting],\n",
        "    allow_transfer=True,\n",
        "    before_model_callback=before_model_callback,\n",
        "    after_model_callback=after_model_callback,\n",
        "    before_tool_callback=before_tool_callback,\n",
        "    after_tool_callback=after_tool_callback,\n",
        "    before_agent_callback=before_agent_callback,\n",
        "    after_agent_callback=after_agent_callback,\n",
        ")\n",
        "\n",
        "# Set parent-child relationship\n",
        "weather_agent.children = [greeting_agent]\n",
        "greeting_agent.parent_agent = weather_agent\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=weather_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "async def call_agent_async(query):\n",
        "    content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "    async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(\"Agent Response: \", final_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUPvRHL6k4nx",
        "outputId": "d409e78d-e0e6-4577-ff28-31232b200773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before Agent Callback: Agent weather_agent\n",
            "Before Model Callback: Agent weather_agent, Request: [Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"What's the weather in Chicago?\")], role='user')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After Model Callback: Agent weather_agent, Response: parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'city': 'Chicago'}, name='get_weather'), function_response=None, inline_data=None, text=None)] role='model'\n",
            "Before Tool Callback: Tool get_weather, Args: {'city': 'Chicago'}\n",
            "After Tool Callback: Tool get_weather, Response: Weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\n",
            "Before Model Callback: Agent weather_agent, Request: [Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"What's the weather in Chicago?\")], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'city': 'Chicago'}, name='get_weather'), function_response=None, inline_data=None, text=None)], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='get_weather', response={'result': 'Weather in Chicago is 25 degrees Celsius, sunny with a clear sky.'}), inline_data=None, text=None)], role='user')]\n",
            "After Model Callback: Agent weather_agent, Response: parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='OK. The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\\n')] role='model'\n",
            "Agent Response:  OK. The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\n",
            "\n",
            "After Agent Callback: Agent weather_agent\n"
          ]
        }
      ],
      "source": [
        "await call_agent_async(\"What's the weather in Chicago?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QefPfrpUk4k6",
        "outputId": "6ae52fa5-0b96-4d5c-97a4-6c7b0a282eff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before Agent Callback: Agent weather_agent\n",
            "Before Model Callback: Agent weather_agent, Request: [Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"What's the weather in Chicago?\")], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'city': 'Chicago'}, name='get_weather'), function_response=None, inline_data=None, text=None)], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='get_weather', response={'result': 'Weather in Chicago is 25 degrees Celsius, sunny with a clear sky.'}), inline_data=None, text=None)], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='OK. The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\\n')], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello, there!')], role='user')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After Model Callback: Agent weather_agent, Response: parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'greeting_agent'}, name='transfer_to_agent'), function_response=None, inline_data=None, text=None)] role='model'\n",
            "Before Tool Callback: Tool transfer_to_agent, Args: {'agent_name': 'greeting_agent'}\n",
            "After Tool Callback: Tool transfer_to_agent, Response: {}\n",
            "Before Agent Callback: Agent greeting_agent\n",
            "Before Model Callback: Agent greeting_agent, Request: [Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"What's the weather in Chicago?\")], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='OK. The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\\n')], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello, there!')], role='user')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After Model Callback: Agent greeting_agent, Response: parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'name': 'there'}, name='get_greeting'), function_response=None, inline_data=None, text=None)] role='model'\n",
            "Before Tool Callback: Tool get_greeting, Args: {'name': 'there'}\n",
            "After Tool Callback: Tool get_greeting, Response: Hello, there!\n",
            "Before Model Callback: Agent greeting_agent, Request: [Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"What's the weather in Chicago?\")], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='OK. The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\\n')], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello, there!')], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'name': 'there'}, name='get_greeting'), function_response=None, inline_data=None, text=None)], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='get_greeting', response={'result': 'Hello, there!'}), inline_data=None, text=None)], role='user')]\n",
            "After Model Callback: Agent greeting_agent, Response: parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello, there!\\n')] role='model'\n",
            "Agent Response:  Hello, there!\n",
            "\n",
            "After Agent Callback: Agent greeting_agent\n",
            "After Agent Callback: Agent weather_agent\n"
          ]
        }
      ],
      "source": [
        "await call_agent_async(\"Hello, there!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9C98rKGk4iH",
        "outputId": "c354ea54-459a-45e3-8966-6e519838bcec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before Agent Callback: Agent greeting_agent\n",
            "Before Model Callback: Agent greeting_agent, Request: [Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"What's the weather in Chicago?\")], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='OK. The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\\n')], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello, there!')], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'name': 'there'}, name='get_greeting'), function_response=None, inline_data=None, text=None)], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='get_greeting', response={'result': 'Hello, there!'}), inline_data=None, text=None)], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello, there!\\n')], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='How about Toronto?')], role='user')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After Model Callback: Agent greeting_agent, Response: parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'weather_agent'}, name='transfer_to_agent'), function_response=None, inline_data=None, text=None)] role='model'\n",
            "Before Tool Callback: Tool transfer_to_agent, Args: {'agent_name': 'weather_agent'}\n",
            "After Tool Callback: Tool transfer_to_agent, Response: {}\n",
            "Before Agent Callback: Agent weather_agent\n",
            "Before Model Callback: Agent weather_agent, Request: [Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"What's the weather in Chicago?\")], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'city': 'Chicago'}, name='get_weather'), function_response=None, inline_data=None, text=None)], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='get_weather', response={'result': 'Weather in Chicago is 25 degrees Celsius, sunny with a clear sky.'}), inline_data=None, text=None)], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='OK. The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\\n')], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello, there!')], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'greeting_agent'}, name='transfer_to_agent'), function_response=None, inline_data=None, text=None)], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='transfer_to_agent', response={}), inline_data=None, text=None)], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello, there!\\n')], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='How about Toronto?')], role='user')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After Model Callback: Agent weather_agent, Response: parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'city': 'Toronto'}, name='get_weather'), function_response=None, inline_data=None, text=None)] role='model'\n",
            "Before Tool Callback: Tool get_weather, Args: {'city': 'Toronto'}\n",
            "After Tool Callback: Tool get_weather, Response: Weather in Toronto is 30 degrees Celsius, partly cloudy with a overcast sky.\n",
            "Before Model Callback: Agent weather_agent, Request: [Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"What's the weather in Chicago?\")], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'city': 'Chicago'}, name='get_weather'), function_response=None, inline_data=None, text=None)], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='get_weather', response={'result': 'Weather in Chicago is 25 degrees Celsius, sunny with a clear sky.'}), inline_data=None, text=None)], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='OK. The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\\n')], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello, there!')], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'agent_name': 'greeting_agent'}, name='transfer_to_agent'), function_response=None, inline_data=None, text=None)], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='transfer_to_agent', response={}), inline_data=None, text=None)], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello, there!\\n')], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='How about Toronto?')], role='user'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'city': 'Toronto'}, name='get_weather'), function_response=None, inline_data=None, text=None)], role='model'), Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=FunctionResponse(id=None, name='get_weather', response={'result': 'Weather in Toronto is 30 degrees Celsius, partly cloudy with a overcast sky.'}), inline_data=None, text=None)], role='user')]\n",
            "After Model Callback: Agent weather_agent, Response: parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='OK. The weather in Toronto is 30 degrees Celsius, partly cloudy with a overcast sky.\\n')] role='model'\n",
            "Agent Response:  OK. The weather in Toronto is 30 degrees Celsius, partly cloudy with a overcast sky.\n",
            "\n",
            "After Agent Callback: Agent weather_agent\n",
            "After Agent Callback: Agent greeting_agent\n"
          ]
        }
      ],
      "source": [
        "await call_agent_async(\"How about Toronto?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLmmHTi3ChKE"
      },
      "source": [
        "### LLM Agent with before_agent_callback and state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2X8Fh4BChmN",
        "outputId": "f7d3487e-96d9-4d73-9d08-a9e2f49ea407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running LLM Agent Normally ---\n",
            "[Callback] Entering agent: SimpleLlmAgent (Invocation: e-b4399c56-6c14-40b6-b339-ff2100bdcff6)\n",
            "[Callback] Condition not met: Proceeding with agent SimpleLlmAgent.\n",
            "Event Output: SimpleLlmAgent: Hello!\n",
            "\n",
            "--- Running LLM Agent with Skip Condition ---\n",
            "[Callback] Entering agent: SimpleLlmAgent (Invocation: e-fe4f9989-3376-4707-88a8-2b135dc01a89)\n",
            "[Callback] Condition met: Skipping agent SimpleLlmAgent.\n",
            "Event Output: SimpleLlmAgent: Agent SimpleLlmAgent was skipped by callback.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\" # Or your preferred model\n",
        "\n",
        "# --- Define the Callback Function (Same as before) ---\n",
        "def simple_before_agent_logger(callback_context: CallbackContext) -> Optional[types.Content]:\n",
        "    \"\"\"Logs entry into an agent and checks a condition.\"\"\"\n",
        "    agent_name = callback_context.agent_name\n",
        "    invocation_id = callback_context.invocation_id\n",
        "    print(f\"[Callback] Entering agent: {agent_name} (Invocation: {invocation_id})\")\n",
        "\n",
        "    # Example: Check a condition in state\n",
        "    if callback_context.state.get(\"skip_agent\", False):\n",
        "        print(f\"[Callback] Condition met: Skipping agent {agent_name}.\")\n",
        "        # Return Content to skip the agent's run\n",
        "        return types.Content(parts=[types.Part(text=f\"Agent {agent_name} was skipped by callback.\")])\n",
        "    else:\n",
        "        print(f\"[Callback] Condition not met: Proceeding with agent {agent_name}.\")\n",
        "        # Return None to allow the agent's run to execute\n",
        "        return None\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent and Assign Callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"SimpleLlmAgent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are a simple agent. Just say 'Hello!'\",\n",
        "        description=\"An LLM agent demonstrating before_agent_callback\",\n",
        "        before_agent_callback=simple_before_agent_logger\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(agent=my_llm_agent, app_name=\"llm_demo_app\", session_service=session_service)\n",
        "    session_id_run = \"llm_session_run_1\"\n",
        "    session_id_skip = \"llm_session_skip_1\"\n",
        "    user_id = \"llm_test_user\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(app_name=\"llm_demo_app\", user_id=user_id, session_id=session_id_run)\n",
        "    session_service.create_session(app_name=\"llm_demo_app\", user_id=user_id, session_id=session_id_skip,\n",
        "                                  state={\"skip_agent\": True}) # Set state to trigger skip condition\n",
        "\n",
        "    print(\"--- Running LLM Agent Normally ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_run,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"Run normally\")])):\n",
        "        # Only print final LLM response or callback override\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\") # Added strip() for cleaner output\n",
        "\n",
        "    print(\"\\n--- Running LLM Agent with Skip Condition ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_skip, new_message=types.Content(parts=[types.Part(text=\"Skip this agent\")])):\n",
        "         # Only print final LLM response or callback override\n",
        "         if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhpRxvkuFoIt"
      },
      "source": [
        "### LLM Agent with after_agent_callback and state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2hWWDwNFodQ",
        "outputId": "e766ace8-bb30-40a8-f2f4-2215f38c771b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running LLM Agent Normally (with after_agent_callback) ---\n",
            "Event Output: SimpleLlmAgentWithAfter: Processing complete!\n",
            "[Callback] Exiting agent: SimpleLlmAgentWithAfter (Invocation: e-f5a5021a-eb55-41d4-9273-ab41630292e5)\n",
            "[Callback] Agent run status from state: Completed Normally\n",
            "[Callback] No concluding note added for agent SimpleLlmAgentWithAfter.\n",
            "\n",
            "--- Running LLM Agent with Concluding Note Condition ---\n",
            "Event Output: SimpleLlmAgentWithAfter: Processing complete!\n",
            "[Callback] Exiting agent: SimpleLlmAgentWithAfter (Invocation: e-98aac661-9c0d-463c-ac4d-c31ef4d3d419)\n",
            "[Callback] Agent run status from state: Completed Normally\n",
            "[Callback] Adding concluding note for agent SimpleLlmAgentWithAfter.\n",
            "Event Output: SimpleLlmAgentWithAfter: Concluding note added by after_agent_callback.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\" # Or your preferred model\n",
        "\n",
        "# --- Define the Callback Function ---\n",
        "def simple_after_agent_logger(callback_context: CallbackContext) -> Optional[types.Content]:\n",
        "    \"\"\"Logs exit from an agent and optionally appends a message.\"\"\"\n",
        "    agent_name = callback_context.agent_name\n",
        "    invocation_id = callback_context.invocation_id\n",
        "    print(f\"[Callback] Exiting agent: {agent_name} (Invocation: {invocation_id})\")\n",
        "\n",
        "    # Example: Check state potentially modified during the agent's run\n",
        "    final_status = callback_context.state.get(\"agent_run_status\", \"Completed Normally\")\n",
        "    print(f\"[Callback] Agent run status from state: {final_status}\")\n",
        "\n",
        "    # Example: Optionally return Content to append a message\n",
        "    if callback_context.state.get(\"add_concluding_note\", False):\n",
        "        print(f\"[Callback] Adding concluding note for agent {agent_name}.\")\n",
        "        # Return Content to append after the agent's own output\n",
        "        return types.Content(parts=[types.Part(text=f\"Concluding note added by after_agent_callback.\")])\n",
        "    else:\n",
        "        print(f\"[Callback] No concluding note added for agent {agent_name}.\")\n",
        "        # Return None - no additional message appended\n",
        "        return None\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent and Assign Callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"SimpleLlmAgentWithAfter\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are a simple agent. Just say 'Processing complete!'\",\n",
        "        description=\"An LLM agent demonstrating after_agent_callback\",\n",
        "        after_agent_callback=simple_after_agent_logger # Assign the function here\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(agent=my_llm_agent, app_name=\"llm_demo_app_after\", session_service=session_service)\n",
        "    session_id_run = \"llm_session_run_after_1\"\n",
        "    session_id_conclude = \"llm_session_conclude_1\"\n",
        "    user_id = \"llm_test_user_after\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(app_name=\"llm_demo_app_after\", user_id=user_id, session_id=session_id_run)\n",
        "    # Session where the callback will add a note\n",
        "    session_service.create_session(app_name=\"llm_demo_app_after\", user_id=user_id, session_id=session_id_conclude,\n",
        "                                  state={\"add_concluding_note\": True})\n",
        "\n",
        "    print(\"--- Running LLM Agent Normally (with after_agent_callback) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_run,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"Run normally\")])):\n",
        "        # Print any event content from agent or callback\n",
        "        if event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "    print(\"\\n--- Running LLM Agent with Concluding Note Condition ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_conclude,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"Run and conclude\")])):\n",
        "        # Print any event content from agent or callback\n",
        "         if event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGs43srmLq7y"
      },
      "source": [
        "### LLM Agent with before_model_callback and state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCxIFM8CLrNx",
        "outputId": "a277150a-8394-4eb9-f2d1-16729331f7c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running LLM Agent Normally (with before_model_callback modification) ---\n",
            "[Callback] Before model call for agent: ModelCallbackAgent\n",
            "[Callback] Inspecting last user message: 'Tell me a short joke.'\n",
            "[Callback] Modified system instruction to: '[Modified by Callback] You are a helpful assistant.\n",
            "\n",
            "You are an agent. Your internal name is \"ModelCallbackAgent\".\n",
            "\n",
            " The description about you is \"An LLM agent demonstrating before_model_callback\"'\n",
            "[Callback] Proceeding with LLM call.\n",
            "Event Output: ModelCallbackAgent: Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "--- Running LLM Agent with BLOCK Keyword (triggering skip) ---\n",
            "[Callback] Before model call for agent: ModelCallbackAgent\n",
            "[Callback] Inspecting last user message: 'BLOCK this request.'\n",
            "[Callback] Modified system instruction to: '[Modified by Callback] You are a helpful assistant.\n",
            "\n",
            "You are an agent. Your internal name is \"ModelCallbackAgent\".\n",
            "\n",
            " The description about you is \"An LLM agent demonstrating before_model_callback\"'\n",
            "[Callback] 'BLOCK' keyword found. Skipping LLM call.\n",
            "Event Output: ModelCallbackAgent: LLM call was blocked by before_model_callback.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "# Need LlmRequest and LlmResponse for the callback signature and return type\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\" # Or your preferred model\n",
        "\n",
        "# --- Define the Callback Function ---\n",
        "def simple_before_model_modifier(\n",
        "    callback_context: CallbackContext, llm_request: LlmRequest\n",
        ") -> Optional[LlmResponse]:\n",
        "    \"\"\"Inspects/modifies the LLM request or skips the call.\"\"\"\n",
        "    agent_name = callback_context.agent_name\n",
        "    print(f\"[Callback] Before model call for agent: {agent_name}\")\n",
        "\n",
        "    # Inspect the last user message in the request contents\n",
        "    last_user_message = \"\"\n",
        "    if llm_request.contents and llm_request.contents[-1].role == 'user':\n",
        "         if llm_request.contents[-1].parts:\n",
        "            last_user_message = llm_request.contents[-1].parts[0].text\n",
        "    print(f\"[Callback] Inspecting last user message: '{last_user_message}'\")\n",
        "\n",
        "    # --- Modification Example ---\n",
        "    # Add a prefix to the system instruction\n",
        "    original_instruction = llm_request.config.system_instruction or types.Content(role=\"system\", parts=[])\n",
        "    prefix = \"[Modified by Callback] \"\n",
        "    # Ensure system_instruction is Content and parts list exists\n",
        "    if not isinstance(original_instruction, types.Content):\n",
        "         # Handle case where it might be a string (though config expects Content)\n",
        "         original_instruction = types.Content(role=\"system\", parts=[types.Part(text=str(original_instruction))])\n",
        "    if not original_instruction.parts:\n",
        "        original_instruction.parts.append(types.Part(text=\"\")) # Add an empty part if none exist\n",
        "\n",
        "    # Modify the text of the first part\n",
        "    modified_text = prefix + (original_instruction.parts[0].text or \"\")\n",
        "    original_instruction.parts[0].text = modified_text\n",
        "    llm_request.config.system_instruction = original_instruction\n",
        "    print(f\"[Callback] Modified system instruction to: '{modified_text}'\")\n",
        "\n",
        "\n",
        "    # --- Skip Example ---\n",
        "    # Check if the last user message contains \"BLOCK\"\n",
        "    if \"BLOCK\" in last_user_message.upper():\n",
        "        print(\"[Callback] 'BLOCK' keyword found. Skipping LLM call.\")\n",
        "        # Return an LlmResponse to skip the actual LLM call\n",
        "        return LlmResponse(\n",
        "            content=types.Content(\n",
        "                role=\"model\",\n",
        "                parts=[types.Part(text=\"LLM call was blocked by before_model_callback.\")],\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        print(\"[Callback] Proceeding with LLM call.\")\n",
        "        # Return None to allow the (modified) request to go to the LLM\n",
        "        return None\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent and Assign Callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"ModelCallbackAgent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are a helpful assistant.\", # Base instruction\n",
        "        description=\"An LLM agent demonstrating before_model_callback\",\n",
        "        before_model_callback=simple_before_model_modifier # Assign the function here\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(agent=my_llm_agent, app_name=\"llm_model_cb_app\", session_service=session_service)\n",
        "    session_id_run = \"model_cb_run_1\"\n",
        "    session_id_block = \"model_cb_block_1\"\n",
        "    user_id = \"model_cb_user\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(app_name=\"llm_model_cb_app\", user_id=user_id, session_id=session_id_run)\n",
        "    session_service.create_session(app_name=\"llm_model_cb_app\", user_id=user_id, session_id=session_id_block)\n",
        "\n",
        "    print(\"--- Running LLM Agent Normally (with before_model_callback modification) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_run,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"Tell me a short joke.\")])):\n",
        "        # Only print final LLM response or callback override\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "    print(\"\\n--- Running LLM Agent with BLOCK Keyword (triggering skip) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_block,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"BLOCK this request.\")])):\n",
        "         # Only print final LLM response or callback override\n",
        "         if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qhO2G_wLrgl"
      },
      "source": [
        "### LLM Agent with after_model_callback and state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n38CnCEfLrw_",
        "outputId": "c17c46d0-06ea-4d12-8153-899eab7bbe6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running LLM Agent Normally (Callback passes response through) ---\n",
            "[Callback] After model call for agent: AfterModelCallbackAgent\n",
            "[Callback] Inspected original response text: 'Hello! I'm AfterModelCallbackAgent, an LLM agent designed to demonstrate the use of `after_model_cal...'\n",
            "[Callback] 'scientists' not found. Passing original response through.\n",
            "Event Output: AfterModelCallbackAgent: Hello! I'm AfterModelCallbackAgent, an LLM agent designed to demonstrate the use of `after_model_callback`. I'm here to help you explore how to use this feature!\n",
            "\n",
            "--- Running LLM Agent with Input Triggering Modification ---\n",
            "[Callback] After model call for agent: AfterModelCallbackAgent\n",
            "[Callback] Inspected original response text: 'Why don't scientists trust atoms? \n",
            "\n",
            "Because they make up everything!\n",
            "...'\n",
            "[Callback] Found 'scientists'. Modifying response.\n",
            "[Callback] Returning modified response.\n",
            "Event Output: AfterModelCallbackAgent: Why don't funny scientists trust atoms? \n",
            "\n",
            "Because they make up everything!\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional\n",
        "import copy # Needed to safely modify response content\n",
        "\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\" # Or your preferred model\n",
        "\n",
        "# --- Define the Callback Function ---\n",
        "def simple_after_model_modifier(\n",
        "    callback_context: CallbackContext, llm_response: LlmResponse\n",
        ") -> Optional[LlmResponse]:\n",
        "    \"\"\"Inspects/modifies the LLM response after it's received.\"\"\"\n",
        "    agent_name = callback_context.agent_name\n",
        "    print(f\"[Callback] After model call for agent: {agent_name}\")\n",
        "\n",
        "    # --- Inspection ---\n",
        "    original_text = \"\"\n",
        "    if llm_response.content and llm_response.content.parts:\n",
        "        # Assuming simple text response for this example\n",
        "        if llm_response.content.parts[0].text:\n",
        "            original_text = llm_response.content.parts[0].text\n",
        "            print(f\"[Callback] Inspected original response text: '{original_text[:100]}...'\") # Log snippet\n",
        "        elif llm_response.content.parts[0].function_call:\n",
        "             print(f\"[Callback] Inspected response: Contains function call '{llm_response.content.parts[0].function_call.name}'. No text modification.\")\n",
        "             return None # Don't modify tool calls in this example\n",
        "        else:\n",
        "             print(\"[Callback] Inspected response: No text content found.\")\n",
        "             return None\n",
        "    elif llm_response.error_message:\n",
        "        print(f\"[Callback] Inspected response: Contains error '{llm_response.error_message}'. No modification.\")\n",
        "        return None\n",
        "    else:\n",
        "        print(\"[Callback] Inspected response: Empty LlmResponse.\")\n",
        "        return None # Nothing to modify\n",
        "\n",
        "    # --- Modification Example ---\n",
        "    # Replace \"scientists\" with \"funny scientists\" (case-insensitive)\n",
        "    search_term = \"scientists\"\n",
        "    replace_term = \"funny scientists\"\n",
        "    if search_term in original_text.lower():\n",
        "        print(f\"[Callback] Found '{search_term}'. Modifying response.\")\n",
        "        modified_text = original_text.replace(search_term, replace_term)\n",
        "        modified_text = modified_text.replace(search_term.capitalize(), replace_term.capitalize()) # Handle capitalization\n",
        "\n",
        "        # Create a NEW LlmResponse with the modified content\n",
        "        # Deep copy parts to avoid modifying original if other callbacks exist\n",
        "        modified_parts = [copy.deepcopy(part) for part in llm_response.content.parts]\n",
        "        modified_parts[0].text = modified_text # Update the text in the copied part\n",
        "\n",
        "        new_response = LlmResponse(\n",
        "             content=types.Content(role=\"model\", parts=modified_parts),\n",
        "             # Copy other relevant fields if necessary, e.g., grounding_metadata\n",
        "             grounding_metadata=llm_response.grounding_metadata\n",
        "             )\n",
        "        print(f\"[Callback] Returning modified response.\")\n",
        "        return new_response # Return the modified response\n",
        "    else:\n",
        "        print(f\"[Callback] '{search_term}' not found. Passing original response through.\")\n",
        "        # Return None to use the original llm_response\n",
        "        return None\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent and Assign Callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"AfterModelCallbackAgent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are a helpful assistant.\",\n",
        "        description=\"An LLM agent demonstrating after_model_callback\",\n",
        "        after_model_callback=simple_after_model_modifier # Assign the function here\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(agent=my_llm_agent, app_name=\"llm_after_model_cb_app\", session_service=session_service)\n",
        "    session_id_run = \"after_model_cb_run_1\"\n",
        "    session_id_modify = \"after_model_cb_modify_1\"\n",
        "    user_id = \"after_model_cb_user\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(app_name=\"llm_after_model_cb_app\", user_id=user_id, session_id=session_id_run)\n",
        "    session_service.create_session(app_name=\"llm_after_model_cb_app\", user_id=user_id, session_id=session_id_modify)\n",
        "\n",
        "    print(\"--- Running LLM Agent Normally (Callback passes response through) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_run,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"Say hello.\")])):\n",
        "        # Only print final LLM response\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "    print(\"\\n--- Running LLM Agent with Input Triggering Modification ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_modify,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"Tell me a short joke.\")])):\n",
        "         # Only print final LLM response\n",
        "         if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNdkfIVGQ3Af"
      },
      "source": [
        "### LLM Agent with before_tool_callback and state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zsGbxXwQ3Po",
        "outputId": "0b14f3a7-0107-42ed-c5db-3b45e8d91d7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running Agent (Normal Tool Call - Germany) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Callback] Before tool call for tool 'get_capital_city' in agent 'ToolCallbackAgent'\n",
            "[Callback] Original args: {'country': 'Germany'}\n",
            "[Callback] Proceeding with original or previously modified args.\n",
            "--- Tool 'get_capital_city' executing with country: Germany ---\n",
            "Event Output: ToolCallbackAgent: The capital of Germany is Berlin.\n",
            "\n",
            "--- Running Agent (Tool Call Triggering Modification - Canada -> France) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Callback] Before tool call for tool 'get_capital_city' in agent 'ToolCallbackAgent'\n",
            "[Callback] Original args: {'country': 'Canada'}\n",
            "[Callback] Detected 'Canada'. Modifying args to 'France'.\n",
            "[Callback] Modified args: {'country': 'France'}\n",
            "--- Tool 'get_capital_city' executing with country: France ---\n",
            "Event Output: ToolCallbackAgent: The capital of France is Paris.\n",
            "\n",
            "--- Running Agent (Tool Call Triggering Skip - BLOCK) ---\n",
            "Event Output: ToolCallbackAgent: I cannot fulfill that request. The tool requires a valid country name, and 'BLOCK' is not a recognized country. Can you provide me with a valid country name?\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional, Dict, Any\n",
        "import copy\n",
        "\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "# Need LlmRequest, LlmResponse for context\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "# Need BaseTool, ToolContext for the callback signature\n",
        "from google.adk.tools.base_tool import BaseTool\n",
        "from google.adk.tools.tool_context import ToolContext\n",
        "# Using FunctionTool to easily create a tool\n",
        "from google.adk.tools.function_tool import FunctionTool\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\" # Or your preferred model\n",
        "\n",
        "# --- Define a Simple Tool Function ---\n",
        "def get_capital_city(country: str) -> str:\n",
        "    \"\"\"Retrieves the capital city of a given country.\"\"\"\n",
        "    print(f\"--- Tool 'get_capital_city' executing with country: {country} ---\")\n",
        "    country_capitals = {\n",
        "        \"united states\": \"Washington, D.C.\",\n",
        "        \"canada\": \"Ottawa\", # Intentionally correct here\n",
        "        \"france\": \"Paris\",\n",
        "        \"germany\": \"Berlin\",\n",
        "    }\n",
        "    return country_capitals.get(country.lower(), f\"Capital not found for {country}\")\n",
        "\n",
        "# --- Wrap the function into a Tool ---\n",
        "capital_tool = FunctionTool(func=get_capital_city)\n",
        "\n",
        "# --- Define the Callback Function ---\n",
        "def simple_before_tool_modifier(\n",
        "    tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext\n",
        ") -> Optional[Dict]:\n",
        "    \"\"\"Inspects/modifies tool args or skips the tool call.\"\"\"\n",
        "    agent_name = tool_context.agent_name\n",
        "    tool_name = tool.name\n",
        "    print(f\"[Callback] Before tool call for tool '{tool_name}' in agent '{agent_name}'\")\n",
        "    print(f\"[Callback] Original args: {args}\")\n",
        "\n",
        "    # --- Modification Example ---\n",
        "    # If the tool is 'get_capital_city' and country is 'Canada', change it to 'France'\n",
        "    if tool_name == 'get_capital_city' and args.get('country', '').lower() == 'canada':\n",
        "        print(\"[Callback] Detected 'Canada'. Modifying args to 'France'.\")\n",
        "        args['country'] = 'France' # Modify the args dictionary directly\n",
        "        print(f\"[Callback] Modified args: {args}\")\n",
        "        return None # Proceed with modified args\n",
        "\n",
        "    # --- Skip Example ---\n",
        "    # If the tool is 'get_capital_city' and country is 'BLOCK'\n",
        "    if tool_name == 'get_capital_city' and args.get('country', '').upper() == 'BLOCK':\n",
        "        print(\"[Callback] Detected 'BLOCK'. Skipping tool execution.\")\n",
        "        # Return a dictionary to be used as the tool result, skipping the actual tool call\n",
        "        return {\"result\": \"Tool execution was blocked by before_tool_callback.\"}\n",
        "\n",
        "    print(\"[Callback] Proceeding with original or previously modified args.\")\n",
        "    # Return None to allow the tool to execute normally (with original or modified args)\n",
        "    return None\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent with the tool and callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"ToolCallbackAgent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are an agent that can find capital cities. Use the get_capital_city tool.\",\n",
        "        description=\"An LLM agent demonstrating before_tool_callback\",\n",
        "        tools=[capital_tool], # Add the tool here\n",
        "        before_tool_callback=simple_before_tool_modifier # Assign the callback here\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(agent=my_llm_agent, app_name=\"llm_tool_cb_app\", session_service=session_service)\n",
        "    session_id_run = \"tool_cb_run_1\"\n",
        "    session_id_modify = \"tool_cb_modify_1\"\n",
        "    session_id_block = \"tool_cb_block_1\"\n",
        "    user_id = \"tool_cb_user\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(app_name=\"llm_tool_cb_app\", user_id=user_id, session_id=session_id_run)\n",
        "    session_service.create_session(app_name=\"llm_tool_cb_app\", user_id=user_id, session_id=session_id_modify)\n",
        "    session_service.create_session(app_name=\"llm_tool_cb_app\", user_id=user_id, session_id=session_id_block)\n",
        "\n",
        "    print(\"--- Running Agent (Normal Tool Call - Germany) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_run,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"What is the capital of Germany?\")])):\n",
        "        # Only print final LLM response\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "    print(\"\\n--- Running Agent (Tool Call Triggering Modification - Canada -> France) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_modify,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"What is the capital of Canada?\")])):\n",
        "         # Only print final LLM response\n",
        "         if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "    print(\"\\n--- Running Agent (Tool Call Triggering Skip - BLOCK) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_block,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"What is the capital of BLOCK?\")])):\n",
        "         # Only print final LLM response\n",
        "         if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-36dSreYQ3mG"
      },
      "source": [
        "### LLM Agent with after_tool_callback and state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEFUbgV3Q352",
        "outputId": "58bb6039-c6d4-4023-e626-7f4cd5de808f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running Agent (Callback passes result through - France) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Tool 'get_capital_city' executing with country: France ---\n",
            "[Callback] After tool call for tool 'get_capital_city' in agent 'AfterToolCallbackAgent'\n",
            "[Callback] Args used: {'country': 'France'}\n",
            "[Callback] Original tool_response: {'result': 'Paris'}\n",
            "[Callback] Passing original tool response through.\n",
            "Event Output: AfterToolCallbackAgent: The capital of France is Paris.\n",
            "\n",
            "--- Running Agent (Callback modifies result - United States) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Tool 'get_capital_city' executing with country: United States ---\n",
            "[Callback] After tool call for tool 'get_capital_city' in agent 'AfterToolCallbackAgent'\n",
            "[Callback] Args used: {'country': 'United States'}\n",
            "[Callback] Original tool_response: {'result': 'Washington, D.C.'}\n",
            "[Callback] Detected 'Washington, D.C.'. Modifying tool response.\n",
            "[Callback] Modified tool_response: {'result': 'Washington, D.C. (Note: This is the capital of the USA).', 'note_added_by_callback': True}\n",
            "Event Output: AfterToolCallbackAgent: The capital of the United States is Washington, D.C. (Note: This is the capital of the USA).\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from typing import AsyncGenerator, Optional, Dict, Any\n",
        "import copy # Good practice for modifying results\n",
        "\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.adk.tools.base_tool import BaseTool\n",
        "from google.adk.tools.tool_context import ToolContext\n",
        "from google.adk.tools.function_tool import FunctionTool\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.events import Event\n",
        "from google.genai import types\n",
        "\n",
        "# Ensure GEMINI_2_FLASH is defined (replace if needed)\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\" # Or your preferred model\n",
        "\n",
        "# --- Define a Simple Tool Function (Same as before) ---\n",
        "def get_capital_city(country: str) -> str:\n",
        "    \"\"\"Retrieves the capital city of a given country.\"\"\"\n",
        "    print(f\"--- Tool 'get_capital_city' executing with country: {country} ---\")\n",
        "    country_capitals = {\n",
        "        \"united states\": \"Washington, D.C.\",\n",
        "        \"canada\": \"Ottawa\",\n",
        "        \"france\": \"Paris\",\n",
        "        \"germany\": \"Berlin\",\n",
        "    }\n",
        "    return {\"result\": country_capitals.get(country.lower(), f\"Capital not found for {country}\")}\n",
        "\n",
        "# --- Wrap the function into a Tool ---\n",
        "capital_tool = FunctionTool(func=get_capital_city)\n",
        "\n",
        "# --- Define the Callback Function ---\n",
        "def simple_after_tool_modifier(\n",
        "    tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext, tool_response: Dict\n",
        ") -> Optional[Dict]:\n",
        "    \"\"\"Inspects/modifies the tool result after execution.\"\"\"\n",
        "    agent_name = tool_context.agent_name\n",
        "    tool_name = tool.name\n",
        "    print(f\"[Callback] After tool call for tool '{tool_name}' in agent '{agent_name}'\")\n",
        "    print(f\"[Callback] Args used: {args}\")\n",
        "    print(f\"[Callback] Original tool_response: {tool_response}\")\n",
        "\n",
        "    # Default structure for function tool results is {\"result\": <return_value>}\n",
        "    original_result_value = tool_response.get(\"result\", \"\")\n",
        "    # original_result_value = tool_response\n",
        "\n",
        "    # --- Modification Example ---\n",
        "    # If the tool was 'get_capital_city' and result is 'Washington, D.C.'\n",
        "    if tool_name == 'get_capital_city' and original_result_value == \"Washington, D.C.\":\n",
        "        print(\"[Callback] Detected 'Washington, D.C.'. Modifying tool response.\")\n",
        "\n",
        "        # IMPORTANT: Create a new dictionary or modify a copy\n",
        "        modified_response = copy.deepcopy(tool_response)\n",
        "        modified_response[\"result\"] = f\"{original_result_value} (Note: This is the capital of the USA).\"\n",
        "        modified_response[\"note_added_by_callback\"] = True # Add extra info if needed\n",
        "\n",
        "        print(f\"[Callback] Modified tool_response: {modified_response}\")\n",
        "        return modified_response # Return the modified dictionary\n",
        "\n",
        "    print(\"[Callback] Passing original tool response through.\")\n",
        "    # Return None to use the original tool_response\n",
        "    return None\n",
        "\n",
        "# --- Setup and Run ---\n",
        "async def main():\n",
        "    # 1. Create LlmAgent with the tool and callback\n",
        "    my_llm_agent = LlmAgent(\n",
        "        name=\"AfterToolCallbackAgent\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are an agent that finds capital cities using the get_capital_city tool. Report the result clearly.\",\n",
        "        description=\"An LLM agent demonstrating after_tool_callback\",\n",
        "        tools=[capital_tool], # Add the tool\n",
        "        after_tool_callback=simple_after_tool_modifier # Assign the callback\n",
        "    )\n",
        "\n",
        "    # 2. Setup Runner and Session\n",
        "    session_service = InMemorySessionService()\n",
        "    runner = Runner(agent=my_llm_agent, app_name=\"llm_after_tool_cb_app\", session_service=session_service)\n",
        "    session_id_run = \"after_tool_cb_run_1\"\n",
        "    session_id_modify = \"after_tool_cb_modify_1\"\n",
        "    user_id = \"after_tool_cb_user\"\n",
        "\n",
        "    # Create sessions\n",
        "    session_service.create_session(app_name=\"llm_after_tool_cb_app\", user_id=user_id, session_id=session_id_run)\n",
        "    session_service.create_session(app_name=\"llm_after_tool_cb_app\", user_id=user_id, session_id=session_id_modify)\n",
        "\n",
        "    print(\"--- Running Agent (Callback passes result through - France) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_run,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"What is the capital of France?\")])):\n",
        "        # Only print final LLM response\n",
        "        if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "    print(\"\\n--- Running Agent (Callback modifies result - United States) ---\")\n",
        "    async for event in runner.run_async(user_id=user_id, session_id=session_id_modify,\n",
        "                                        new_message=types.Content(role=\"user\",\n",
        "                                                                  parts=[types.Part(text=\"What is the capital of the United States?\")])):\n",
        "         # Only print final LLM response\n",
        "         if event.is_final_response() and event.content:\n",
        "            print(f\"Event Output: {event.author}: {event.content.parts[0].text.strip()}\")\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yecR7byQ01bQ"
      },
      "source": [
        "### LLM Agent with Gaurdrail (Profanity Checker with before_model callback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xexSYqxPuSiH"
      },
      "outputs": [],
      "source": [
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.agents.llm_agent import AfterModelCallback, BeforeModelCallback\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.adk.agents.llm_agent import LlmAgent\n",
        "from typing import Any, List, Optional\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.agents import LlmAgent\n",
        "\n",
        "\n",
        "def profanity_guardrail(\n",
        "    callback_context: CallbackContext, llm_request: LlmRequest\n",
        ") -> Optional[LlmResponse]:\n",
        "    \"\"\"Check for profanity in the model request.\"\"\"\n",
        "    profanity_list: List[str] = [\"badword1\", \"badword2\", \"badword3\"]\n",
        "    if llm_request.contents:\n",
        "        for content in llm_request.contents:\n",
        "            for part in content.parts:\n",
        "                if part.text:\n",
        "                    for profanity in profanity_list:\n",
        "                        if profanity in part.text.lower():\n",
        "                            callback_context.state[\"profanity_trigger\"] = True\n",
        "                            return LlmResponse(\n",
        "                                content=types.Content(\n",
        "                                    role=\"model\",\n",
        "                                    parts=[\n",
        "                                        types.Part(\n",
        "                                            text=(\n",
        "                                                \"No bad word allowed.\"\n",
        "                                            )\n",
        "                                        )\n",
        "                                    ],\n",
        "                                )\n",
        "                            )\n",
        "    return None\n",
        "\n",
        "\n",
        "# Tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Retrieves weather information for the given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city for which to retrieve weather information.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the weather information for the specified city,\n",
        "        or a message indicating that the weather information was not found.\n",
        "    \"\"\"\n",
        "    cities = {\n",
        "        'chicago': {'temperature': 25, 'condition': 'sunny', 'sky': 'clear'},\n",
        "        'toronto': {'temperature': 30, 'condition': 'partly cloudy', 'sky': 'overcast'},\n",
        "        'chennai': {'temperature': 15, 'condition': 'rainy', 'sky': 'cloudy'},\n",
        "    }\n",
        "\n",
        "    city_lower = city.lower()\n",
        "    if city_lower in cities:\n",
        "        weather_data = cities[city_lower]\n",
        "        return f\"Weather in {city} is {weather_data['temperature']} degrees Celsius, {weather_data['condition']} with a {weather_data['sky']} sky.\"\n",
        "    else:\n",
        "        return f\"Weather information for {city} not found.\"\n",
        "\n",
        "async def run_query(query: str):\n",
        "    weather_agent = LlmAgent(\n",
        "        model=GEMINI_2_FLASH,\n",
        "        name=\"weather_agent\",\n",
        "        instruction=\"\"\"You are a Weather Information Agent. Your task is to provide weather information for a given city.\n",
        "\n",
        "        When a user provides a prompt, extract the city name.\n",
        "        Then, use the `get_weather` tool to retrieve the weather information for that city.\n",
        "        Finally, present the weather information to the user in a clear and concise manner.\n",
        "        If the user asks for a greeting, transfer to the greeting agent.\"\"\",\n",
        "        description=\"\"\"You are an agent who can fetch weather information for a city.\n",
        "        You have access to the `get_weather` tool to accomplish this task.\"\"\",\n",
        "        tools=[get_weather],\n",
        "        before_model_callback=profanity_guardrail,\n",
        "    )\n",
        "\n",
        "    session_service = InMemorySessionService()\n",
        "    session = session_service.create_session(\n",
        "        app_name=\"weather_app\", user_id=\"12345\", session_id=\"123344\"\n",
        "    )\n",
        "    runner = Runner(\n",
        "        agent=weather_agent,\n",
        "        app_name=\"weather_app\",\n",
        "        session_service=session_service,\n",
        "    )\n",
        "\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    async for event in runner.run_async(\n",
        "        user_id=\"12345\", session_id=\"123344\", new_message=content\n",
        "    ):\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "            print(f\"Query: {query}\")\n",
        "            print(f\"Agent Response: {final_response}\")\n",
        "            print(\"-\" * 20)\n",
        "        if \"profanity_trigger\" in event.actions.state_delta:\n",
        "            print(f\"Profanity Triggered: {event.actions.state_delta['profanity_trigger']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K2D004BzaEv",
        "outputId": "b2b789fb-166c-4392-9721-9b4ec7156e14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What is the weather in Chicago?\n",
            "Agent Response: The weather in Chicago is 25 degrees Celsius, sunny with a clear sky.\n",
            "\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "await run_query(\"What is the weather in Chicago?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3L1N4AXza-C",
        "outputId": "4efaf126-23e5-43ea-ec15-8d5173a586a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: what the badword1 is the weather in Chicago?\n",
            "Agent Response: No bad word allowed.\n",
            "--------------------\n",
            "Profanity Triggered: True\n"
          ]
        }
      ],
      "source": [
        "await run_query(\"what the badword1 is the weather in Chicago?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG4lwya4iUoX"
      },
      "source": [
        "### LlmAgent with All Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a89KSH0tiUb1",
        "outputId": "f6159277-4327-47fe-f8e5-3472906376f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Workflow for Query: 'Help! My computer screen keeps flickering constantly.' ---\n",
            "\n",
            "[User Submits Ticket: 'Help! My computer screen keeps flickering constantly.']\n",
            "\n",
            "[Callback Triggered: before_agent_callback]\n",
            "  -> Processing Ticket ID: TICKET-ABC\n",
            "  -> Agent 'support_agent' starting.\n",
            "\n",
            "[Callback Triggered: before_model_callback]\n",
            "  -> Preparing to call LLM for agent 'support_agent'.\n",
            "  -> Safety check/prompt augmentation applied (simulated).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Callback Triggered: after_model_callback]\n",
            "  -> Received LLM response for agent 'support_agent'.\n",
            "  -> LLM Raw Response (brief): Okay, I understand your computer screen is flickering. I can help you with that.\n",
            "\n",
            "...\n",
            "  -> PII check passed (simulated).\n",
            "\n",
            "[AFW: LLM decided to use Tool 'kb_search']\n",
            "\n",
            "[Callback Triggered: before_tool_callback for Tool: 'kb_search']\n",
            "  -> Attempting to call tool 'kb_search' with args: {'keywords': ['screen', 'flickering']}\n",
            "  -> Keyword validation passed.\n",
            "      [Tool Executing: kb_search with keywords: ['screen', 'flickering']]\n",
            "      [Tool Result: Found 4 steps]\n",
            "\n",
            "[Callback Triggered: after_tool_callback for Tool: 'kb_search']\n",
            "  -> Tool 'kb_search' executed with args: {'keywords': ['screen', 'flickering']}\n",
            "  -> Received tool response (brief): {'solutions': ['Check the display cable connection.', 'Try a different monitor port.', 'Update the graphics driver.', 'Adjust the screen refresh rate.']}...\n",
            "  -> Caching tool result (simulated).\n",
            "\n",
            "[AFW: Received result from Tool 'kb_search']\n",
            "  -> AFW: Sending tool result back to LLM for final response generation...\n",
            "\n",
            "[Callback Triggered: before_model_callback]\n",
            "  -> Preparing to call LLM for agent 'support_agent'.\n",
            "  -> Safety check/prompt augmentation applied (simulated).\n",
            "\n",
            "[Callback Triggered: after_model_callback]\n",
            "  -> Received LLM response for agent 'support_agent'.\n",
            "  -> LLM Raw Response (brief): Okay, I have some troubleshooting steps for you. Please try the following:\n",
            "\n",
            "1.  **Check the display ...\n",
            "  -> PII check passed (simulated).\n",
            "\n",
            "[AFW: Sending Final Response to User]\n",
            "  -> Response: Okay, I have some troubleshooting steps for you. Please try the following:\n",
            "\n",
            "1.  **Check the display cable connection:** Make sure the cable connecting your monitor to your computer is securely plugged in at both ends.\n",
            "2.  **Try a different monitor port:** If you're using a desktop, try connecting the monitor to a different port on your graphics card (e.g., try a different HDMI or DisplayPort).\n",
            "3.  **Update the graphics driver:** Outdated or corrupted graphics drivers can cause screen flickering. You can usually download the latest drivers from the manufacturer's website (NVIDIA, AMD, or Intel).\n",
            "4.  **Adjust the screen refresh rate:** Sometimes, an incorrect refresh rate can cause flickering. Go to your display settings and try a different refresh rate.\n",
            "\n",
            "Let me know if any of these steps resolve the issue!\n",
            "\n",
            "\n",
            "[Callback Triggered: after_agent_callback]\n",
            "  -> Finished processing for Ticket ID: TICKET-ABC.\n",
            "  -> Updating external system: Ticket TICKET-ABC status set to 'Responded'.\n",
            "\n",
            "--- Workflow Finished for Query: 'Help! My computer screen keeps flickering constantly.' ---\n",
            "Final Session State (example): ticket_id='TICKET-ABC', status='completed'\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Okay, I have some troubleshooting steps for you. Please try the following:\\n\\n1.  **Check the display cable connection:** Make sure the cable connecting your monitor to your computer is securely plugged in at both ends.\\n2.  **Try a different monitor port:** If you're using a desktop, try connecting the monitor to a different port on your graphics card (e.g., try a different HDMI or DisplayPort).\\n3.  **Update the graphics driver:** Outdated or corrupted graphics drivers can cause screen flickering. You can usually download the latest drivers from the manufacturer's website (NVIDIA, AMD, or Intel).\\n4.  **Adjust the screen refresh rate:** Sometimes, an incorrect refresh rate can cause flickering. Go to your display settings and try a different refresh rate.\\n\\nLet me know if any of these steps resolve the issue!\\n\""
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import asyncio\n",
        "import warnings\n",
        "import os\n",
        "from typing import Any, Optional, Dict, List, AsyncGenerator\n",
        "\n",
        "# --- ADK Imports ---\n",
        "from google.adk.agents import Agent, LlmAgent, BaseAgent # Using LlmAgent directly\n",
        "from google.adk.sessions import InMemorySessionService, Session, State\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.adk.tools.base_tool import BaseTool\n",
        "from google.adk.tools.tool_context import ToolContext\n",
        "from google.adk.tools.function_tool import FunctionTool\n",
        "from google.adk.events import Event\n",
        "from google.adk.agents.invocation_context import InvocationContext\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.genai import types\n",
        "\n",
        "# Suppress specific UserWarning from google.generativeai if necessary\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='google.generativeai.types.content_types')\n",
        "\n",
        "# --- Constants ---\n",
        "APP_NAME = \"support_ticket_app\"\n",
        "USER_ID = \"customer_123\"\n",
        "SESSION_ID = \"ticket_session_abc\"\n",
        "AGENT_NAME = \"support_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\" # Or your preferred Gemini model\n",
        "\n",
        "# Ensure Project and Location are set if using Vertex AI backend\n",
        "# Example:\n",
        "# os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"your-gcp-project-id\"\n",
        "# os.environ[\"GOOGLE_CLOUD_LOCATION\"] = \"us-central1\"\n",
        "# os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\"\n",
        "\n",
        "\n",
        "# --- Simulated Knowledge Base Tool ---\n",
        "def kb_search(keywords: List[str]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Searches the knowledge base for troubleshooting steps based on keywords.\n",
        "\n",
        "    Args:\n",
        "        keywords: A list of keywords related to the issue (e.g., ['screen', 'flickering']).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing potential solutions or an empty dictionary if none found.\n",
        "    \"\"\"\n",
        "    print(f\"      [Tool Executing: kb_search with keywords: {keywords}]\")\n",
        "    # Simple mock implementation\n",
        "    mock_kb = {\n",
        "        \"screen\": [\"Check the display cable connection.\", \"Try a different monitor port.\"],\n",
        "        \"flickering\": [\"Update the graphics driver.\", \"Adjust the screen refresh rate.\"],\n",
        "        \"display\": [\"Ensure monitor power is on.\", \"Reboot the computer.\"],\n",
        "        \"keyboard\": [\"Check battery if wireless.\", \"Try a different USB port.\"]\n",
        "    }\n",
        "    results = []\n",
        "    for keyword in keywords:\n",
        "        if keyword.lower() in mock_kb:\n",
        "            results.extend(mock_kb[keyword.lower()])\n",
        "\n",
        "    if results:\n",
        "        # Remove duplicates while preserving order (if Python 3.7+)\n",
        "        unique_results = list(dict.fromkeys(results))\n",
        "        print(f\"      [Tool Result: Found {len(unique_results)} steps]\")\n",
        "        return {\"solutions\": unique_results}\n",
        "    else:\n",
        "        print(\"      [Tool Result: No relevant KB articles found]\")\n",
        "        return {\"solutions\": [\"No specific troubleshooting steps found in KB for these keywords.\"]}\n",
        "\n",
        "# Wrap the function into a FunctionTool\n",
        "kb_search_tool = FunctionTool(func=kb_search)\n",
        "\n",
        "\n",
        "# --- Callback Implementations ---\n",
        "\n",
        "def log_before_agent(callback_context: CallbackContext) -> Optional[types.Content]:\n",
        "    \"\"\"Callback executed before the agent starts processing.\"\"\"\n",
        "    ticket_id = f\"TICKET-{SESSION_ID.split('_')[-1].upper()}\" # Simulate getting ticket ID\n",
        "    print(f\"\\n[Callback Triggered: before_agent_callback]\")\n",
        "    print(f\"  -> Processing Ticket ID: {ticket_id}\")\n",
        "    # Example state modification: Store ticket ID\n",
        "    callback_context.state[\"ticket_id\"] = ticket_id\n",
        "    print(f\"  -> Agent '{callback_context.agent_name}' starting.\")\n",
        "    return None # Return None to allow agent execution\n",
        "\n",
        "def log_after_agent(callback_context: CallbackContext) -> Optional[types.Content]:\n",
        "    \"\"\"Callback executed after the agent finishes processing.\"\"\"\n",
        "    ticket_id = callback_context.state.get(\"ticket_id\", \"UNKNOWN\")\n",
        "    print(f\"\\n[Callback Triggered: after_agent_callback]\")\n",
        "    print(f\"  -> Finished processing for Ticket ID: {ticket_id}.\")\n",
        "    # Example: Simulate updating ticket status in an external system\n",
        "    print(f\"  -> Updating external system: Ticket {ticket_id} status set to 'Responded'.\")\n",
        "    # Example state modification\n",
        "    callback_context.state[\"processing_status\"] = \"completed\"\n",
        "    return None # Return None, we don't want to append extra content here\n",
        "\n",
        "def log_before_model(callback_context: CallbackContext, llm_request: LlmRequest) -> Optional[LlmResponse]:\n",
        "    \"\"\"Callback executed before sending the request to the LLM.\"\"\"\n",
        "    print(f\"\\n[Callback Triggered: before_model_callback]\")\n",
        "    print(f\"  -> Preparing to call LLM for agent '{callback_context.agent_name}'.\")\n",
        "    # Example: Log request details (be careful with PII in real scenarios)\n",
        "    # print(f\"  -> LLM Request Contents (brief): {str(llm_request.contents)[:200]}...\")\n",
        "    # Example: Add a safety reminder (Note: modifying system_instruction directly might be overwritten)\n",
        "    # llm_request.append_instructions([\"Remember to be helpful and safe.\"]) # Use append_instructions\n",
        "    print(f\"  -> Safety check/prompt augmentation applied (simulated).\")\n",
        "    return None # Return None to proceed with LLM call\n",
        "\n",
        "def check_after_model(callback_context: CallbackContext, llm_response: LlmResponse) -> Optional[LlmResponse]:\n",
        "    \"\"\"Callback executed after receiving the response from the LLM.\"\"\"\n",
        "    print(f\"\\n[Callback Triggered: after_model_callback]\")\n",
        "    print(f\"  -> Received LLM response for agent '{callback_context.agent_name}'.\")\n",
        "    # Example: Log response details (be careful with PII)\n",
        "    response_text = llm_response.content.parts[0].text if llm_response.content and llm_response.content.parts else \"[No Text]\"\n",
        "    print(f\"  -> LLM Raw Response (brief): {response_text[:100]}...\")\n",
        "    # Example: Simulate PII check\n",
        "    if \"password\" in response_text.lower() or \"credit card\" in response_text.lower():\n",
        "        print(\"  -> !! PII potentially detected in LLM response (simulated) !!\")\n",
        "        # Could modify response here, e.g., return an error or redacted text\n",
        "        # return LlmResponse(content=types.Content(parts=[types.Part(text=\"[Response redacted due to potential PII]\")]))\n",
        "    else:\n",
        "        print(\"  -> PII check passed (simulated).\")\n",
        "    return None # Return None to use the original (or potentially modified) LLM response\n",
        "\n",
        "def validate_before_tool(tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext) -> Optional[Dict]:\n",
        "    \"\"\"Callback executed before a tool is called.\"\"\"\n",
        "    print(f\"\\n[Callback Triggered: before_tool_callback for Tool: '{tool.name}']\")\n",
        "    print(f\"  -> Attempting to call tool '{tool.name}' with args: {args}\")\n",
        "    # Example: Validate arguments\n",
        "    if tool.name == \"kb_search\" and \"keywords\" in args:\n",
        "        if not isinstance(args[\"keywords\"], list) or not args[\"keywords\"]:\n",
        "            print(\"  -> !! Validation Failed: Keywords must be a non-empty list. Skipping tool call. !!\")\n",
        "            return {\"error\": \"Invalid keywords provided for KB search.\"} # Return error to LLM\n",
        "        print(\"  -> Keyword validation passed.\")\n",
        "    return None # Return None to proceed with actual tool execution\n",
        "\n",
        "def log_after_tool(tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext, tool_response: Dict) -> Optional[Dict]:\n",
        "    \"\"\"Callback executed after a tool has run.\"\"\"\n",
        "    print(f\"\\n[Callback Triggered: after_tool_callback for Tool: '{tool.name}']\")\n",
        "    print(f\"  -> Tool '{tool.name}' executed with args: {args}\")\n",
        "    print(f\"  -> Received tool response (brief): {str(tool_response)[:200]}...\")\n",
        "    # Example: Caching simulation (just log it)\n",
        "    print(f\"  -> Caching tool result (simulated).\")\n",
        "    # Example: Modify response if needed\n",
        "    # if \"solutions\" in tool_response and tool_response[\"solutions\"]:\n",
        "    #    tool_response[\"solutions\"].append(\"Also, try restarting your device.\") # Append suggestion\n",
        "    return None # Return None to use the original (or modified) tool response\n",
        "\n",
        "\n",
        "# --- Agent Definition ---\n",
        "support_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are an IT Support Agent. Your goal is to help users troubleshoot technical issues.\n",
        "1. Analyze the user's problem description (support ticket).\n",
        "2. Identify keywords related to the issue.\n",
        "3. If the issue relates to common hardware problems like 'screen', 'display', 'flickering', 'keyboard', use the `kb_search` tool with the identified keywords to find troubleshooting steps.\n",
        "4. Based on your analysis and any results from the `kb_search` tool, provide a clear, step-by-step response to the user.\n",
        "5. If the `kb_search` tool doesn't return useful information, state that and provide general troubleshooting advice (e.g., restart, check connections).\n",
        "\"\"\",\n",
        "    description=\"First-level IT support agent that analyzes issues and uses a knowledge base.\",\n",
        "    tools=[kb_search_tool],\n",
        "    allow_transfer=False,\n",
        "\n",
        "    # --- Assign Callbacks ---\n",
        "    before_agent_callback=log_before_agent,\n",
        "    after_agent_callback=log_after_agent,\n",
        "    before_model_callback=log_before_model,\n",
        "    after_model_callback=check_after_model,\n",
        "    before_tool_callback=validate_before_tool,\n",
        "    after_tool_callback=log_after_tool,\n",
        ")\n",
        "\n",
        "# --- Session and Runner Setup ---\n",
        "session_service = InMemorySessionService()\n",
        "# Ensure session is created before running\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=support_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "# --- Agent Interaction Logic ---\n",
        "async def call_agent_and_show_flow(query):\n",
        "  print(f\"\\n--- Starting Workflow for Query: '{query}' ---\")\n",
        "  print(f\"\\n[User Submits Ticket: '{query}']\")\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  final_response_text = \"[Agent did not produce a final response text]\"\n",
        "\n",
        "  async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
        "      # Print event details to trace the flow\n",
        "      # print(f\"\\nDEBUG Event: Author={event.author}, Partial={event.partial}, Final={event.is_final_response()}, Content={str(event.content)[:100]}...\")\n",
        "\n",
        "      if event.get_function_calls():\n",
        "           print(f\"\\n[AFW: LLM decided to use Tool '{event.get_function_calls()[0].name}']\")\n",
        "           # Before tool callback is triggered internally by the runner/flow\n",
        "\n",
        "      elif event.get_function_responses():\n",
        "           print(f\"\\n[AFW: Received result from Tool '{event.get_function_responses()[0].name}']\")\n",
        "           # After tool callback is triggered internally by the runner/flow\n",
        "           print(\"  -> AFW: Sending tool result back to LLM for final response generation...\")\n",
        "\n",
        "      if event.is_final_response() and event.content and event.content.parts:\n",
        "          # Check if there's text before accessing parts[0]\n",
        "          if event.content.parts[0].text:\n",
        "               final_response_text = event.content.parts[0].text\n",
        "               print(f\"\\n[AFW: Sending Final Response to User]\")\n",
        "               print(f\"  -> Response: {final_response_text}\")\n",
        "          else:\n",
        "               print(f\"\\n[AFW: Final Response - Non-text content received: {event.content.parts}]\")\n",
        "               final_response_text = \"[Non-text final response]\"\n",
        "          # After agent callback will be triggered after this loop finishes internally\n",
        "\n",
        "  print(f\"\\n--- Workflow Finished for Query: '{query}' ---\")\n",
        "  # Retrieve final state to show callback modification\n",
        "  final_session = session_service.get_session(APP_NAME, USER_ID, SESSION_ID)\n",
        "  print(f\"Final Session State (example): ticket_id='{final_session.state.get('ticket_id')}', status='{final_session.state.get('processing_status')}'\")\n",
        "  return final_response_text\n",
        "\n",
        "\n",
        "await call_agent_and_show_flow(\"Help! My computer screen keeps flickering constantly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muBWkgw38ZqH"
      },
      "source": [
        "LlmAgent with Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ogerj1niUYc",
        "outputId": "f0b666b9-336b-4990-ee56-1b634d1357dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  The current weather in London is cloudy with a temperature of 18 degrees Celsius and a chance of rain.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.tools import FunctionTool\n",
        "\n",
        "\n",
        "# Tool 1\n",
        "def get_weather_report(city: str) -> dict:\n",
        "    \"\"\"Retrieves the current weather report for a specified city.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the weather information with a 'status' key ('success' or 'error') and a 'report' key with the weather details if successful, or an 'error_message' if an error occurred.\n",
        "    \"\"\"\n",
        "    if city.lower() == \"london\":\n",
        "        return {\"status\": \"success\", \"report\": \"The current weather in London is cloudy with a temperature of 18 degrees Celsius and a chance of rain.\"}\n",
        "    elif city.lower() == \"paris\":\n",
        "        return {\"status\": \"success\", \"report\": \"The weather in Paris is sunny with a temperature of 25 degrees Celsius.\"}\n",
        "    else:\n",
        "        return {\"status\": \"error\", \"error_message\": f\"Weather information for '{city}' is not available.\"}\n",
        "\n",
        "weather_tool = FunctionTool(func=get_weather_report)\n",
        "\n",
        "\n",
        "# Tool 2\n",
        "def analyze_sentiment(text: str) -> dict:\n",
        "    \"\"\"Analyzes the sentiment of the given text.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with 'sentiment' ('positive', 'negative', or 'neutral') and a 'confidence' score.\n",
        "    \"\"\"\n",
        "    if \"good\" in text.lower() or \"sunny\" in text.lower():\n",
        "        return {\"sentiment\": \"positive\", \"confidence\": 0.8}\n",
        "    elif \"rain\" in text.lower() or \"bad\" in text.lower():\n",
        "        return {\"sentiment\": \"negative\", \"confidence\": 0.7}\n",
        "    else:\n",
        "        return {\"sentiment\": \"neutral\", \"confidence\": 0.6}\n",
        "\n",
        "sentiment_tool = FunctionTool(func=analyze_sentiment)\n",
        "\n",
        "\n",
        "# Agent\n",
        "weather_sentiment_agent = Agent(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    name='weather_sentiment_agent',\n",
        "    instruction=\"\"\"You are a helpful assistant that provides weather information and analyzes the sentiment of user feedback.\n",
        "\n",
        "**If the user asks about the weather in a specific city, use the 'get_weather_report' tool to retrieve the weather details.**\n",
        "\n",
        "**If the 'get_weather_report' tool returns a 'success' status, provide the weather report to the user.**\n",
        "\n",
        "**If the 'get_weather_report' tool returns an 'error' status, inform the user that the weather information for the specified city is not available and ask if they have another city in mind.**\n",
        "\n",
        "**After providing a weather report, if the user gives feedback on the weather (e.g., 'That's good' or 'I don't like rain'), use the 'analyze_sentiment' tool to understand their sentiment.** Then, briefly acknowledge their sentiment.\n",
        "\n",
        "You can handle these tasks sequentially if needed.\"\"\",\n",
        "    tools=[weather_tool, sentiment_tool]\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=weather_sentiment_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "  for event in events:\n",
        "      if event.is_final_response():\n",
        "          final_response = event.content.parts[0].text\n",
        "          print(\"Agent Response: \", final_response)\n",
        "\n",
        "call_agent(\"What's the weather in london?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPHRfE4LiUVX",
        "outputId": "a794f310-a2f6-4de3-8ade-dfa1051af56a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  I understand you're not a fan of rain.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "call_agent(\"I don't like rain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeIyQycIiUE6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6BDA5a38zdh"
      },
      "source": [
        "### LLMAgent with FunctionTool, ToolContext and Input/Output Schema - Example 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFQ29opgxJPR"
      },
      "outputs": [],
      "source": [
        "from google.adk.tools.function_tool import FunctionTool\n",
        "from google.adk.tools.tool_context import ToolContext\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "\n",
        "# Constants\n",
        "APP_NAME = \"recipe_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"recipe_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# --- Mock Data ---\n",
        "recipes = {\n",
        "    \"pasta carbonara\": {\n",
        "        \"ingredients\": [\"pasta\", \"eggs\", \"guanciale\", \"pecorino romano\", \"black pepper\"],\n",
        "        \"dietary_restrictions\": [\"none\"],\n",
        "    },\n",
        "    \"chicken tikka masala\": {\n",
        "        \"ingredients\": [\"chicken\", \"yogurt\", \"ginger\", \"garlic\", \"masala blend\"],\n",
        "        \"dietary_restrictions\": [\"none\"],\n",
        "    },\n",
        "    \"vegan lentil soup\": {\n",
        "        \"ingredients\": [\"lentils\", \"carrots\", \"celery\", \"onion\", \"vegetable broth\"],\n",
        "        \"dietary_restrictions\": [\"vegan\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "# --- Input/Output Schemas ---\n",
        "\n",
        "class SearchRecipesInput(BaseModel):\n",
        "    keyword: str = Field(description=\"The keyword to search for (e.g., ingredient).\")\n",
        "\n",
        "class SearchRecipesOutput(BaseModel):\n",
        "    recipes: List[str] = Field(description=\"List of recipe names matching the keyword.\")\n",
        "    message: Optional[str] = Field(default=None, description=\"Informative message if no recipes are found.\")\n",
        "\n",
        "\n",
        "class CheckDietaryRestrictionsInput(BaseModel):\n",
        "    recipe_name: str = Field(description=\"The name of the recipe to check.\")\n",
        "    dietary_restriction: str = Field(description=\"The dietary restriction to check for (e.g., 'vegan').\")\n",
        "\n",
        "class CheckDietaryRestrictionsOutput(BaseModel):\n",
        "    is_suitable: bool = Field(description=\"Whether the recipe is suitable for the given dietary restriction.\")\n",
        "    message: str = Field(description=\"Informative message about the suitability.\")\n",
        "\n",
        "\n",
        "class GetIngredientListInput(BaseModel):\n",
        "    recipe_name: str = Field(description=\"The name of the recipe.\")\n",
        "\n",
        "class GetIngredientListOutput(BaseModel):\n",
        "    ingredients: List[str] = Field(description=\"List of ingredients for the recipe.\")\n",
        "    message: Optional[str] = Field(default=None, description=\"Informative message if the recipe is not found.\")\n",
        "\n",
        "class RecipeAgentInput(BaseModel):\n",
        "    query: str = Field(description=\"The user's query to the recipe agent.\")\n",
        "\n",
        "class RecipeAgentOutput(BaseModel):\n",
        "    response: str = Field(description=\"The agent's response to the user's query.\")\n",
        "\n",
        "\n",
        "# --- Tools ---\n",
        "\n",
        "## FunctionTool\n",
        "\n",
        "# `FunctionTool` is a wrapper that transforms a regular Python function into a tool that an `LlmAgent` can use. It handles:\n",
        "# - **Function Declaration:**  Automatically generates a function declaration (name, description, parameters) that the LLM understands.  This declaration tells the LLM how to call the function.\n",
        "# - **Input/Output Conversion:** Converts between the LLM's JSON-based function call arguments and the Python function's arguments, and vice-versa.\n",
        "# - **Schema Validation:**  (Optional, but highly recommended) Uses `pydantic` schemas to validate the input and output of the function, ensuring type safety and data integrity.\n",
        "\n",
        "## ToolContext\n",
        "\n",
        "# `ToolContext` provides contextual information to a tool when it's invoked.  This information is *automatically* injected by the framework.  It includes:\n",
        "# - **Session Information:**  Access to the current session (user ID, session ID, etc.).\n",
        "# - **Artifact Management:** Methods for loading and saving artifacts (files, data) associated with the session.\n",
        "# - **State Access:** Read and modify the session state.\n",
        "# - **Other Context:**  Information about the current invocation, such as the agent being run.\n",
        "\n",
        "def search_recipes(keyword: str, tool_context: ToolContext) -> str:\n",
        "    \"\"\"Searches for recipes based on a keyword.\"\"\"\n",
        "    print(f\"Tool Context in search_recipes: {tool_context.__dict__}\")\n",
        "    matching_recipes = [\n",
        "        recipe_name\n",
        "        for recipe_name, recipe_data in recipes.items()\n",
        "        if keyword.lower() in recipe_name.lower() or keyword.lower() in recipe_data[\"ingredients\"]\n",
        "    ]\n",
        "    if matching_recipes:\n",
        "        output = SearchRecipesOutput(recipes=matching_recipes)\n",
        "        return f\"Recipes matching '{keyword}': {', '.join(output.recipes)}.\"\n",
        "    else:\n",
        "        output = SearchRecipesOutput(message=f\"No recipes found matching '{keyword}'.\")\n",
        "        return output.message\n",
        "\n",
        "\n",
        "def check_dietary_restrictions(recipe_name: str, dietary_restriction: str, tool_context: ToolContext) -> str:\n",
        "    \"\"\"Checks if a recipe is suitable for a given dietary restriction.\"\"\"\n",
        "    print(f\"Tool Context in check_dietary_restrictions: {tool_context}\")\n",
        "\n",
        "    recipe_data = recipes.get(recipe_name.lower())\n",
        "    if recipe_data:\n",
        "        if dietary_restriction.lower() in recipe_data[\"dietary_restrictions\"]:\n",
        "            output = CheckDietaryRestrictionsOutput(is_suitable=True, message=f\"'{recipe_name}' is suitable for a '{dietary_restriction}' diet.\")\n",
        "            return output.message\n",
        "        else:\n",
        "            output = CheckDietaryRestrictionsOutput(is_suitable=False, message=f\"'{recipe_name}' is not suitable for a '{dietary_restriction}' diet.\")\n",
        "            return output.message\n",
        "    else:\n",
        "        output = CheckDietaryRestrictionsOutput(is_suitable=False, message=f\"Recipe '{recipe_name}' not found.\")\n",
        "        return output.message\n",
        "\n",
        "\n",
        "def get_ingredient_list(recipe_name: str, tool_context: ToolContext) -> str:\n",
        "    \"\"\"Returns a list of ingredients for a given recipe.\"\"\"\n",
        "    print(f\"Tool Context in get_ingredient_list: {tool_context}\")\n",
        "\n",
        "    recipe_data = recipes.get(recipe_name.lower())\n",
        "    if recipe_data:\n",
        "        output = GetIngredientListOutput(ingredients=recipe_data['ingredients'])\n",
        "        return f\"Ingredients for '{recipe_name}': {', '.join(output.ingredients)}.\"\n",
        "    else:\n",
        "        output = GetIngredientListOutput(message=f\"Recipe '{recipe_name}' not found.\")\n",
        "        return output.message\n",
        "\n",
        "\n",
        "# Wrap functions with FunctionTool (using the tool's name, description and parameters)\n",
        "search_recipes_tool = FunctionTool(func=search_recipes)\n",
        "check_dietary_restrictions_tool = FunctionTool(func=check_dietary_restrictions)\n",
        "get_ingredient_list_tool = FunctionTool(func=get_ingredient_list)\n",
        "\n",
        "\n",
        "# --- Agent ---\n",
        "\n",
        "## LLMAgent\n",
        "\n",
        "# `LLMAgent` is a specialized agent class designed for interacting with Large Language Models (LLMs).  It handles the complexities of:\n",
        "# - Sending prompts to the LLM.\n",
        "# - Receiving responses from the LLM.\n",
        "# - Detecting and handling function calls (tool use) requested by the LLM.\n",
        "# - Managing the conversation state.\n",
        "\n",
        "recipe_agent = LlmAgent(  # Use LlmAgent\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Recipe Agent. Your task is to help users find recipes and check their suitability for dietary restrictions.\n",
        "\n",
        "    You have access to three tools:\n",
        "    1. `search_recipes`: Use this tool to find recipes based on a keyword (e.g., ingredient).\n",
        "    2. `check_dietary_restrictions`: Use this tool to check if a recipe is suitable for a given dietary restriction.\n",
        "    3. `get_ingredient_list`: Use this tool to get a list of ingredients for a given recipe.\n",
        "\n",
        "    When a user provides a prompt, first determine what they are asking for.\n",
        "    - If they are asking for recipes based on a keyword, use the `search_recipes` tool.\n",
        "    - If they are asking if a recipe is suitable for a dietary restriction, use the `check_dietary_restrictions` tool.\n",
        "    - If they are asking for a list of ingredients for a recipe, use the `get_ingredient_list` tool.\n",
        "    Finally, present the information to the user in a clear and concise manner.\n",
        "    \"\"\",\n",
        "    description=\"\"\"An agent that can find recipes, check dietary restrictions, and list ingredients.\n",
        "    It has access to the `search_recipes`, `check_dietary_restrictions`, and `get_ingredient_list` tools.\"\"\",\n",
        "    tools=[search_recipes_tool, check_dietary_restrictions_tool, get_ingredient_list_tool],\n",
        "    input_schema=RecipeAgentInput,  # Agent-level input schema\n",
        "    output_schema=RecipeAgentOutput,  # Agent-level output schema\n",
        "    allow_transfer=False\n",
        ")\n",
        "\n",
        "# --- Session and Runner ---\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=recipe_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# --- Agent Interaction ---\n",
        "\n",
        "## Input/Output Schemas (Agent Level)\n",
        "\n",
        "# Defining input and output schemas at the *agent* level (like `RecipeAgentInput` and `RecipeAgentOutput`) provides a high-level contract for how users interact with the agent.  This:\n",
        "# - Improves the agent's predictability.\n",
        "# - Makes it easier to integrate the agent into larger systems.\n",
        "# - Provides a clear interface for the agent.\n",
        "\n",
        "## Event Types\n",
        "\n",
        "# The `Runner` generates a stream of `Event` objects that represent different stages of the agent's processing. Understanding these events is key to debugging and controlling the agent's behavior.  The main types are:\n",
        "# - **Tool Calls:** The LLM requests the use of a specific tool.\n",
        "# - **Tool Results:** The output produced by a tool.\n",
        "# - **Agent Responses:** Textual responses generated by the LLM.\n",
        "# - **Final Response:** The agent's complete, final response.\n",
        "\n",
        "def call_agent(query):\n",
        "    content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "    for event in events:\n",
        "        if event.get_function_calls():  # Check for function calls using the method\n",
        "            print(f\"  LLM Requested Tool Call: {event.get_function_calls()[0].name}\")\n",
        "        elif event.get_function_responses():  # Check for function responses\n",
        "            print(f\"    Tool Result: {event.content.parts[0].text}\")\n",
        "        elif event.content and event.content.role == 'model': # Check for model response\n",
        "            print(f\"  Agent Response: {event.content.parts[0].text}\")\n",
        "        elif event.is_final_response():\n",
        "            print(f\"Agent Final Response: {event.content.parts[0].text}\")\n",
        "        else:\n",
        "            print(f\"  Other Event: {event}\")  # For debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-Qa3RjIzG05",
        "outputId": "e9fb4806-9c78-4a18-d722-06006a81b1df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tool Context in search_recipes: {'_invocation_context': InvocationContext(artifact_service=None, session_service=<google.adk.sessions.in_memory_session_service.InMemorySessionService object at 0x789c05187650>, memory_service=None, invocation_id='e-9b8bcd03-cb93-4ea5-a185-c4df1199c30c', branch=None, agent=LlmAgent(name='recipe_agent', description='An agent that can find recipes, check dietary restrictions, and list ingredients.\\n    It has access to the `search_recipes`, `check_dietary_restrictions`, and `get_ingredient_list` tools.', parent_agent=None, children=[], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash-001', instruction='You are a Recipe Agent. Your task is to help users find recipes and check their suitability for dietary restrictions.\\n\\n    You have access to three tools:\\n    1. `search_recipes`: Use this tool to find recipes based on a keyword (e.g., ingredient).\\n    2. `check_dietary_restrictions`: Use this tool to check if a recipe is suitable for a given dietary restriction.\\n    3. `get_ingredient_list`: Use this tool to get a list of ingredients for a given recipe.\\n\\n    When a user provides a prompt, first determine what they are asking for.\\n    - If they are asking for recipes based on a keyword, use the `search_recipes` tool.\\n    - If they are asking if a recipe is suitable for a dietary restriction, use the `check_dietary_restrictions` tool.\\n    - If they are asking for a list of ingredients for a recipe, use the `get_ingredient_list` tool.\\n    Finally, present the information to the user in a clear and concise manner.\\n    ', global_instruction='', tools=[<google.adk.tools.function_tool.FunctionTool object at 0x789c05187490>, <google.adk.tools.function_tool.FunctionTool object at 0x789c05187510>, <google.adk.tools.function_tool.FunctionTool object at 0x789c051874d0>], generate_content_config=None, allow_transfer=False, disallow_transfer_to_sibling=False, include_contents='default', input_schema=<class '__main__.RecipeAgentInput'>, output_schema=<class '__main__.RecipeAgentOutput'>, output_key=None, planner=None, code_executor=None, examples=None, before_model_callback=None, after_model_callback=None, before_tool_callback=None, after_tool_callback=None), user_content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Find recipes with chicken.')], role='user'), session=Session(id='123344', app_name='recipe_app', user_id='12345', state={}, events=[Event(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Find recipes with chicken.')], role='user'), grounding_metadata=None, partial=None, turn_complete=None, error_code=None, error_message=None, interrupted=None, invocation_id='e-9b8bcd03-cb93-4ea5-a185-c4df1199c30c', author='user', actions=EventActions(skip_summarization=None, state_delta={}, artifact_delta={}, transfer_to_agent=None, escalate=None, requested_auth_configs={}), long_running_tool_ids=None, branch=None, id='lMJADWI3', timestamp=1742883308.148655), Event(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id='af-cc9955c6-352f-44d0-9e36-37623ab8b1ce', args={'keyword': 'chicken'}, name='search_recipes'), function_response=None, inline_data=None, text=None)], role='model'), grounding_metadata=None, partial=None, turn_complete=None, error_code=None, error_message=None, interrupted=None, invocation_id='e-9b8bcd03-cb93-4ea5-a185-c4df1199c30c', author='recipe_agent', actions=EventActions(skip_summarization=None, state_delta={}, artifact_delta={}, transfer_to_agent=None, escalate=None, requested_auth_configs={}), long_running_tool_ids=set(), branch=None, id='US5tQH36', timestamp=1742883308.149755)], last_update_time=1742883308.149755), streaming=<StreamingMode.NONE: None>, end_invocation=False, live_request_queue=None, active_streaming_tools=None, response_modalities=None, support_cfc=False, transcription_cache=None), '_event_actions': EventActions(skip_summarization=None, state_delta={}, artifact_delta={}, transfer_to_agent=None, escalate=None, requested_auth_configs={}), '_state': <google.adk.sessions.state.State object at 0x789c05664590>, 'function_call_id': 'af-cc9955c6-352f-44d0-9e36-37623ab8b1ce', 'auth_response': None}\n",
            "  LLM Requested Tool Call: search_recipes\n",
            "    Tool Result: None\n",
            "  Agent Response: OK. I found one recipe with chicken: chicken tikka masala.\n"
          ]
        }
      ],
      "source": [
        "call_agent(\"Find recipes with chicken.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk8SzHQY03gg",
        "outputId": "17237287-4b7a-4e09-cbed-9a25c659e299"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tool Context in get_ingredient_list: <agents.tools.tool_context.ToolContext object at 0x79671a5be9d0>  LLM Requested Tool Call: get_ingredient_list\n",
            "\n",
            "    Tool Result: None\n",
            "  Agent Response: The ingredients for chicken tikka masala are: chicken, yogurt, ginger, garlic, masala blend.\n"
          ]
        }
      ],
      "source": [
        "call_agent(\"can you give me recipe for it?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLK3PIN8zGxx",
        "outputId": "3f5dbe28-ec7e-4f5e-98ca-0747243fb43a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tool Context in check_dietary_restrictions: <agents.tools.tool_context.ToolContext object at 0x796719185950>  LLM Requested Tool Call: check_dietary_restrictions\n",
            "\n",
            "    Tool Result: None\n",
            "  Agent Response: No, pasta carbonara is not suitable for a vegan diet.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "call_agent(\"Is pasta carbonara suitable for a vegan diet?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHamR-wWzNSP",
        "outputId": "ac23252b-b69d-4a6b-f2e3-b80b3131d935"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tool Context in get_ingredient_list: <agents.tools.tool_context.ToolContext object at 0x796719185950>  LLM Requested Tool Call: get_ingredient_list\n",
            "\n",
            "    Tool Result: None\n",
            "  Agent Response: The ingredients for vegan lentil soup are: lentils, carrots, celery, onion, vegetable broth.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "call_agent(\"What are the ingredients in vegan lentil soup?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz35ZWrc7Koo"
      },
      "source": [
        "**Normal Python Function:**\n",
        "\n",
        "*   **Direct Execution:** A standard Python function is executed directly when called. You pass arguments, it performs computations, and it returns a value.\n",
        "*   **No LLM Interaction:** It doesn't inherently interact with Large Language Models (LLMs). It's just plain Python code.\n",
        "*   **No Automatic Input/Output Handling:** You are responsible for handling input validation, data type conversions, and formatting the output.\n",
        "\n",
        "**Function Wrapped with `FunctionTool`:**\n",
        "\n",
        "*   **Indirect Execution via LLM:**  A `FunctionTool` *doesn't* execute the function directly. Instead, it provides a *description* of the function (its name, purpose, parameters, and expected output) to an LLM. The LLM decides *when* and *if* to use the function based on the user's request and the context of the conversation.\n",
        "*   **LLM-Driven Invocation:** The LLM generates a *function call* (a JSON object) specifying the function to use and the arguments to pass. The `FunctionTool` then:\n",
        "    *   **Validates Input:** Checks if the arguments provided by the LLM match the function's expected input schema (if defined using pydantic).\n",
        "    *   **Converts Input:** Transforms the LLM's JSON arguments into the correct Python data types expected by the function.\n",
        "    *   **Executes the Function:** Calls the underlying Python function with the converted arguments.\n",
        "    *   **Validates Output:** (Optional) Checks if the function's return value matches the expected output schema.\n",
        "    *   **Converts Output:** Transforms the Python return value into a JSON-compatible format that the LLM can understand.\n",
        "    *   **Returns to LLM:** Sends the function's result (as JSON) back to the LLM.\n",
        "*   **Automatic Declaration:** `FunctionTool` automatically generates the *function declaration* that's sent to the LLM. This declaration is crucial; it's how the LLM knows about the tool's capabilities.  This declaration is derived from the function's signature (name, parameters) and docstring.\n",
        "* **ToolContext Injection:** It automatically provides a `ToolContext` object to the wrapped function, giving the function access to session information, artifacts, and other contextual data.\n",
        "\n",
        "**When to Use Which (and Why):**\n",
        "\n",
        "*   **Use a Normal Python Function:**\n",
        "    *   **Internal Logic:** When the function is part of your agent's internal logic *and is not intended to be directly accessible to the LLM*.  For example, helper functions, data processing routines within a tool, etc.\n",
        "    *   **Direct Control:** When you need precise, deterministic control over when and how the function is executed. You call it directly from your code.\n",
        "\n",
        "*   **Use `FunctionTool`:**\n",
        "    *   **LLM-Accessible Tools:** When you want to expose a function's capabilities *to the LLM*. This allows the LLM to decide when to use the function to fulfill the user's request.  This is the core concept of tool use with LLMs.\n",
        "    *   **Dynamic Behavior:** When you want the agent's behavior to be more dynamic and adaptable. The LLM can choose the appropriate tool based on the situation, rather than you hardcoding the tool usage.\n",
        "    *   **Input/Output Handling:** When you want to leverage automatic input validation, type conversion, and output formatting. This simplifies your code and makes it more robust.  Using `pydantic` schemas with `FunctionTool` is highly recommended for this.\n",
        "    * **Contextual Information:** When your tool needs access to session-specific data (like user ID, session ID, or artifacts) via the `ToolContext`.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "Think of `FunctionTool` as a bridge between your Python code and the LLM.  It allows the LLM to \"see\" and \"use\" your functions as tools.  Normal Python functions are for internal logic; `FunctionTool`-wrapped functions are for exposing capabilities to the LLM.  The developer should use `FunctionTool` whenever they want the LLM to be able to choose to execute a function as part of its response strategy. They should use a normal Python function when they *don't* want the LLM to have direct access to call that function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc6ZWVU0MfKf"
      },
      "source": [
        "### LLMAgent with FunctionTool, ToolContext and Input/Output Schema - Example 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDUaCw_JUvEn",
        "outputId": "66adca48-55d9-4d07-9986-df4c688bbdfb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  LLM Requested Tool Call: search_recipes\n",
            "    Tool Result: None\n",
            "  Agent Response: OK. I found one recipe that includes pasta: pasta carbonara.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.tools.function_tool import FunctionTool\n",
        "from google.adk.tools.tool_context import ToolContext\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "\n",
        "# --- Input/Output Schemas ---\n",
        "\n",
        "class SearchRecipesInput(BaseModel):\n",
        "    keyword: str = Field(description=\"The keyword to search for (e.g., ingredient).\")\n",
        "\n",
        "class SearchRecipesOutput(BaseModel):\n",
        "    recipes: List[str] = Field(description=\"List of recipe names matching the keyword.\")\n",
        "    message: Optional[str] = Field(default=None, description=\"Informative message if no recipes are found.\")\n",
        "\n",
        "class RecipeAgentInput(BaseModel):\n",
        "    query: str = Field(description=\"The user's query to the recipe agent.\")\n",
        "\n",
        "class RecipeAgentOutput(BaseModel):\n",
        "    response: str = Field(description=\"The agent's response to the user's query.\")\n",
        "\n",
        "# --- Tools ---\n",
        "\n",
        "def search_recipes(keyword: str, tool_context: ToolContext) -> str:\n",
        "    \"\"\"Searches for recipes based on a keyword.\"\"\"\n",
        "    recipes = {\n",
        "        \"pasta carbonara\": {\n",
        "            \"ingredients\": [\"pasta\", \"eggs\", \"guanciale\", \"pecorino romano\", \"black pepper\"],\n",
        "        },\n",
        "        \"chicken tikka masala\": {\n",
        "            \"ingredients\": [\"chicken\", \"yogurt\", \"ginger\", \"garlic\", \"masala blend\"],\n",
        "        },\n",
        "        \"vegan lentil soup\": {\n",
        "            \"ingredients\": [\"lentils\", \"carrots\", \"celery\", \"onion\", \"vegetable broth\"],\n",
        "        },\n",
        "    }\n",
        "    matching_recipes = [\n",
        "        recipe_name\n",
        "        for recipe_name, recipe_data in recipes.items()\n",
        "        if keyword.lower() in recipe_name.lower() or keyword.lower() in recipe_data[\"ingredients\"]\n",
        "    ]\n",
        "    if matching_recipes:\n",
        "        output = SearchRecipesOutput(recipes=matching_recipes)\n",
        "        return f\"Recipes matching '{keyword}': {', '.join(output.recipes)}.\"\n",
        "    else:\n",
        "        output = SearchRecipesOutput(message=f\"No recipes found matching '{keyword}'.\")\n",
        "        return output.message\n",
        "\n",
        "# Wrap functions with FunctionTool\n",
        "search_recipes_tool = FunctionTool(func=search_recipes)\n",
        "\n",
        "# --- Agent ---\n",
        "recipe_agent = LlmAgent(\n",
        "    model=\"gemini-2.0-flash-001\",\n",
        "    name=\"recipe_agent\",\n",
        "    instruction=\"\"\"You are a Recipe Agent. Your task is to help users find recipes.\n",
        "    You have access to one tool:\n",
        "    1. `search_recipes`: Use this tool to find recipes based on a keyword (e.g., ingredient).\n",
        "    When a user provides a prompt, use the `search_recipes` tool to find recipes based on the keyword in the prompt.\n",
        "    \"\"\",\n",
        "    description=\"\"\"An agent that can find recipes using the `search_recipes` tool.\"\"\",\n",
        "    allow_transfer=False,\n",
        "    tools=[search_recipes_tool],\n",
        "    input_schema=RecipeAgentInput,\n",
        "    output_schema=RecipeAgentOutput,\n",
        ")\n",
        "\n",
        "# --- Session and Runner ---\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=\"recipe_app\", user_id=\"12345\", session_id=\"123344\")\n",
        "runner = Runner(agent=recipe_agent, app_name=\"recipe_app\", session_service=session_service)\n",
        "\n",
        "# --- Agent Interaction ---\n",
        "\n",
        "def call_agent(query):\n",
        "    content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "    events = runner.run(user_id=\"12345\", session_id=\"123344\", new_message=content)\n",
        "\n",
        "    for event in events:\n",
        "        if event.get_function_calls():\n",
        "            print(f\"  LLM Requested Tool Call: {event.get_function_calls()[0].name}\")\n",
        "        elif event.get_function_responses():\n",
        "            print(f\"    Tool Result: {event.content.parts[0].text}\")\n",
        "        elif event.content and event.content.role == 'model':\n",
        "            print(f\"  Agent Response: {event.content.parts[0].text}\")\n",
        "        elif event.is_final_response():\n",
        "            print(f\"Agent Final Response: {event.content.parts[0].text}\")\n",
        "        else:\n",
        "            print(f\"  Other Event: {event}\")\n",
        "\n",
        "# Example usage\n",
        "call_agent(\"What recipes include pasta?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPo0-P_AwXm6"
      },
      "source": [
        "###  Quickstart session, states and SessionService"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp3LOwJN3VVH",
        "outputId": "35f7180d-5965-466d-e940-bf30da2e32f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Session State: {}\n",
            "\n",
            "Agent Response:  Paris\n",
            "\n",
            "Final Session State: {'capital_city': 'Paris\\n'}\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types\n",
        "\n",
        "# --- Constants ---\n",
        "APP_NAME = \"capital_finder_app\"\n",
        "USER_ID = \"quickstart_user\"\n",
        "SESSION_ID = \"session_abc\"\n",
        "MODEL = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# Agent\n",
        "capital_agent = LlmAgent(\n",
        "    model=MODEL,\n",
        "    name=\"CapitalFinderAgent\",\n",
        "    instruction=\"\"\"You are an agent that finds the capital of a given country.\n",
        "    When asked for the capital, respond *only* with the name of the capital city.\n",
        "    \"\"\",\n",
        "    output_key=\"capital_city\" # Save the agent's final response text to state['capital_city']\n",
        ")\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=capital_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "  for event in events:\n",
        "      if event.is_final_response():\n",
        "          final_response = event.content.parts[0].text\n",
        "          print(\"\\nAgent Response: \", final_response)\n",
        "\n",
        "initial_session = session_service.get_session(APP_NAME, USER_ID, SESSION_ID)\n",
        "print(f\"Initial Session State: {initial_session.state}\") # Should be empty {}\n",
        "\n",
        "call_agent(\"What is the capital of france?\")\n",
        "\n",
        "final_session = session_service.get_session(APP_NAME, USER_ID, SESSION_ID)\n",
        "print(f\"Final Session State: {final_session.state}\") # Should now contain {'capital_city': 'Paris'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl5zb-k4MTak"
      },
      "source": [
        "### Session State - State Manupilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtLbWd9-cqih"
      },
      "outputs": [],
      "source": [
        "from google.adk.agents import LlmAgent\n",
        "from google.genai import types\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.events import Event, EventActions\n",
        "\n",
        "# --- Constants ---\n",
        "APP_NAME = \"task_manager_app\"\n",
        "USER_ID = \"test_user\"\n",
        "AGENT_NAME = \"task_manager_agent\"\n",
        "MODEL_NAME = \"gemini-2.0-flash-001\"  # Or any suitable model\n",
        "\n",
        "# --- Agent Definition ---\n",
        "#  Simplified instruction, as we're handling logic directly\n",
        "task_agent = LlmAgent(\n",
        "    model=MODEL_NAME,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"\"\"You are a Task Management Agent. Respond to user requests to manage tasks.\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def add_task(session, task_description):\n",
        "    \"\"\"Adds a task to the task list.\"\"\"\n",
        "    tasks = session.state.get(\"user:tasks\", [])  # Get current tasks (or empty list)\n",
        "    new_task_id = len(tasks) + 1\n",
        "    new_task = {\"id\": new_task_id, \"description\": task_description, \"status\": \"pending\"}\n",
        "    tasks.append(new_task)\n",
        "    # Use EventActions to update the state (delta update)\n",
        "    add_event = Event(author=\"agent\", actions=EventActions(state_delta={\"user:tasks\": tasks}))\n",
        "    session_service.append_event(session, add_event)\n",
        "    return f\"Task '{task_description}' added with ID {new_task_id}.\"\n",
        "\n",
        "def modify_task(session, task_id, new_status):\n",
        "    \"\"\"Modifies the status of a task.\"\"\"\n",
        "    tasks = session.state.get(\"user:tasks\", [])\n",
        "    try:\n",
        "        task_id = int(task_id)  # Ensure task_id is an integer\n",
        "    except ValueError:\n",
        "        return \"Invalid task ID. Please provide a number.\"\n",
        "\n",
        "    for i, task in enumerate(tasks):\n",
        "        if task[\"id\"] == task_id:\n",
        "            tasks[i][\"status\"] = new_status\n",
        "            # Update state via EventActions\n",
        "            modify_event = Event(author='agent', actions=EventActions(state_delta={\"user:tasks\": tasks}))\n",
        "            session_service.append_event(session, modify_event)\n",
        "            return f\"Task {task_id} status updated to '{new_status}'.\"\n",
        "    return f\"Task with ID {task_id} not found.\"\n",
        "\n",
        "\n",
        "def delete_task(session, task_id):\n",
        "    \"\"\"Deletes a task from the task list.\"\"\"\n",
        "    tasks = session.state.get(\"user:tasks\", [])\n",
        "    try:\n",
        "        task_id = int(task_id)\n",
        "    except ValueError:\n",
        "        return \"Invalid task ID.  Please provide a number.\"\n",
        "\n",
        "    updated_tasks = [task for task in tasks if task[\"id\"] != task_id]\n",
        "    if len(updated_tasks) < len(tasks):\n",
        "        # Update state via EventActions\n",
        "        delete_event = Event(author='agent', actions=EventActions(state_delta={\"user:tasks\": updated_tasks}))\n",
        "        session_service.append_event(session, delete_event)\n",
        "        return f\"Task {task_id} deleted.\"\n",
        "    return f\"Task with ID {task_id} not found.\"\n",
        "\n",
        "def list_tasks(session):\n",
        "    \"\"\"Lists all tasks for the user.\"\"\"\n",
        "    tasks = session.state.get(\"user:tasks\", [])\n",
        "    if not tasks:\n",
        "        return \"You have no tasks.\"\n",
        "    task_list_str = \"\\n\".join(\n",
        "        f\"{task['id']}: {task['description']} ({task['status']})\" for task in tasks\n",
        "    )\n",
        "    return f\"Your tasks:\\n{task_list_str}\"\n",
        "\n",
        "\n",
        "def call_agent(user_input, session):\n",
        "    \"\"\"Sends user input to the agent and processes events.\"\"\"\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=user_input)])\n",
        "    events = runner.run(user_id=USER_ID, session_id=session.id, new_message=content) #session.id, not SESSION_ID\n",
        "\n",
        "    final_response_text = \"\"\n",
        "    for event in events:\n",
        "      if event.content and event.content.role == 'model':\n",
        "          final_response_text = event.content.parts[0].text\n",
        "          break  # Exit loop after getting the final response.\n",
        "\n",
        "    return final_response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgLYUT1Ycqb_",
        "outputId": "0ac9b658-9f92-48d5-86ba-4c9b1b0906b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created session with ID: b760e622-b598-4529-8d5e-77e72fd19c43\n"
          ]
        }
      ],
      "source": [
        "# 1. Create a Session (with initial state, if any)\n",
        "#  demonstrates: create_session, InMemorySessionService, initial state\n",
        "USER_ID = \"test_user2\"\n",
        "session_service = InMemorySessionService()\n",
        "\n",
        "session = session_service.create_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID\n",
        ")  #  Let the service generate the ID\n",
        "\n",
        "print(f\"Created session with ID: {session.id}\")\n",
        "runner = Runner(agent=task_agent, app_name=APP_NAME, session_service=session_service)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k5wliFueuF_U",
        "outputId": "558911ac-d631-41d0-ff5f-f63e06d39f32"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'e834de5e-cbf1-42cb-92a4-a592f6b6bc5e'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "session.id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G8I0MtT4Gfy",
        "outputId": "b2dba833-1a26-423a-cbf7-27ef7cecfbba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Retrieved session state:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieved_session = session_service.get_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=session.id\n",
        ")\n",
        "print(f\"\\nRetrieved session state:\")\n",
        "retrieved_session.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "tGg1H36a2k-8",
        "outputId": "8e47f9f9-2882-4e0e-ee5b-b3e625a9f25e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Adding tasks directly to state via Function...\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Task 'Buy groceries' added with ID 4.\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"\\nAdding tasks directly to state via Function...\")\n",
        "\n",
        "add_task(session, \"Buy milk\")\n",
        "add_task(session, \"Walk the dog\")\n",
        "add_task(session, \"Prepare presentation\")\n",
        "add_task(session, \"Buy groceries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSK4sovb1OWx",
        "outputId": "3ef0996b-e931-4758-e456-1d0afdf2ee37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Retrieved session state:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'user:tasks': [{'id': 1, 'description': 'Buy milk', 'status': 'pending'},\n",
              "  {'id': 2, 'description': 'Walk the dog', 'status': 'pending'},\n",
              "  {'id': 3, 'description': 'Prepare presentation', 'status': 'pending'},\n",
              "  {'id': 4, 'description': 'Buy groceries', 'status': 'pending'}]}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3. Retrieve and display the session (demonstrates get_session)\n",
        "retrieved_session = session_service.get_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=session.id\n",
        ")\n",
        "print(f\"\\nRetrieved session state:\")\n",
        "retrieved_session.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUOEucZl1VdT",
        "outputId": "24dd38a5-7f5a-4dcc-c238-917d19f45a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Listing tasks...\n",
            "Your tasks:\n",
            "1: Buy milk (pending)\n",
            "2: Walk the dog (pending)\n",
            "3: Prepare presentation (pending)\n",
            "4: Buy groceries (pending)\n"
          ]
        }
      ],
      "source": [
        "# 4. List tasks (demonstrates accessing state)\n",
        "print(\"\\nListing tasks...\")\n",
        "print(list_tasks(retrieved_session))  # Using a helper, accessing state directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3EWU1Tb14Vn",
        "outputId": "f945c5dd-c3c7-4b64-9752-136ff6a74b3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Modifying task 2...\n",
            "Task 2 status updated to 'completed'.\n",
            "Modified session state:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'user:tasks': [{'id': 1, 'description': 'Buy milk', 'status': 'pending'},\n",
              "  {'id': 2, 'description': 'Walk the dog', 'status': 'completed'},\n",
              "  {'id': 3, 'description': 'Prepare presentation', 'status': 'pending'},\n",
              "  {'id': 4, 'description': 'Buy groceries', 'status': 'pending'}]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 5. Modify a task (demonstrates state modification)\n",
        "print(\"\\nModifying task 2...\")\n",
        "print(modify_task(retrieved_session, \"2\", \"completed\"))\n",
        "retrieved_session = session_service.get_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=session.id\n",
        ")\n",
        "print(f\"Modified session state:\")\n",
        "retrieved_session.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yHQmUMB16c4",
        "outputId": "ab0f2fd1-1d4a-4831-84b6-cdc1fac331cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Deleting task 1...\n",
            "Task 1 deleted.\n"
          ]
        }
      ],
      "source": [
        "# 6. Delete a task (demonstrates state deletion)\n",
        "print(\"\\nDeleting task 1...\")\n",
        "print(delete_task(retrieved_session, \"1\"))\n",
        "retrieved_session = session_service.get_session(\n",
        "    app_name=APP_NAME, user_id=USER_ID, session_id=session.id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFHKk3zA18nz",
        "outputId": "3d287272-a29f-4b79-a465-3661acf1845f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session state after deletion:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'user:tasks': [{'id': 2,\n",
              "   'description': 'Walk the dog',\n",
              "   'status': 'completed'},\n",
              "  {'id': 3, 'description': 'Prepare presentation', 'status': 'pending'},\n",
              "  {'id': 4, 'description': 'Buy groceries', 'status': 'pending'}]}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Session state after deletion:\")\n",
        "retrieved_session.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTsLpN4a2Ddr",
        "outputId": "481bcb3b-d3cd-4580-c90d-2b15d8998f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sessions for user test_user2:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ListSessionsResponse(session_ids=['b760e622-b598-4529-8d5e-77e72fd19c43'])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 7. List sessions for the user (demonstrates list_sessions)\n",
        "# Demonstrates:  list_sessions\n",
        "print(f\"\\nSessions for user {USER_ID}:\")\n",
        "session_service.list_sessions(APP_NAME, USER_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjZI2ZgK2GDc",
        "outputId": "6a2ded3d-6d11-49eb-8ad5-39783b63c9fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Deleted session: e834de5e-cbf1-42cb-92a4-a592f6b6bc5e\n"
          ]
        }
      ],
      "source": [
        "# 8. Delete a session (demonstrates delete_session).\n",
        "session_service.delete_session(app_name=APP_NAME, user_id=USER_ID, session_id=session.id)\n",
        "print(f\"\\nDeleted session: {session.id}\")\n",
        "retrieved_session = session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=session.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p4fVhCh2Iho",
        "outputId": "e6d7a072-80e2-422b-f951-6ec993a716bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session deleted successfully, as could not be retrieved.\n"
          ]
        }
      ],
      "source": [
        "if retrieved_session: # Should be None.\n",
        "    print(retrieved_session)\n",
        "else:\n",
        "    print(\"Session deleted successfully, as could not be retrieved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlK7fRkrHfeo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmsaW8mdMKTD"
      },
      "source": [
        "### Session State Example 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICK_GXIfHfbi",
        "outputId": "ce7392f8-8205-44bb-d533-74f0e0aa542b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'order_status': 'shipped', 'items': ['shirt', 'pants'], 'tracking_number': 'XYZ123'}\n",
            "{'order_status': 'shipped', 'items': ['shirt', 'pants']}\n",
            "{'order_status': 'shipped', 'items': ['shirt', 'pants'], 'app:max_retries': 3, 'user:preferred_contact': 'email', 'temp:request_id': 'temp_val'}\n",
            "{'order_status': 'pending', 'items': ['shirt']}\n",
            "{}\n",
            "{'task_manager_app': {'test_user': {'tasks': [{'id': 1, 'description': 'Buy milk', 'status': 'pending'}]}, 'test_user2': {'tasks': [{'id': 2, 'description': 'Walk the dog', 'status': 'completed'}, {'id': 3, 'description': 'Prepare presentation', 'status': 'pending'}, {'id': 4, 'description': 'Buy groceries', 'status': 'pending'}]}}}\n"
          ]
        }
      ],
      "source": [
        "from google.adk.sessions import InMemorySessionService\n",
        "\n",
        "# Create the session service\n",
        "session_service = InMemorySessionService()\n",
        "\n",
        "# Create a session (with initial state)\n",
        "session = session_service.create_session(\n",
        "    app_name=\"my_app\",\n",
        "    user_id=\"user1\",\n",
        "    state={\"order_status\": \"pending\", \"items\": [\"shirt\"]},\n",
        ")\n",
        "\n",
        "# --- Direct State Manipulation (Generally NOT Recommended) ---\n",
        "\n",
        "# 1. Retrieve the session (to get the current state)\n",
        "retrieved_session = session_service.get_session(\n",
        "    app_name=\"my_app\", user_id=\"user1\", session_id=session.id\n",
        ")\n",
        "\n",
        "# 2. Access the state dictionary directly\n",
        "current_state = retrieved_session.state\n",
        "\n",
        "# 3. Modify the state directly\n",
        "current_state[\"order_status\"] = \"shipped\"  # Update existing key\n",
        "current_state[\"tracking_number\"] = \"XYZ123\"  # Add new key\n",
        "current_state[\"items\"] = [\"shirt\", \"pants\"]   #Modify existing key.\n",
        "\n",
        "# 4.  The changes are reflected *immediately* in the retrieved session.\n",
        "print(retrieved_session.state)\n",
        "# Output will show:\n",
        "# {'order_status': 'shipped', 'items': ['shirt', 'pants'], 'tracking_number': 'XYZ123'}\n",
        "\n",
        "# --- Deleting a Key (Directly) ---\n",
        "\n",
        "# 1. Access the state dictionary\n",
        "current_state = retrieved_session.state\n",
        "\n",
        "# 2. Delete the key using 'del' or set to None.\n",
        "del current_state[\"tracking_number\"]  # Or: current_state[\"tracking_number\"] = None\n",
        "# del current_state[\"order_status\"] # Example showing how to delete completely.\n",
        "\n",
        "\n",
        "# 3. Verify the deletion\n",
        "print(retrieved_session.state)  # 'tracking_number' is gone\n",
        "# Output\n",
        "# {'order_status': 'shipped', 'items': ['shirt', 'pants']}\n",
        "\n",
        "\n",
        "# --- Demonstrating State Prefixes (Directly) ---\n",
        "\n",
        "current_state[\"app:max_retries\"] = 3  # Adding an app-level setting\n",
        "current_state[\"user:preferred_contact\"] = \"email\"  # User-specific setting\n",
        "current_state[\"temp:request_id\"] = \"temp_val\"       # Temporary value\n",
        "\n",
        "print(retrieved_session.state)\n",
        "\n",
        "retrieved_session2 = session_service.get_session(\n",
        "    app_name=\"my_app\", user_id=\"user1\", session_id=session.id\n",
        ")\n",
        "\n",
        "print(retrieved_session2.state)  # 'temp:request_id' will *not* be present\n",
        "# Demonstrating how to access:\n",
        "print(session_service.app_state)\n",
        "print(session_service.user_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "648gwMHOMNWx"
      },
      "source": [
        "### Session State - delta_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbf_eXbYHfYp",
        "outputId": "c5a07f45-7747-4a69-d868-ffd6adde2af0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'order_status': 'shipped', 'items': ['shirt', 'pants'], 'tracking_number': 'XYZ456'}\n",
            "\n",
            "State with prefixes added:\n",
            "{'order_status': 'shipped', 'items': ['shirt', 'pants'], 'tracking_number': 'XYZ456', 'temp:request_id': 'temp_value'}\n",
            "\n",
            "State after retrieval (temp should be gone):\n",
            "{'order_status': 'pending', 'items': ['shirt'], 'notes': 'Initial order', 'app:max_retries': 3, 'user:pref_contact': 'email'}\n",
            "\n",
            "App State:\n",
            "{'my_app': {'max_retries': 3}}\n",
            "\n",
            "User State:\n",
            "{'task_manager_app': {'test_user': {'tasks': [{'id': 1, 'description': 'Buy milk', 'status': 'pending'}]}, 'test_user2': {'tasks': [{'id': 2, 'description': 'Walk the dog', 'status': 'completed'}, {'id': 3, 'description': 'Prepare presentation', 'status': 'pending'}, {'id': 4, 'description': 'Buy groceries', 'status': 'pending'}]}}, 'my_app': {'user1': {'pref_contact': 'email'}}}\n"
          ]
        }
      ],
      "source": [
        "from google.adk.sessions import InMemorySessionService\n",
        "\n",
        "\n",
        "# Create the session service\n",
        "session_service = InMemorySessionService()\n",
        "\n",
        "# Create a session (with initial state)\n",
        "session = session_service.create_session(\n",
        "    app_name=\"my_app\",\n",
        "    user_id=\"user1\",\n",
        "    state={\"order_status\": \"pending\", \"items\": [\"shirt\"], \"notes\": \"Initial order\"},\n",
        ")\n",
        "\n",
        "# --- Direct State Manipulation (Generally NOT Recommended) ---\n",
        "\n",
        "# 1. Retrieve the session (to get the current state)\n",
        "retrieved_session = session_service.get_session(\n",
        "    app_name=\"my_app\", user_id=\"user1\", session_id=session.id\n",
        ")\n",
        "\n",
        "# 2. Access the state dictionary directly\n",
        "current_state = retrieved_session.state\n",
        "\n",
        "# --- Simulating a Delta Update (Directly) ---\n",
        "\n",
        "# Directly modify the state dictionary as if applying a state_delta:\n",
        "current_state[\"order_status\"] = \"shipped\"  # Update existing key\n",
        "current_state[\"tracking_number\"] = \"XYZ456\"  # Add a new key\n",
        "current_state[\"items\"].append(\"pants\")      # Modify a list (append)\n",
        "del current_state[\"notes\"]                   # Delete a key\n",
        "\n",
        "# The changes are reflected *immediately* in the retrieved_session (with InMemorySessionService)\n",
        "print(retrieved_session.state)\n",
        "\n",
        "# --- Demonstrating State Prefixes (Directly) ---\n",
        "# We need to use setdefault to properly initialize nested dictionaries.\n",
        "session_service.app_state.setdefault(\"my_app\", {})[\"max_retries\"] = 3  # app-level\n",
        "session_service.user_state.setdefault(\"my_app\", {}).setdefault(\"user1\", {})[\"pref_contact\"] = \"email\"\n",
        "current_state[\"temp:request_id\"] = \"temp_value\"\n",
        "\n",
        "print(\"\\nState with prefixes added:\")\n",
        "print(retrieved_session.state) #temp wont be there\n",
        "\n",
        "retrieved_session = session_service.get_session(\n",
        "    app_name=\"my_app\", user_id=\"user1\", session_id=session.id\n",
        ")\n",
        "print(\"\\nState after retrieval (temp should be gone):\")\n",
        "\n",
        "print(retrieved_session.state)\n",
        "\n",
        "print(\"\\nApp State:\")\n",
        "print(session_service.app_state)\n",
        "print(\"\\nUser State:\")\n",
        "print(session_service.user_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4ATx8UGHs7x",
        "outputId": "c09937d6-494f-440f-c72a-76398334ff9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'order_status': 'shipped', 'items': ['shirt', 'hat', 'socks'], 'app:max_retries': 3, 'user:pref_contact': 'email', 'tracking_number': 'XYZ456'}\n"
          ]
        }
      ],
      "source": [
        "from google.adk.sessions import InMemorySessionService\n",
        "\n",
        "# Create the session service and session (as before)\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(\n",
        "    app_name=\"my_app\",\n",
        "    user_id=\"user1\",\n",
        "    state={\"order_status\": \"pending\", \"items\": [\"shirt\", \"hat\"], \"notes\": \"Initial order\"},\n",
        ")\n",
        "retrieved_session = session_service.get_session(\n",
        "    app_name=\"my_app\", user_id=\"user1\", session_id=session.id\n",
        ")\n",
        "current_state = retrieved_session.state\n",
        "\n",
        "# --- Simulate a Delta Update (Directly) ---\n",
        "current_state[\"order_status\"] = \"shipped\"  # Update\n",
        "current_state[\"tracking_number\"] = \"XYZ456\"  # Add\n",
        "current_state[\"items\"].append(\"socks\")      # Modify (append to list)\n",
        "del current_state[\"notes\"]                 # Delete\n",
        "\n",
        "print(retrieved_session.state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU_OHcRejR6j"
      },
      "source": [
        "### Accessing Session Properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUzzDIZyjSQL",
        "outputId": "8a0695ab-755c-4397-e368-ed751758c8fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Examining Session Properties ---\n",
            "ID (`id`):                0f0e38f1-fc71-47f2-9c07-b78bf8c04bd6\n",
            "Application Name (`app_name`): my_app\n",
            "User ID (`user_id`):         example_user\n",
            "State (`state`):           {'initial_value': 1}\n",
            "Events (`events`):         []\n",
            "Last Update (`last_update_time`): 1743134817.66\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "from google.adk.sessions import InMemorySessionService\n",
        "\n",
        "\n",
        "# Create a simple session to examine its properties\n",
        "temp_service = InMemorySessionService()\n",
        "example_session = temp_service.create_session(\n",
        "    app_name=\"my_app\",\n",
        "    user_id=\"example_user\",\n",
        "    state={\"initial_value\": 1}\n",
        ")\n",
        "\n",
        "print(f\"--- Examining Session Properties ---\")\n",
        "print(f\"ID (`id`):                {example_session.id}\")   # Unique identifier\n",
        "print(f\"Application Name (`app_name`): {example_session.app_name}\") # Which app it belongs to\n",
        "print(f\"User ID (`user_id`):         {example_session.user_id}\")    # Who the user is\n",
        "print(f\"State (`state`):           {example_session.state}\")       # The dynamic 'notes' dictionary\n",
        "print(f\"Events (`events`):         {example_session.events}\")      # The conversation history (initially empty)\n",
        "print(f\"Last Update (`last_update_time`): {example_session.last_update_time:.2f}\") # When it was last triggered\n",
        "print(f\"---------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYc-fk34FL2h"
      },
      "source": [
        "### Using InMemorySessionService Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOOfCY_k59Bt",
        "outputId": "4742d4d6-dc91-4682-8486-8f4624b13322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Demonstrating InMemorySessionService ---\n",
            "Created Session: ID=mem_session_1, State={'counter': 0}\n",
            "Appended Event, state['counter'] should be 1\n",
            "Retrieved Session: ID=mem_session_1, State={'counter': 1}\n",
            "Events in session: 2\n",
            "List Sessions for user_mem: ['mem_session_1']\n",
            "Deleted Session: mem_session_1\n",
            "Retrieve after delete: Session not found (correct)\n",
            "------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example: Using InMemorySessionService Methods\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.events import Event, EventActions\n",
        "from google.genai import types\n",
        "import time\n",
        "import uuid\n",
        "\n",
        "print(\"\\n--- Demonstrating InMemorySessionService ---\")\n",
        "\n",
        "# 1. Instantiate\n",
        "session_service = InMemorySessionService()\n",
        "app_name, user_id = \"memory_app\", \"user_mem\"\n",
        "session_id = \"mem_session_1\"\n",
        "\n",
        "# 2. Create Session\n",
        "current_session = session_service.create_session(\n",
        "    app_name=app_name, user_id=user_id, session_id=session_id, state={\"counter\": 0}\n",
        ")\n",
        "print(f\"Created Session: ID={current_session.id}, State={current_session.state}\")\n",
        "\n",
        "# 3. Append Event with State Delta\n",
        "user_event = Event(\n",
        "    invocation_id=\"inv_1\", author=\"user\", content=types.Content(parts=[types.Part(text=\"Increment\")])\n",
        ")\n",
        "session_service.append_event(current_session, user_event)  # No state change yet\n",
        "\n",
        "agent_event = Event(\n",
        "    invocation_id=\"inv_2\", author=\"agent\",\n",
        "    actions=EventActions(state_delta={\"counter\": 1})  # Increment counter\n",
        ")\n",
        "session_service.append_event(current_session, agent_event)\n",
        "print(f\"Appended Event, state['counter'] should be 1\")\n",
        "\n",
        "# 4. Get Session\n",
        "retrieved_session = session_service.get_session(app_name, user_id, session_id)\n",
        "print(f\"Retrieved Session: ID={retrieved_session.id}, State={retrieved_session.state}\")\n",
        "print(f\"Events in session: {len(retrieved_session.events)}\")  # Shows 2 events were added\n",
        "\n",
        "# 5. List Sessions\n",
        "session_list = session_service.list_sessions(app_name, user_id)\n",
        "print(f\"List Sessions for {user_id}: {session_list.session_ids}\")\n",
        "\n",
        "# 6. Delete Session\n",
        "session_service.delete_session(app_name, user_id, session_id)\n",
        "print(f\"Deleted Session: {session_id}\")\n",
        "\n",
        "# 7. Get Session (should fail)\n",
        "deleted_session = session_service.get_session(app_name, user_id, session_id)\n",
        "print(f\"Retrieve after delete: {'Session found (unexpected!)' if deleted_session else 'Session not found (correct)'}\")\n",
        "print(\"------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQRDTzpZFIBi"
      },
      "source": [
        "###  Using DatabaseSessionService Methods (with SQLite for demo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmah-tOJBgl0",
        "outputId": "aff0690d-d516-4c2c-8f72-44dc5db1974e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Demonstrating DatabaseSessionService (SQLite) ---\n",
            "Removed existing demo DB file: ./db_sessions_demo.db\n",
            "Instantiated DatabaseSessionService. DB file './db_sessions_demo.db' ensured/created.\n",
            "Created Session: ID=db_session_1, State={'status': 'new'}\n",
            "Appended Event, state should be updated in the database.\n",
            "Retrieved Session: ID=db_session_1, State={'status': 'processing', 'db_key': 'db_val'}\n",
            "List Sessions for user_db: ['db_session_1']\n",
            "Deleted Session: db_session_1\n",
            "Retrieve after delete: Session not found (correct)\n",
            "--------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example: Using DatabaseSessionService Methods (with SQLite for demo)\n",
        "\n",
        "# NOTE: Requires `sqlalchemy` to be installed.\n",
        "# NOTE: This creates a file 'db_sessions_demo.db' in the current directory.\n",
        "from google.adk.sessions import DatabaseSessionService\n",
        "from google.adk.events import Event, EventActions\n",
        "from google.genai import types # Make sure types is imported\n",
        "import time\n",
        "import uuid\n",
        "import os # To manage the demo database file\n",
        "\n",
        "print(\"\\n--- Demonstrating DatabaseSessionService (SQLite) ---\")\n",
        "DB_FILE = \"./db_sessions_demo.db\" # Define path for the database file\n",
        "DB_DIR = os.path.dirname(DB_FILE) # Get directory path\n",
        "\n",
        "# Ensure the directory exists (useful if DB_FILE includes subdirectories)\n",
        "if DB_DIR and not os.path.exists(DB_DIR):\n",
        "    os.makedirs(DB_DIR)\n",
        "    print(f\"Created directory: {DB_DIR}\")\n",
        "\n",
        "# Remove the database file if it exists from a previous run for a clean demo\n",
        "if os.path.exists(DB_FILE):\n",
        "    os.remove(DB_FILE)\n",
        "    print(f\"Removed existing demo DB file: {DB_FILE}\")\n",
        "\n",
        "# 1. Instantiate (using SQLite file)\n",
        "# The DatabaseSessionService's __init__ method will handle DB/table creation.\n",
        "db_service = DatabaseSessionService(db_url=f\"sqlite:///{DB_FILE}\")\n",
        "print(f\"Instantiated DatabaseSessionService. DB file '{DB_FILE}' ensured/created.\")\n",
        "\n",
        "APP_DB, USER_DB = \"db_app\", \"user_db\"\n",
        "SESSION_ID_DB = \"db_session_1\"\n",
        "\n",
        "# 2. Create Session\n",
        "session_db = db_service.create_session(\n",
        "    app_name=APP_DB, user_id=USER_DB, session_id=SESSION_ID_DB, state={\"status\": \"new\"}\n",
        ")\n",
        "print(f\"Created Session: ID={session_db.id}, State={session_db.state}\")\n",
        "\n",
        "# 3. Append Event with State Delta\n",
        "# *** FIX: Ensure event has a 'content' object, even if minimal ***\n",
        "event_db_1 = Event(\n",
        "    invocation_id=\"inv_db1\", author=\"agent\",\n",
        "    content=types.Content(parts=[types.Part(text=\"System update: Processing\")]), # Add content\n",
        "    actions=EventActions(state_delta={\"status\": \"processing\", \"db_key\": \"db_val\"})\n",
        ")\n",
        "# Note: append_event updates the state in the DB and the passed session's last_update_time\n",
        "db_service.append_event(session_db, event_db_1)\n",
        "print(f\"Appended Event, state should be updated in the database.\")\n",
        "\n",
        "# 4. Get Session (re-fetch from DB to see persisted changes)\n",
        "# Note: Must re-fetch session to see DB changes reflected in the object state\n",
        "retrieved_session_db = db_service.get_session(APP_DB, USER_DB, SESSION_ID_DB)\n",
        "print(f\"Retrieved Session: ID={retrieved_session_db.id}, State={retrieved_session_db.state}\")\n",
        "# Note: Events are not automatically loaded by get_session in this implementation by default.\n",
        "\n",
        "# 5. List Sessions\n",
        "sessions_list_db = db_service.list_sessions(APP_DB, USER_DB)\n",
        "print(f\"List Sessions for {USER_DB}: {sessions_list_db.session_ids}\")\n",
        "\n",
        "# 6. List Events (Not Implemented in DatabaseSessionService base implementation)\n",
        "try:\n",
        "    db_service.list_events(APP_DB, USER_DB, SESSION_ID_DB)\n",
        "except NotImplementedError as e:\n",
        "    print(f\"List Events: Received NotImplementedError (as expected in base class)\")\n",
        "except AttributeError as e:\n",
        "     print(f\"List Events: Method not found or not implemented (as expected in base class)\")\n",
        "\n",
        "# 7. Delete Session\n",
        "db_service.delete_session(APP_DB, USER_DB, SESSION_ID_DB)\n",
        "print(f\"Deleted Session: {SESSION_ID_DB}\")\n",
        "\n",
        "# 8. Get Session (should fail)\n",
        "deleted_session_check_db = db_service.get_session(APP_DB, USER_DB, SESSION_ID_DB)\n",
        "print(f\"Retrieve after delete: {'Session found (unexpected!)' if deleted_session_check_db else 'Session not found (correct)'}\")\n",
        "\n",
        "# Cleanup demo file (optional, good practice for demos)\n",
        "# if os.path.exists(DB_FILE):\n",
        "#     os.remove(DB_FILE)\n",
        "#     print(f\"Cleaned up demo DB file: {DB_FILE}\")\n",
        "print(\"--------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtgJPaV1Qw6j"
      },
      "source": [
        "### LlMAgent with AntrhopicVertex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptc7IKdrQxLx",
        "outputId": "0a39001a-a64f-4639-b09c-34f581413e08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running Anthropic Agent Example ---\n",
            "Using model: claude-3-5-sonnet-v2@20241022\n",
            "Make sure GCP Project 'lavi-llm-experiment' and Location 'us-east5' are correct and the model is available there.\n",
            "Ensure you are authenticated with GCP (e.g., `gcloud auth application-default login`).\n",
            "\n",
            "User Query: What is the capital of France?\n",
            "Agent Response: Paris\n",
            "\n",
            "User Query: What's the capital of Japan?\n",
            "Agent Response: Tokyo\n",
            "\n",
            "User Query: Tell me the capital of Germany.\n",
            "Agent Response: Berlin\n",
            "--- Example Finished ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import asyncio\n",
        "import warnings\n",
        "\n",
        "# --- GCP/Vertex AI Configuration ---\n",
        "# Make sure these are set in your environment OR uncomment and set here\n",
        "# os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"your-project-id\"\n",
        "# os.environ[\"GOOGLE_CLOUD_LOCATION\"] = \"us-east5\" # Make sure its us-east5 or europe-west1\n",
        "\n",
        "# --- ADK Imports ---\n",
        "from google.adk.agents import Agent, LlmAgent # Using LlmAgent directly\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types\n",
        "from google.adk.models.anthropic_llm import Claude\n",
        "from google.adk.models.registry import LLMRegistry\n",
        "\n",
        "# Manually register the Claude model class with the registry\n",
        "# This step is crucial if the framework doesn't do it automatically\n",
        "LLMRegistry.register(Claude)\n",
        "\n",
        "\n",
        "# --- Constants ---\n",
        "APP_NAME = \"anthropic_capital_app\"\n",
        "USER_ID = \"anthropic_user\"\n",
        "SESSION_ID = \"anthropic_session_1\"\n",
        "AGENT_NAME = \"claude_capital_agent\"\n",
        "# --- Anthropic Model Name (via Vertex AI) ---\n",
        "# Use the model identifier available in your Vertex AI region.\n",
        "# Example: Claude 3.5 Sonnet. Check Vertex AI documentation for available models.\n",
        "ANTHROPIC_MODEL = \"claude-3-7-sonnet@20250219\"\n",
        "# You might need to adjust this based on availability in your specific GCP project/location.\n",
        "# Other possibilities: \"claude-3-haiku@...\", \"claude-3-opus@...\"\n",
        "\n",
        "# --- Agent Definition ---\n",
        "# Simplest agent: takes user input, uses the LLM directly to answer.\n",
        "capital_agent = LlmAgent(\n",
        "    model=ANTHROPIC_MODEL, # Specify the Anthropic model identifier\n",
        "    name=AGENT_NAME,\n",
        "    instruction=\"You are a helpful assistant. When asked for the capital of a country, provide only the name of the capital city.\",\n",
        "    description=\"An agent that provides the capital city of a country using an Anthropic model.\",\n",
        "    # No tools needed for this simple task\n",
        "    tools=[],\n",
        "    allow_transfer=False # Keep it simple\n",
        ")\n",
        "\n",
        "# --- Session and Runner Setup ---\n",
        "session_service = InMemorySessionService()\n",
        "# Ensure session is created before running\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=capital_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "# --- Agent Interaction Logic ---\n",
        "# Using async version as it's preferred\n",
        "async def call_agent_async(query):\n",
        "  print(f\"\\nUser Query: {query}\")\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  final_response_text = \"Agent did not produce a final response.\"\n",
        "  try:\n",
        "      async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
        "          if event.is_final_response() and event.content and event.content.parts:\n",
        "              final_response_text = event.content.parts[0].text\n",
        "              print(f\"Agent Response: {final_response_text}\")\n",
        "              # Break after final response for simplicity in this example\n",
        "              break\n",
        "          elif event.error_message:\n",
        "              final_response_text = f\"Agent Error: {event.error_message}\"\n",
        "              print(final_response_text)\n",
        "              break\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred during agent execution: {e}\")\n",
        "      final_response_text = f\"Execution Error: {e}\"\n",
        "\n",
        "  return final_response_text\n",
        "\n",
        "# --- Example Usage ---\n",
        "async def run_example():\n",
        "    print(\"--- Running Anthropic Agent Example ---\")\n",
        "    print(f\"Using model: {ANTHROPIC_MODEL}\")\n",
        "    print(f\"Make sure GCP Project '{os.environ['GOOGLE_CLOUD_PROJECT']}' and Location '{os.environ['GOOGLE_CLOUD_LOCATION']}' are correct and the model is available there.\")\n",
        "    print(\"Ensure you are authenticated with GCP (e.g., `gcloud auth application-default login`).\")\n",
        "\n",
        "    await call_agent_async(\"What is the capital of France?\")\n",
        "    await call_agent_async(\"What's the capital of Japan?\")\n",
        "    await call_agent_async(\"Tell me the capital of Germany.\")\n",
        "    print(\"--- Example Finished ---\")\n",
        "\n",
        "await run_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFLdX7tcIRuz"
      },
      "source": [
        "### LlmAgent with Artifact Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WDdQocw8t28",
        "outputId": "c398e1ed-4ea5-4f4c-f9e5-7ecbee1e804d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "K_e3JrZQISCl",
        "outputId": "bbbf5b5b-7a41-4311-fab5-c3d63e800bf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verified local PDF exists: /content/multimodal-finanace-qa_data_unstructured_production_blogpost_Hex-LLM on TPUs in Vertex AI Model Garden _ Google Cloud Blog.pdf\n"
          ]
        },
        {
          "ename": "ValidationError",
          "evalue": "2 validation errors for LlmAgent\ntools.0.callable\n  Input should be callable [type=callable_type, input_value=<module 'google.adk.tools...load_artifacts_tool.py'>, input_type=module]\n    For further information visit https://errors.pydantic.dev/2.11/v/callable_type\ntools.0.is-instance[BaseTool]\n  Input should be an instance of BaseTool [type=is_instance_of, input_value=<module 'google.adk.tools...load_artifacts_tool.py'>, input_type=module]\n    For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2144912e73ba>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# --- Agent Definition ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# We remove the load_document tool as loading happens externally now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m doc_qa_agent = LlmAgent(\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGEMINI_2_FLASH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAGENT_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for LlmAgent\ntools.0.callable\n  Input should be callable [type=callable_type, input_value=<module 'google.adk.tools...load_artifacts_tool.py'>, input_type=module]\n    For further information visit https://errors.pydantic.dev/2.11/v/callable_type\ntools.0.is-instance[BaseTool]\n  Input should be an instance of BaseTool [type=is_instance_of, input_value=<module 'google.adk.tools...load_artifacts_tool.py'>, input_type=module]\n    For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of"
          ]
        }
      ],
      "source": [
        "# --- Imports ---\n",
        "import os\n",
        "import fitz # Still needed if you want to verify the local file exists or for other ops\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.genai import types\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.artifacts.in_memory_artifact_service import InMemoryArtifactService # Import InMemoryArtifactService\n",
        "from google.adk.runners import Runner\n",
        "# We no longer need the custom load_document tool\n",
        "# from google.adk.tools.function_tool import FunctionTool\n",
        "# from google.adk.tools.tool_context import ToolContext\n",
        "from google.adk.tools import load_artifacts_tool # Import the built-in tool\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional, List\n",
        "\n",
        "# --- Constants ---\n",
        "APP_NAME = \"doc_qa_app\"\n",
        "USER_ID = \"doc_user_1\"\n",
        "SESSION_ID = \"doc_session_1\"\n",
        "AGENT_NAME = \"doc_qa_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\" # Or \"gemini-1.5-flash-latest\"\n",
        "\n",
        "# --- Actual File Paths ---\n",
        "# Make sure these paths are correct for your environment\n",
        "LOCAL_PDF_PATH = \"/content/multimodal-finanace-qa_data_unstructured_production_blogpost_Hex-LLM on TPUs in Vertex AI Model Garden _ Google Cloud Blog.pdf\"\n",
        "GCS_PDF_URI = \"gs://mlops-for-genai/multimodal-finanace-qa/data/unstructured/prototype/google_cloud_tpu_blog_training.pdf\"\n",
        "\n",
        "\n",
        "\n",
        "# --- Agent Definition ---\n",
        "# We remove the load_document tool as loading happens externally now\n",
        "doc_qa_agent = LlmAgent(\n",
        "    model=GEMINI_2_FLASH,\n",
        "    name=AGENT_NAME,\n",
        "    description=\"An agent that answers questions based on provided PDF documents.\",\n",
        "    # Updated instructions reflecting pre-loaded artifacts\n",
        "    instruction=f\"\"\"You are a Document Question Answering Agent. Your goal is to answer user questions based *only* on the content of PDF documents that have been loaded as artifacts.\n",
        "\n",
        "Documents are already available as artifacts named 'local_doc' and 'gcs_doc'.\n",
        "\n",
        "Follow these steps when the user asks a question:\n",
        "1.  Determine which artifact ('local_doc' or 'gcs_doc') is relevant to the user's question. If the user doesn't specify, you might need to ask or infer based on the question content.\n",
        "2.  Use the `load_artifacts` tool to load the content of the relevant artifact(s) into your context. Provide the artifact name(s) you determined in step 1.\n",
        "3.  Once the artifact content is loaded (you'll receive it as a tool response), answer the user's question based *strictly* on the provided text from the artifact.\n",
        "4.  If the answer isn't found in the artifact, state that clearly. Do not use your general knowledge.\n",
        "5.  You can also use the artifact service tools implicitly provided to list artifacts if needed.\n",
        "\"\"\",\n",
        "    tools=[\n",
        "        load_artifacts_tool # Only need the built-in load_artifacts tool now\n",
        "        ],\n",
        "    generate_content_config=types.GenerateContentConfig(\n",
        "        max_output_tokens=1024,\n",
        "        temperature=0.2\n",
        "    ),\n",
        ")\n",
        "\n",
        "# --- Session, Artifact Service, and Runner Setup ---\n",
        "session_service = InMemorySessionService()\n",
        "artifact_service = InMemoryArtifactService() # Use the in-memory artifact service\n",
        "\n",
        "# --- Pre-loading Artifacts Directly ---\n",
        "print(\"\\n=== Pre-loading Artifacts ===\")\n",
        "try:\n",
        "    # 1. Load Local PDF\n",
        "    print(f\"Loading local file: {LOCAL_PDF_PATH}\")\n",
        "    with open(LOCAL_PDF_PATH, \"rb\") as f:\n",
        "        local_pdf_bytes = f.read()\n",
        "    local_part = types.Part.from_bytes(data=local_pdf_bytes, mime_type='application/pdf')\n",
        "    local_version = artifact_service.save_artifact(\n",
        "        app_name=APP_NAME,\n",
        "        user_id=USER_ID,\n",
        "        session_id=SESSION_ID,\n",
        "        filename='local_doc', # Assign the name the agent will use\n",
        "        artifact=local_part\n",
        "    )\n",
        "    print(f\"  Saved 'local_doc' (version {local_version}) to artifact service.\")\n",
        "\n",
        "    # 2. Load GCS PDF\n",
        "    # Ensure your Colab environment is authenticated if the URI isn't public\n",
        "    # from google.colab import auth\n",
        "    # auth.authenticate_user() # Uncomment if needed\n",
        "    print(f\"Loading GCS file: {GCS_PDF_URI}\")\n",
        "    gcs_part = types.Part.from_uri(uri=GCS_PDF_URI, mime_type='application/pdf')\n",
        "    gcs_version = artifact_service.save_artifact(\n",
        "        app_name=APP_NAME,\n",
        "        user_id=USER_ID,\n",
        "        session_id=SESSION_ID,\n",
        "        filename='gcs_doc', # Assign the name the agent will use\n",
        "        artifact=gcs_part\n",
        "    )\n",
        "    print(f\"  Saved 'gcs_doc' (version {gcs_version}) to artifact service.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error pre-loading artifact: {e}\")\n",
        "    exit() # Stop if files aren't found\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during pre-loading: {e}\")\n",
        "    # Depending on the error (e.g., GCS auth), you might want to exit or continue\n",
        "    exit()\n",
        "\n",
        "print(\"=== Artifact Pre-loading Complete ===\\n\")\n",
        "\n",
        "\n",
        "# *** Crucial Step: Pass the SAME artifact_service instance to the Runner ***\n",
        "runner = Runner(\n",
        "    agent=doc_qa_agent,\n",
        "    app_name=APP_NAME,\n",
        "    session_service=session_service,\n",
        "    artifact_service=artifact_service # Make sure runner uses the service with pre-loaded data\n",
        ")\n",
        "\n",
        "# Create the session (state is initially empty, artifacts are separate)\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "\n",
        "# --- Agent Interaction Function (Modified slightly for clarity) ---\n",
        "def call_agent(query):\n",
        "  print(f\"\\n--- User Query ---\\n{query}\")\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  final_response_text = \"Agent did not provide a final text response.\"\n",
        "  print(\"\\n--- Agent Execution Trace ---\")\n",
        "\n",
        "  try:\n",
        "      events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "      for event in events:\n",
        "          if event.content:\n",
        "              if event.get_function_calls():\n",
        "                  fc = event.get_function_calls()[0]\n",
        "                  print(f\"  [LLM -> Tool Request] Function Call: {fc.name}({fc.args})\")\n",
        "              elif event.get_function_responses():\n",
        "                   fr = event.get_function_responses()[0]\n",
        "                   response_str = str(fr.response)\n",
        "                   if len(response_str) > 300: # Increased truncation length\n",
        "                       response_str = response_str[:300] + \"... (truncated)\"\n",
        "                   print(f\"  [Tool -> LLM Response] Function Result ({fr.name}): {response_str}\")\n",
        "              elif event.content.parts[0].text:\n",
        "                    print(f\"  [LLM -> User] Agent Thought/Response: {event.content.parts[0].text.strip()}\")\n",
        "                    if event.is_final_response():\n",
        "                         final_response_text = event.content.parts[0].text.strip()\n",
        "              else:\n",
        "                  print(f\"  [Event] Other content: {event.content}\")\n",
        "          elif event.actions:\n",
        "               print(f\"  [Event] Actions: {event.actions}\")\n",
        "          else:\n",
        "               print(f\"  [Event] Other event type: {type(event)}\")\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"\\n--- Error during agent execution ---\")\n",
        "      print(e)\n",
        "      # Attempt to retrieve the latest state even on error\n",
        "      retrieved_session = session_service.get_session(APP_NAME, USER_ID, SESSION_ID)\n",
        "      print(\"Session state on error:\", retrieved_session.state if retrieved_session else \"N/A\")\n",
        "      final_response_text = f\"An error occurred: {e}\"\n",
        "\n",
        "  print(f\"\\n--- Final Agent Response ---\")\n",
        "  print(final_response_text)\n",
        "  print(\"-\" * (len(\"--- Final Agent Response ---\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfhlC_U69nWx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Demonstrating Artifact Service Methods & Agent Interaction ---\n",
        "\n",
        "print(\"\\n=== Demonstrating Artifact Service After Pre-load ===\\n\")\n",
        "\n",
        "# 1. List Artifact Keys (Verify pre-load)\n",
        "print(\"1. Listing available artifacts...\")\n",
        "current_artifacts = artifact_service.list_artifact_keys(APP_NAME, USER_ID, SESSION_ID)\n",
        "print(f\"  Current artifact keys: {current_artifacts}\") # Should show ['gcs_doc', 'local_doc']\n",
        "\n",
        "# 2. List Versions\n",
        "print(\"\\n2. Listing versions for 'local_doc'...\")\n",
        "local_doc_versions = artifact_service.list_versions(APP_NAME, USER_ID, SESSION_ID, 'local_doc')\n",
        "print(f\"  Versions for 'local_doc': {local_doc_versions}\")\n",
        "\n",
        "# 3. Ask agent questions (Should trigger load_artifacts tool)\n",
        "print(\"\\n3. Asking a question about the local document (Hex LLM Blog)...\")\n",
        "call_agent(\"What is this blog post about? Use the local_doc artifact.\")\n",
        "\n",
        "print(\"\\n4. Asking a question about the GCS document (TPU Blog)...\")\n",
        "call_agent(\"What is the main topic of the document named 'gcs_doc'?\")\n",
        "\n",
        "# 4. Delete Artifact\n",
        "print(\"\\n5. Deleting 'local_doc' artifact...\")\n",
        "artifact_service.delete_artifact(APP_NAME, USER_ID, SESSION_ID, 'local_doc')\n",
        "print(\"  'local_doc' deleted.\")\n",
        "current_artifacts_after_delete = artifact_service.list_artifact_keys(APP_NAME, USER_ID, SESSION_ID)\n",
        "print(f\"  Current artifact keys after delete: {current_artifacts_after_delete}\")\n",
        "\n",
        "print(\"\\n6. Trying to ask about the deleted document...\")\n",
        "call_agent(\"Summarize the local_doc artifact for me.\") # Agent should state it cannot find 'local_doc'\n",
        "\n",
        "print(\"\\n=== End of Demonstration ===\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn_4RtWb9nRi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL3kCAdK9nOr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOfQIQXz9mNS"
      },
      "outputs": [],
      "source": [
        "# --- Demonstrating Artifact Service Methods ---\n",
        "\n",
        "print(\"\\n=== Demonstrating Artifact Service ===\\n\")\n",
        "\n",
        "# 1. Save Artifact (via the load_document tool)\n",
        "print(\"1. Loading local document...\")\n",
        "call_agent(f\"Load the document at '{DUMMY_LOCAL_PDF_PATH}' and call it 'local_doc'.\")\n",
        "\n",
        "print(\"\\n2. Loading GCS document...\")\n",
        "# Ensure your Colab environment is authenticated to access GCS if the URI isn't public\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "call_agent(f\"Load the document from '{GCS_PDF_URI}' and name it 'gcs_doc'.\")\n",
        "\n",
        "# 2. List Artifact Keys\n",
        "print(\"\\n3. Listing available artifacts...\")\n",
        "current_artifacts = artifact_service.list_artifact_keys(APP_NAME, USER_ID, SESSION_ID)\n",
        "print(f\"  Current artifact keys: {current_artifacts}\")\n",
        "\n",
        "# 3. List Versions\n",
        "print(\"\\n4. Listing versions for 'local_doc'...\")\n",
        "local_doc_versions = artifact_service.list_versions(APP_NAME, USER_ID, SESSION_ID, 'local_doc')\n",
        "print(f\"  Versions for 'local_doc': {local_doc_versions}\")\n",
        "\n",
        "# 4. Load Artifact (via the agent asking a question)\n",
        "print(\"\\n5. Asking a question about the local document...\")\n",
        "call_agent(\"What is the dummy local document about?\") # This should trigger load_artifacts tool\n",
        "\n",
        "print(\"\\n6. Asking a question about the GCS document...\")\n",
        "# Note: The content of the public GCS PDF (2403.05530.pdf) is about \"Gemini 1.5\".\n",
        "call_agent(\"What model is discussed in the 'gcs_doc' artifact?\")\n",
        "\n",
        "# 5. Delete Artifact\n",
        "print(\"\\n7. Deleting 'local_doc' artifact...\")\n",
        "artifact_service.delete_artifact(APP_NAME, USER_ID, SESSION_ID, 'local_doc')\n",
        "print(\"  'local_doc' deleted.\")\n",
        "current_artifacts_after_delete = artifact_service.list_artifact_keys(APP_NAME, USER_ID, SESSION_ID)\n",
        "print(f\"  Current artifact keys after delete: {current_artifacts_after_delete}\")\n",
        "\n",
        "print(\"\\n8. Trying to ask about the deleted document...\")\n",
        "call_agent(\"Tell me again, what was the local document about?\") # Agent should state it cannot find the artifact\n",
        "\n",
        "print(\"\\n=== End of Demonstration ===\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkxOWKJGiPv4"
      },
      "outputs": [],
      "source": [
        "# --- Setup: Install necessary libraries ---\n",
        "# pypdf is used to read local PDF bytes for the from_bytes example\n",
        "!pip install google-cloud-aiplatform google-adk pypdf --quiet\n",
        "\n",
        "# --- ADK Imports ---\n",
        "import os\n",
        "import asyncio\n",
        "import warnings\n",
        "import mimetypes\n",
        "from typing import Any, Optional, Dict, List, AsyncGenerator\n",
        "\n",
        "# Use pypdf for reading local PDF bytes\n",
        "try:\n",
        "    import pypdf\n",
        "except ImportError:\n",
        "    print(\"Please install pypdf: !pip install pypdf\")\n",
        "    pypdf = None # Set to None if import fails\n",
        "\n",
        "from google.adk.agents import Agent, LlmAgent\n",
        "from google.adk.sessions import InMemorySessionService, Session\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.artifacts import BaseArtifactService, InMemoryArtifactService, GcsArtifactService\n",
        "from google.adk.tools import load_artifacts # Import the specific tool\n",
        "from google.genai import types\n",
        "\n",
        "\n",
        "# --- Constants ---\n",
        "APP_NAME_BASE = \"pdf_summarizer_app\"\n",
        "USER_ID = \"pdf_user\"\n",
        "AGENT_NAME = \"pdf_summarizer_agent\"\n",
        "MODEL_NAME = \"gemini-2.0-flash-001\" # Changed to generally available model\n",
        "\n",
        "# --- Helper Function to Save PDF Artifact ---\n",
        "def save_pdf_artifact(\n",
        "    file_path_or_uri: str,\n",
        "    artifact_service: BaseArtifactService,\n",
        "    app_name: str,\n",
        "    user_id: str,\n",
        "    session_id: str,\n",
        "    save_as_filename: Optional[str] = None\n",
        ") -> Optional[str]:\n",
        "    \"\"\"Creates a Part (from_uri or from_bytes) and saves it using the artifact service.\"\"\"\n",
        "    print(f\"  Processing PDF: {file_path_or_uri}\")\n",
        "    actual_filename = os.path.basename(file_path_or_uri) if not save_as_filename else save_as_filename\n",
        "    pdf_part: Optional[types.Part] = None\n",
        "    mime_type = \"application/pdf\" # Explicitly PDF\n",
        "\n",
        "    try:\n",
        "        if file_path_or_uri.startswith(\"gs://\"):\n",
        "            # Create a Part referencing the GCS URI\n",
        "            pdf_part = types.Part.from_uri(uri=file_path_or_uri, mime_type=mime_type)\n",
        "            print(f\"    Created Part from URI.\")\n",
        "\n",
        "        else:\n",
        "            # Local File: Read bytes and create Part from bytes\n",
        "            if not pypdf:\n",
        "                 print(\"    Error: pypdf library is required to read local PDF bytes but not found.\")\n",
        "                 return None\n",
        "            if not os.path.exists(file_path_or_uri):\n",
        "                print(f\"    Error: Local PDF file not found: {file_path_or_uri}\")\n",
        "                return None\n",
        "\n",
        "            # --- Reading Local PDF Bytes ---\n",
        "            # Note: For summarization, sending extracted text is often better for the LLM.\n",
        "            # However, to demonstrate from_bytes with the *raw PDF*, we read the raw bytes.\n",
        "            # The LLM needs multimodal capabilities to process raw PDF bytes directly.\n",
        "            # Gemini 2 Flash might struggle with raw PDF bytes. Gemini 1.5 Pro is better.\n",
        "            # Consider extracting text first if using a model less capable with raw PDFs.\n",
        "            with open(file_path_or_uri, \"rb\") as f:\n",
        "                 pdf_bytes = f.read()\n",
        "            pdf_part = types.Part.from_bytes(data=pdf_bytes, mime_type=mime_type)\n",
        "            print(f\"    Created Part from bytes ({len(pdf_bytes)} bytes).\")\n",
        "            # --- End Reading Local PDF Bytes ---\n",
        "\n",
        "            # --- Alternative: Extract Text from Local PDF (Often better for LLM) ---\n",
        "            # pdf_reader = pypdf.PdfReader(file_path_or_uri)\n",
        "            # extracted_text = \"\".join(page.extract_text() or \"\" for page in pdf_reader.pages)\n",
        "            # if not extracted_text:\n",
        "            #     print(f\"    Warning: Could not extract text from local PDF: {file_path_or_uri}\")\n",
        "            #     return None\n",
        "            # text_bytes = extracted_text.encode('utf-8')\n",
        "            # pdf_part = types.Part.from_bytes(data=text_bytes, mime_type='text/plain') # Send as text\n",
        "            # print(f\"    Created Part from extracted text ({len(text_bytes)} bytes).\")\n",
        "            # --- End Alternative ---\n",
        "\n",
        "\n",
        "        if pdf_part:\n",
        "            version = artifact_service.save_artifact(\n",
        "                app_name=app_name,\n",
        "                user_id=user_id,\n",
        "                session_id=session_id,\n",
        "                filename=actual_filename,\n",
        "                artifact=pdf_part\n",
        "            )\n",
        "            print(f\"    Saved artifact '{actual_filename}' as version {version}.\")\n",
        "            return actual_filename\n",
        "        else:\n",
        "             print(f\"    Error: Could not create Part for {file_path_or_uri}\")\n",
        "             return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    Error processing file {file_path_or_uri}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Agent Definition ---\n",
        "pdf_summarizer_agent = LlmAgent(\n",
        "    model=MODEL_NAME,\n",
        "    name=AGENT_NAME,\n",
        "    instruction=f\"\"\"You are a PDF document summarization assistant.\n",
        "You will be given the filename of a PDF saved as an artifact.\n",
        "Your task is to:\n",
        "1. Use the `{load_artifacts.name}` tool to load the content of the specified PDF artifact.\n",
        "2. Read the content provided by the tool (which might be the full text or the raw PDF data depending on the model's capability).\n",
        "3. Provide a concise summary of the document.\n",
        "If you cannot access or process the document, state that clearly.\n",
        "\"\"\",\n",
        "    description=\"Summarizes PDF documents provided as artifacts.\",\n",
        "    tools=[load_artifacts], # Make the tool available\n",
        "    allow_transfer=False,\n",
        ")\n",
        "\n",
        "# --- Agent Interaction Logic ---\n",
        "async def run_pdf_summarization(\n",
        "        artifact_service: BaseArtifactService,\n",
        "        session_service: InMemorySessionService,\n",
        "        app_name: str,\n",
        "        session_id: str,\n",
        "        pdf_path_or_uri: str):\n",
        "    \"\"\"Saves the PDF as an artifact and runs the summarizer agent.\"\"\"\n",
        "\n",
        "    print(f\"\\n--- Starting PDF Summarization using {artifact_service.__class__.__name__} ---\")\n",
        "    session = session_service.create_session(app_name=app_name, user_id=USER_ID, session_id=session_id)\n",
        "    runner = Runner(agent=pdf_summarizer_agent, app_name=app_name, session_service=session_service, artifact_service=artifact_service)\n",
        "\n",
        "    # 1. Save PDF as artifact\n",
        "    print(\"\\nStep 1: Saving PDF as artifact...\")\n",
        "    saved_filename = save_pdf_artifact(pdf_path_or_uri, artifact_service, app_name, USER_ID, session_id)\n",
        "\n",
        "    if not saved_filename:\n",
        "        print(\"Failed to save PDF as artifact. Aborting summarization.\")\n",
        "        return\n",
        "\n",
        "    # 2. Create the user prompt mentioning the artifact\n",
        "    user_query = f\"Please summarize the document named `{saved_filename}`.\"\n",
        "    print(f\"\\nStep 2: User Query to Agent: '{user_query}'\")\n",
        "\n",
        "    # 3. Run the agent\n",
        "    print(\"\\nStep 3: Running the Summarizer Agent...\")\n",
        "    content = types.Content(role='user', parts=[types.Part(text=user_query)])\n",
        "    final_response_text = \"[Agent did not produce a final response text]\"\n",
        "\n",
        "    try:\n",
        "        async for event in runner.run_async(user_id=USER_ID, session_id=session_id, new_message=content):\n",
        "            # Basic flow tracing\n",
        "            if event.get_function_calls():\n",
        "                fc = event.get_function_calls()[0]\n",
        "                print(f\"  [Agent requested tool: {fc.name} with args: {fc.args}]\")\n",
        "            elif event.get_function_responses():\n",
        "                fr = event.get_function_responses()[0]\n",
        "                print(f\"  [Agent received result for tool: {fr.name}] (Content omitted for brevity)\")\n",
        "\n",
        "            # Capture final response\n",
        "            if event.is_final_response() and event.content and event.content.parts:\n",
        "                 if event.content.parts[0].text:\n",
        "                      final_response_text = event.content.parts[0].text\n",
        "                 else:\n",
        "                      final_response_text = \"[Non-text final response received]\"\n",
        "                 break # Exit loop on final response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  [Error during agent run]: {e}\")\n",
        "        final_response_text = f\"[Error occurred: {e}]\"\n",
        "\n",
        "\n",
        "    print(f\"\\nStep 4: Final Agent Response:\\n{final_response_text}\")\n",
        "    print(f\"--- Flow Finished for {artifact_service.__class__.__name__} ---\")\n",
        "\n",
        "# --- Main Execution Function ---\n",
        "async def main():\n",
        "\n",
        "    # === Example 1: Using InMemoryArtifactService with Local PDF ===\n",
        "    if LOCAL_PDF_PATH and LOCAL_PDF_PATH != \"/path/to/your/local/document.pdf\" and pypdf:\n",
        "        mem_app_name = f\"{APP_NAME_BASE}_mem\"\n",
        "        mem_session_id = f\"{USER_ID}_mem_sess_1\"\n",
        "        mem_artifact_service = InMemoryArtifactService()\n",
        "        mem_session_service = InMemorySessionService()\n",
        "        await run_pdf_summarization(\n",
        "            mem_artifact_service,\n",
        "            mem_session_service,\n",
        "            mem_app_name,\n",
        "            mem_session_id,\n",
        "            LOCAL_PDF_PATH\n",
        "        )\n",
        "    else:\n",
        "        print(\"\\nSkipping InMemoryArtifactService example: LOCAL_PDF_PATH not set or pypdf not available.\")\n",
        "\n",
        "    # === Example 2: Using GcsArtifactService with GCS PDF ===\n",
        "    if GCS_PDF_URI and GCS_PDF_URI != \"gs://your-gcs-bucket-name/path/to/your/document.pdf\":\n",
        "        gcs_app_name = f\"{APP_NAME_BASE}_gcs\"\n",
        "        gcs_session_id = f\"{USER_ID}_gcs_sess_1\"\n",
        "        gcs_artifact_service = GcsArtifactService(bucket_name=GCS_BUCKET_NAME)\n",
        "        gcs_session_service = InMemorySessionService() # Session state still in memory\n",
        "        await run_pdf_summarization(\n",
        "            gcs_artifact_service,\n",
        "            gcs_session_service,\n",
        "            gcs_app_name,\n",
        "            gcs_session_id,\n",
        "            GCS_PDF_URI\n",
        "        )\n",
        "    else:\n",
        "        print(\"\\nSkipping GcsArtifactService example: GCS_PDF_URI not set.\")\n",
        "\n",
        "\n",
        "# --- Run the Examples ---\n",
        "try:\n",
        "    # Get/create event loop for Colab/script compatibility\n",
        "    loop = asyncio.get_running_loop()\n",
        "except RuntimeError:\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "\n",
        "loop.run_until_complete(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4eum1l5eLhI"
      },
      "source": [
        "## Non-LLM Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2hOcAEld7oT"
      },
      "source": [
        "## Multi-Agent - SequenceAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXwdF6JFnWNB"
      },
      "source": [
        "### Simple sequence flow; Parent->Child(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XihTsvVHU8v",
        "outputId": "f40af520-c4d0-4742-dc4b-0415c19ede01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from uuid import uuid4\n",
        "from google.adk.agents.base_agent import BaseAgent\n",
        "from google.adk.agents.sequential_agent import SequentialAgent\n",
        "from google.adk.agents.invocation_context import InvocationContext, new_invocation_context_id\n",
        "from google.adk.events import Event\n",
        "from typing_extensions import override\n",
        "from google.adk.sessions.in_memory_session_service import InMemorySessionService\n",
        "from google.adk.sessions.session import Session\n",
        "from google.genai import types\n",
        "from typing import AsyncGenerator\n",
        "\n",
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "async def main():\n",
        "\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the SequentialAgent\n",
        "    sequential_agent = SequentialAgent(name=\"SequentialAgent\", children=[agent_a, agent_b])\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=sequential_agent,\n",
        "        user_content=types.Content(\n",
        "                parts=[types.Part(text=\"execute\")]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Run the SequentialAgent\n",
        "    async for event in sequential_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l61HK0A6eAK"
      },
      "source": [
        "### Simple sequence flow; Parent->Child(2) - without InnvocationContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ZyIVLGnaPG"
      },
      "source": [
        "### Passing state between Children"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfr_cRxsXSo_",
        "outputId": "d8d20543-b2e2-4841-8958-cd7dfc16e717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Received value: Hello from Agent A!\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from uuid import uuid4\n",
        "from google.adk.agents.base_agent import BaseAgent\n",
        "from google.adk.agents.sequential_agent import SequentialAgent\n",
        "from google.adk.agents.invocation_context import InvocationContext, new_invocation_context_id\n",
        "from google.adk.events import Event\n",
        "from typing_extensions import override\n",
        "from google.adk.sessions.in_memory_session_service import InMemorySessionService\n",
        "from google.adk.sessions.session import Session\n",
        "from google.genai import types\n",
        "from typing import AsyncGenerator\n",
        "\n",
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        # Set a value in the session state\n",
        "        ctx.session.state[\"agent_a_value\"] = \"Hello from Agent A!\"\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        # Retrieve the value from the session state\n",
        "        agent_a_value = ctx.session.state.get(\"agent_a_value\")\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=f\"Agent B: Received value: {agent_a_value}\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "async def main():\n",
        "\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the SequentialAgent\n",
        "    sequential_agent = SequentialAgent(name=\"SequentialAgent\", children=[agent_a, agent_b])\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=sequential_agent,\n",
        "        user_content=types.Content(\n",
        "                parts=[types.Part(text=\"execute\")]\n",
        "            )\n",
        "\n",
        "        )\n",
        "\n",
        "    # Run the SequentialAgent\n",
        "    async for event in sequential_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjsvzuGPK7Rh"
      },
      "source": [
        "### Sequence with LLMAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LekKce1MqqrN",
        "outputId": "f98961c7-e92a-4358-d3f8-688bf5f90e21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "\n",
            "Event: AgentB: Agent B: Starting...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from uuid import uuid4\n",
        "from google.adk.agents.base_agent import BaseAgent\n",
        "from google.adk.agents.sequential_agent import SequentialAgent\n",
        "from google.adk.agents.invocation_context import InvocationContext, new_invocation_context_id\n",
        "from google.adk.events import Event\n",
        "from typing_extensions import override\n",
        "from google.adk.sessions.in_memory_session_service import InMemorySessionService\n",
        "from google.adk.sessions.session import Session\n",
        "from google.genai import types\n",
        "from typing import AsyncGenerator\n",
        "from google.adk.agents.llm_agent import LlmAgent\n",
        "\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "async def main():\n",
        "\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = LlmAgent(name=\"AgentA\", model = GEMINI_2_FLASH,\n",
        "                       instruction = \"You are Agent A. Respond with 'Agent A: Starting...'\")\n",
        "    agent_b = LlmAgent(name=\"AgentB\", model = GEMINI_2_FLASH,\n",
        "                       instruction = \"You are Agent B. Respond with 'Agent B: Starting...'\")\n",
        "\n",
        "    # Create the SequentialAgent\n",
        "    sequential_agent = SequentialAgent(name=\"SequentialAgent\", children=[agent_a, agent_b])\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=sequential_agent,\n",
        "        user_content=types.Content(\n",
        "                parts=[types.Part(text=\"execute\")]\n",
        "            )\n",
        "\n",
        "        )\n",
        "\n",
        "    # Run the SequentialAgent\n",
        "    async for event in sequential_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woeLw3OnA5Xp"
      },
      "source": [
        "### Sequence with LLMAgent and `runner.run_async`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUQiCk6v5XTm",
        "outputId": "b3587fcd-a2fe-465f-df02-aaa53aa6659d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "\n",
            "Event: AgentB: Agent B: Starting...\n",
            "No previous requests to process. Ready for new instructions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "async def main():\n",
        "\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\",\n",
        "        session_id=\"1234\"\n",
        "    )\n",
        "\n",
        "    agent_a = LlmAgent(name=\"AgentA\", model = GEMINI_2_FLASH,\n",
        "                       instruction = \"You are Agent A. Respond with 'Agent A: Starting...'\")\n",
        "    agent_b = LlmAgent(name=\"AgentB\", model = GEMINI_2_FLASH,\n",
        "                       instruction = \"You are Agent B. Respond with 'Agent B: Starting...'\")\n",
        "\n",
        "    # Create the SequentialAgent\n",
        "    sequential_agent = SequentialAgent(name=\"SequentialAgent\", children=[agent_a, agent_b])\n",
        "\n",
        "    # Create the Runner\n",
        "    runner = Runner(agent=sequential_agent, session_service=session_service, app_name=\"test_app\")\n",
        "\n",
        "    # Run the SequentialAgent through the Runner\n",
        "    async for event in runner.run_async(user_id=\"test_user\", session_id=\"1234\", new_message=types.Content(role=\"user\",\n",
        "                parts=[types.Part(text=\"execute\",\n",
        "                                  )]\n",
        "            )):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDROM_P-Alpt"
      },
      "source": [
        "### Sequence with LLMAgent and simple runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPHz5Cyv_z8F",
        "outputId": "5aed0837-587e-4770-f28c-61f7c30e5518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Agent A: Starting...\n",
            "\n",
            "Agent Response:  Agent B: Starting...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents.sequential_agent import SequentialAgent\n",
        "from google.adk.agents.llm_agent import LlmAgent\n",
        "from google.genai import types\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "\n",
        "\n",
        "APP_NAME = \"sequential_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"sequential_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "\n",
        "agent_a = LlmAgent(\n",
        "        name=\"AgentA\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are Agent A. Respond with 'Agent A: Starting...'\",\n",
        "\t\toutput_key = \"agent_a\"\n",
        "    )\n",
        "\n",
        "agent_b = LlmAgent(\n",
        "        name=\"AgentB\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are Agent B. Respond with 'Agent B: Starting...'\",\n",
        "    )\n",
        "\n",
        "# Create the SequentialAgent\n",
        "sequential_agent = SequentialAgent(\n",
        "        name=\"SequentialAgent\", children=[agent_a, agent_b]\n",
        "    )\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=sequential_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "  for event in events:\n",
        "      if event.is_final_response():\n",
        "          final_response = event.content.parts[0].text\n",
        "          print(\"Agent Response: \", final_response)\n",
        "\n",
        "call_agent(\"execute\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAtXbF8DLLTv"
      },
      "source": [
        "### SequenceAgent + LLMAgent + States"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfHw7Ut_rnKz",
        "outputId": "afa2771d-a697-4ada-ce46-b0c4fbc25c07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent Alpha1234: Starting...\n",
            "\n",
            "Event: AgentB: Agent B: Starting...\n",
            " and Agent A output: Agent Alpha1234: Starting...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "class SequentialAgent(BaseAgent):\n",
        "    \"\"\"A shell agent that run its child agents in sequence.\"\"\"\n",
        "\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        for child in self.children:\n",
        "            async for event in child.run_async(ctx):\n",
        "                yield event\n",
        "                if event.actions and event.actions.state_delta:\n",
        "                    ctx.session.state.update(event.actions.state_delta)\n",
        "\n",
        "async def main():\n",
        "\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = LlmAgent(name=\"AgentA\", model = GEMINI_2_FLASH,\n",
        "                       instruction = \"You are Agent A. Respond with 'Agent Alpha1234: Starting...'\",\n",
        "                       output_key=\"agent_a_output\")\n",
        "    agent_b = LlmAgent(name=\"AgentB\", model = GEMINI_2_FLASH,\n",
        "                       instruction = \"You are Agent B. Respond with 'Agent B: Starting...'\",\n",
        "                       )\n",
        "\n",
        "    # Create the SequentialAgent\n",
        "    sequential_agent = SequentialAgent(name=\"SequentialAgent\", children=[agent_a, agent_b])\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=sequential_agent,\n",
        "        user_content=types.Content(\n",
        "                parts=[types.Part(text=\"execute\")]\n",
        "            )\n",
        "\n",
        "        )\n",
        "\n",
        "    # Run the SequentialAgent\n",
        "    async for event in sequential_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            if event.author == \"AgentB\":\n",
        "                agent_a_output = ctx.session.state.get(\"agent_a_output\")\n",
        "                print(f\"Event: {event.author}: {event.content.parts[0].text} and Agent A output: {agent_a_output}\")\n",
        "            else:\n",
        "                print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVS5bHjqO0sD"
      },
      "source": [
        "## Multi-Agent - LoopAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrsvPMqDAu79"
      },
      "source": [
        "### LoopAgent with Simple Runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUYDIdsxAuvu",
        "outputId": "ce93c375-a026-44bb-d48f-99e434763d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Response:  Agent A: Starting...\n",
            "\n",
            "Agent Response:  Agent B: Starting...\n",
            "\n",
            "Agent Response:  Agent A: Acknowledged. Agent B is starting. I am ready for instructions.\n",
            "\n",
            "Agent Response:  Agent B: Ready for instructions as well.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents.loop_agent import LoopAgent\n",
        "from google.adk.agents.llm_agent import LlmAgent\n",
        "from google.genai import types\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "\n",
        "\n",
        "\n",
        "APP_NAME = \"loop_app\"\n",
        "USER_ID = \"12345\"\n",
        "SESSION_ID = \"123344\"\n",
        "AGENT_NAME = \"loop_agent\"\n",
        "GEMINI_2_FLASH = \"gemini-2.0-flash-001\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "agent_a = LlmAgent(\n",
        "        name=\"AgentA\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are Agent A. Respond with 'Agent A: Starting...'\",\n",
        "    )\n",
        "\n",
        "agent_b = LlmAgent(\n",
        "        name=\"AgentB\",\n",
        "        model=GEMINI_2_FLASH,\n",
        "        instruction=\"You are Agent B. Respond with 'Agent B: Starting...'\",\n",
        "    )\n",
        "\n",
        "# Create the LoopAgent\n",
        "loop_agent = LoopAgent(\n",
        "    name=\"LoopAgent\", children=[agent_a, agent_b], max_iterations=2\n",
        ")\n",
        "\n",
        "\n",
        "# Session and Runner\n",
        "session_service = InMemorySessionService()\n",
        "session = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "runner = Runner(agent=loop_agent, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "\n",
        "# Agent Interaction\n",
        "def call_agent(query):\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "  events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
        "\n",
        "  for event in events:\n",
        "      if event.is_final_response():\n",
        "          final_response = event.content.parts[0].text\n",
        "          print(\"Agent Response: \", final_response)\n",
        "\n",
        "call_agent(\"execute\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxOrUPQkO34F"
      },
      "source": [
        "### LoopAgent with InnvocationContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCaz2BtMO4Nu",
        "outputId": "9277dab0-af9e-4f5d-c038-e698e90995fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n",
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents.loop_agent import LoopAgent\n",
        "\n",
        "\n",
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the LoopAgent\n",
        "    loop_agent = LoopAgent(name=\"LoopAgent\", children=[agent_a, agent_b], max_iterations=2)\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=loop_agent,\n",
        "        user_content=types.Content(\n",
        "            parts=[types.Part(text=\"execute\")]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Run the LoopAgent\n",
        "    async for event in loop_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcBLz-NZPHvi"
      },
      "source": [
        "### Stop Condition with EventActions=Escalation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J42_PTYgPJQN",
        "outputId": "ed275827-c79a-498f-f5bc-cfcb37987704"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents.loop_agent import LoopAgent\n",
        "from google.adk.events.event_actions import EventActions\n",
        "\n",
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Finishing...\")]\n",
        "            ),\n",
        "            actions=EventActions(escalate=True),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the LoopAgent\n",
        "    loop_agent = LoopAgent(name=\"LoopAgent\", children=[agent_a, agent_b], max_iterations=10)\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=loop_agent,\n",
        "        user_content=types.Content(\n",
        "            parts=[types.Part(text=\"execute\")]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Run the LoopAgent\n",
        "    async for event in loop_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21opVatuSEME"
      },
      "source": [
        "### Escalation with Condition - defined in state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOCPRwuZSEfD",
        "outputId": "c86d0a6d-4e7b-447f-ad79-ec957d34e8d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n",
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n",
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Example condition: Escalate if session state has a key 'escalate_agent_b'\n",
        "        escalate = ctx.session.state.get('escalate_agent_b', False)\n",
        "\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Finishing...\")]\n",
        "            ),\n",
        "            actions=EventActions(escalate=escalate),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the LoopAgent\n",
        "    loop_agent = LoopAgent(name=\"LoopAgent\", children=[agent_a, agent_b], max_iterations=3)\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=loop_agent,\n",
        "        user_content=types.Content(\n",
        "            parts=[types.Part(text=\"execute\")]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Example: Set a condition to escalate\n",
        "    ctx.session.state['escalate_agent_b'] = False\n",
        "\n",
        "    # Run the LoopAgent\n",
        "    async for event in loop_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu7fJ5QIUECc"
      },
      "source": [
        "## ParallelAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3iVDqUdUGdx"
      },
      "source": [
        "### Simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgqJ9rzxUGo_",
        "outputId": "6c2db6e1-53a4-4de6-ef4a-f45c741b3929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Finishing...\n"
          ]
        }
      ],
      "source": [
        "from google.adk.agents.parallel_agent import ParallelAgent\n",
        "\n",
        "\n",
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        await asyncio.sleep(1)\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        await asyncio.sleep(2)\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the ParallelAgent\n",
        "    parallel_agent = ParallelAgent(name=\"ParallelAgent\", children=[agent_a, agent_b])\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=parallel_agent,\n",
        "        user_content=types.Content(\n",
        "            parts=[types.Part(text=\"execute\")]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Run the ParallelAgent\n",
        "    async for event in parallel_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2iStmHFUcRj"
      },
      "source": [
        "### Shared State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1EZZfy4UdpK",
        "outputId": "f85a2373-550b-4210-b8dc-addea1823e20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: AgentA: Agent A: Starting...\n",
            "Event: AgentB: Agent B: Starting...\n",
            "Event: AgentA: Agent A: Finishing...\n",
            "Event: AgentB: Agent B: Finishing... Received: Data from Agent A\n"
          ]
        }
      ],
      "source": [
        "class AgentA(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        await asyncio.sleep(1)\n",
        "        ctx.session.state['shared_data'] = \"Data from Agent A\"\n",
        "        yield Event(\n",
        "            author=\"AgentA\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent A: Finishing...\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "class AgentB(BaseAgent):\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=\"Agent B: Starting...\")]\n",
        "            ),\n",
        "        )\n",
        "        await asyncio.sleep(2)\n",
        "        shared_data = ctx.session.state.get('shared_data', \"No data from Agent A\")\n",
        "        yield Event(\n",
        "            author=\"AgentB\",\n",
        "            content=types.Content(\n",
        "                parts=[types.Part(text=f\"Agent B: Finishing... Received: {shared_data}\")]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a session\n",
        "    session_service = InMemorySessionService()\n",
        "    session: Session = session_service.create_session(\n",
        "        app_name=\"test_app\", user_id=\"test_user\"\n",
        "    )\n",
        "\n",
        "    agent_a = AgentA(name=\"AgentA\")\n",
        "    agent_b = AgentB(name=\"AgentB\")\n",
        "\n",
        "    # Create the ParallelAgent\n",
        "    parallel_agent = ParallelAgent(name=\"ParallelAgent\", children=[agent_a, agent_b])\n",
        "\n",
        "    # Create InvocationContext\n",
        "    ctx = InvocationContext(\n",
        "        invocation_id=new_invocation_context_id(),\n",
        "        session_service=session_service,\n",
        "        session=session,\n",
        "        agent=parallel_agent,\n",
        "        user_content=types.Content(\n",
        "            parts=[types.Part(text=\"execute\")]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Run the ParallelAgent\n",
        "    async for event in parallel_agent.run_async(ctx):\n",
        "        if event.content and event.content.parts:\n",
        "            print(f\"Event: {event.author}: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vfkz0jeeobT"
      },
      "source": [
        "## CustomAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb4lAAD4eo77",
        "outputId": "f72e46b3-fcb4-464d-9fda-0e0eee87e240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Event: Hello from Custom Agent!\n"
          ]
        }
      ],
      "source": [
        "class CustomAgent(BaseAgent):\n",
        "    \"\"\"A custom agent that generates a simple text message.\"\"\"\n",
        "\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        yield Event(\n",
        "            invocation_id=ctx.invocation_id,\n",
        "            author=self.name,\n",
        "            content=types.Content(\n",
        "                parts=[types.Part.from_text(text=\"Hello from Custom Agent!\")],\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Create a custom agent instance\n",
        "    custom_agent = CustomAgent(name=\"CustomAgent\", description=\"A custom agent\")\n",
        "\n",
        "    # Create a session service\n",
        "    session_service = InMemorySessionService()\n",
        "\n",
        "    # Create a session\n",
        "    session = session_service.create_session(\n",
        "        app_name=\"demo_app\", user_id=\"test_user\", session_id=\"test_session\"\n",
        "    )\n",
        "\n",
        "    # Create a runner instance\n",
        "    runner = Runner(\n",
        "        app_name=\"demo_app\",\n",
        "        agent=custom_agent,\n",
        "        session_service=session_service,\n",
        "    )\n",
        "\n",
        "    # Run the agent\n",
        "    async for event in runner.run_async(\n",
        "        user_id=\"test_user\",\n",
        "        session_id=\"test_session\",\n",
        "        new_message=types.Content(\n",
        "            parts=[types.Part.from_text(text=\"Hi, how are you?\")]\n",
        "        ),\n",
        "    ):\n",
        "        print(f\"Event: {event.content.parts[0].text}\")\n",
        "\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8pMhGmoBbrz"
      },
      "source": [
        "### StoryFlowAgent Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riDvVDPlBb-y"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from uuid import uuid4\n",
        "from google.adk.agents.base_agent import BaseAgent\n",
        "from google.adk.agents.sequential_agent import SequentialAgent\n",
        "from google.adk.agents.invocation_context import InvocationContext, new_invocation_context_id\n",
        "from google.adk.events import Event\n",
        "from typing_extensions import override\n",
        "from google.adk.sessions.in_memory_session_service import InMemorySessionService\n",
        "from google.adk.sessions.session import Session\n",
        "from google.genai import types\n",
        "from typing import AsyncGenerator\n",
        "from google.adk.agents.llm_agent import LlmAgent\n",
        "\n",
        "class StoryFlowAgent(BaseAgent):\n",
        "    \"\"\"\n",
        "    Custom agent for a story generation and refinement workflow.\n",
        "\n",
        "    This agent orchestrates a sequence of LLM agents to generate a story,\n",
        "    critique it, revise it, check grammar and tone, and potentially\n",
        "    regenerate the story if the tone is negative.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        story_generator: LlmAgent,\n",
        "        critic: LlmAgent,\n",
        "        reviser: LlmAgent,\n",
        "        grammar_check: LlmAgent,\n",
        "        tone_check: LlmAgent,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the StoryFlowAgent.\n",
        "\n",
        "        Args:\n",
        "            name: The name of the agent.\n",
        "            story_generator: An LlmAgent to generate the initial story.\n",
        "            critic: An LlmAgent to critique the story.\n",
        "            reviser: An LlmAgent to revise the story based on criticism.\n",
        "            grammar_check: An LlmAgent to check the grammar.\n",
        "            tone_check: An LlmAgent to analyze the tone.\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)  # Initialize the base class\n",
        "        self.story_generator = story_generator\n",
        "        self.critic = critic\n",
        "        self.reviser = reviser\n",
        "        self.grammar_check = grammar_check\n",
        "        self.tone_check = tone_check\n",
        "\n",
        "        # Create a LoopAgent for the critic-reviser cycle\n",
        "        self.loop_agent = LoopAgent(\n",
        "            name=\"CriticReviserLoop\", children=[self.critic, self.reviser], max_iterations=2\n",
        "        )\n",
        "\n",
        "        # Create a SequentialAgent for post-processing\n",
        "        self.sequential_agent = SequentialAgent(\n",
        "            name=\"PostProcessing\", children=[self.grammar_check, self.tone_check]\n",
        "        )\n",
        "\n",
        "        # Important: Define the child agents for framework management.\n",
        "        self.children = [\n",
        "            self.story_generator,\n",
        "            self.loop_agent,\n",
        "            self.sequential_agent,\n",
        "        ]\n",
        "\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        \"\"\"\n",
        "        Implements the custom orchestration logic for the story workflow.\n",
        "\n",
        "        This method is an asynchronous generator that yields Event objects.\n",
        "        It controls the execution of the child agents and implements the\n",
        "        conditional regeneration based on tone.\n",
        "\n",
        "        Args:\n",
        "            ctx: The InvocationContext, providing access to session state.\n",
        "\n",
        "        Yields:\n",
        "            Event: Events generated by the child agents.\n",
        "        \"\"\"\n",
        "        # 1. Initial Story Generation\n",
        "        async for event in self.story_generator.run_async(ctx):\n",
        "            yield event\n",
        "            if event.is_final_response():\n",
        "                story_content = event.content.model_dump_json()\n",
        "                ctx.session.state[\"current_story\"] = story_content\n",
        "\n",
        "        # 2. Critic-Reviser Loop\n",
        "        async for event in self.loop_agent.run_async(ctx):\n",
        "            yield event\n",
        "            # Assuming Reviser updates 'current_story' in session state.\n",
        "\n",
        "        # 3. Sequential Post-Processing (Grammar and Tone Check)\n",
        "        async for event in self.sequential_agent.run_async(ctx):\n",
        "            yield event\n",
        "\n",
        "        # 4. Tone-Based Conditional Logic\n",
        "        tone_check_result = ctx.session.state.get(\"tone_check_result\")\n",
        "        if tone_check_result == \"negative\":\n",
        "            # Regenerate story if the tone is negative\n",
        "            ctx.session.state[\"current_story\"] = \"\"  # Clear previous story.\n",
        "            async for event in self.story_generator.run_async(ctx):\n",
        "                yield event\n",
        "        else:\n",
        "            # If tone is not negative, do nothing (keep current story).\n",
        "            pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOgwhoIOCQkG"
      },
      "outputs": [],
      "source": [
        "# --- Define the individual LLM agents ---\n",
        "\n",
        "story_generator = LlmAgent(\n",
        "    name=\"StoryGenerator\",\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    instruction=\"\"\"You are a story writer. Write a short story about a cat,\n",
        "    based on the topic provided in session state with key 'topic'\"\"\",\n",
        "    input_schema=None,  # No specific input schema required\n",
        "    output_key=\"current_story\",  # Key for storing output in session state\n",
        ")\n",
        "\n",
        "critic = LlmAgent(\n",
        "    name=\"Critic\",\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    instruction=\"\"\"You are a story critic.  Review the story provided in\n",
        "    session state with key 'current_story'. Provide constructive criticism\n",
        "    on how to improve it.\"\"\",\n",
        "    input_schema=None,\n",
        "    output_key=\"criticism\",  # Key for storing criticism in session state\n",
        ")\n",
        "\n",
        "reviser = LlmAgent(\n",
        "    name=\"Reviser\",\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    instruction=\"\"\"You are a story reviser.  Revise the story provided in\n",
        "    session state with key 'current_story', based on the criticism in\n",
        "    session state with key 'criticism'. Output the revised story.\"\"\",\n",
        "    input_schema=None,\n",
        "    output_key=\"current_story\",  # Overwrites the original story\n",
        ")\n",
        "\n",
        "grammar_check = LlmAgent(\n",
        "    name=\"GrammarCheck\",\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    instruction=\"\"\"You are a grammar checker. Check the grammar of the story\n",
        "    provided in session state with key 'current_story'. Output any suggested\n",
        "    corrections. If no corrections, output 'Grammar is good!'.\"\"\",\n",
        "    input_schema=None,\n",
        "    output_key=\"grammar_suggestions\",\n",
        ")\n",
        "\n",
        "tone_check = LlmAgent(\n",
        "    name=\"ToneCheck\",\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    instruction=\"\"\"You are a tone analyzer. Analyze the tone of the story\n",
        "    provided in session state with key 'current_story'. Output 'positive' if\n",
        "    the tone is positive, 'negative' if the tone is negative, or 'neutral'\n",
        "    otherwise.\"\"\",\n",
        "    input_schema=None,\n",
        "    output_key=\"tone_check_result\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "_G6S5rWNCjbV",
        "outputId": "4454d4f8-b6ab-4960-c966-19e8de821f6a"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "\"StoryFlowAgent\" object has no field \"story_generator\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-83db4c0e374a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- Create the custom agent instance ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m story_flow_agent = StoryFlowAgent(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"StoryFlowAgent\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstory_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstory_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcritic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-2115f2f1b1aa>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, story_generator, critic, reviser, grammar_check, tone_check)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \"\"\"\n\u001b[1;32m     43\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Initialize the base class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstory_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstory_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreviser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'extra'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'allow'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_fields__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                 \u001b[0;31m# TODO - matching error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\"{self.__class__.__name__}\" object has no field \"{name}\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'extra'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'allow'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_fields__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_extra\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_extra\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \"StoryFlowAgent\" object has no field \"story_generator\""
          ]
        }
      ],
      "source": [
        "# --- Create the custom agent instance ---\n",
        "story_flow_agent = StoryFlowAgent(\n",
        "    name=\"StoryFlowAgent\",\n",
        "    story_generator=story_generator,\n",
        "    critic=critic,\n",
        "    reviser=reviser,\n",
        "    grammar_check=grammar_check,\n",
        "    tone_check=tone_check,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "sB_JNjIDEb8A",
        "outputId": "65cc7065-d78c-40a3-ddb9-29b292a585e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Setting up Story Workflow ---\n",
            "Initialized StoryFlowAgent with children: ['StoryGenerator', 'CriticReviserLoop', 'PostProcessing']\n",
            "Session created (ID: story_session_175121c4-3ca8-4b5e-9cfa-a6c459c39837) with initial topic: 'a shy squirrel making a friend'\n",
            "\n",
            "--- Starting Agent Run ---\n",
            "\n",
            "--- [StoryFlowAgent] Starting Workflow ---\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'InvocationContext' object has no attribute 'state'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-f88fab8961c0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;31m# --- Run the main function ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-f88fab8961c0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;31m# 3. Run the workflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Starting Agent Run ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     async for event in runner.run_async(\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0muser_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUSER_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0msession_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSESSION_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/adk/runners.py\u001b[0m in \u001b[0;36mrun_async\u001b[0;34m(self, user_id, session_id, new_message, streaming, save_input_blobs_as_artifacts, support_cfc)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0minvocation_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_agent_to_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minvocation_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvocation_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_service\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/adk/agents/base_agent.py\u001b[0m in \u001b[0;36mrun_async\u001b[0;34m(self, parent_context)\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_async_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-f88fab8961c0>\u001b[0m in \u001b[0;36m_run_async_impl\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[1;32m     98\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- [{self.name}] Starting Workflow ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0minitial_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'topic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a brave mouse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"--- [{self.name}] Topic from state: '{initial_topic}' ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    889\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                         \u001b[0;31m# this is the current error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{type(self).__name__!r} object has no attribute {item!r}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'InvocationContext' object has no attribute 'state'"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import os\n",
        "import uuid\n",
        "from typing import Any, AsyncGenerator, Optional\n",
        "\n",
        "# --- ADK Imports ---\n",
        "from google.adk.agents.base_agent import BaseAgent\n",
        "from google.adk.agents.llm_agent import LlmAgent\n",
        "from google.adk.agents.loop_agent import LoopAgent\n",
        "from google.adk.agents.sequential_agent import SequentialAgent\n",
        "from google.adk.agents.invocation_context import InvocationContext, new_invocation_context_id\n",
        "from google.adk.events import Event\n",
        "from google.adk.sessions import InMemorySessionService, Session, State\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types\n",
        "# *** Import PrivateAttr ***\n",
        "from pydantic import Field, PrivateAttr\n",
        "from typing_extensions import override\n",
        "\n",
        "# --- Assume environment setup (like PROJECT_ID, LOCATION) is done ---\n",
        "# print(\"Ensure GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_LOCATION are set.\")\n",
        "# print(\"Ensure GOOGLE_GENAI_USE_VERTEXAI='TRUE' is set for Vertex AI.\")\n",
        "# os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"your-project-id\"\n",
        "# os.environ[\"GOOGLE_CLOUD_LOCATION\"] = \"us-central1\"\n",
        "# os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\"\n",
        "\n",
        "# --- Constants ---\n",
        "MODEL = \"gemini-1.5-flash-latest\" # Use a recent flash model\n",
        "APP_NAME = \"story_workflow_app\"\n",
        "USER_ID = \"story_user_1\"\n",
        "SESSION_ID = f\"story_session_{uuid.uuid4()}\"\n",
        "\n",
        "# --- Custom Agent Definition ---\n",
        "class StoryFlowAgent(BaseAgent):\n",
        "    \"\"\"\n",
        "    Custom agent orchestrating a story generation/refinement workflow.\n",
        "    Demonstrates controlling child agents (LlmAgent, LoopAgent, SequentialAgent)\n",
        "    from within the parent's _run_async_impl.\n",
        "    Uses PrivateAttr for internally managed composite agents.\n",
        "    \"\"\"\n",
        "    # --- Declare child agents as Pydantic fields ---\n",
        "    story_generator: LlmAgent\n",
        "    critic: LlmAgent\n",
        "    reviser: LlmAgent\n",
        "    grammar_check: LlmAgent\n",
        "    tone_check: LlmAgent\n",
        "\n",
        "    # --- Use PrivateAttr for internal composite agents ---\n",
        "    # These are initialized in model_post_init and ignored by Pydantic validation.\n",
        "    # Use leading underscore convention for private attributes.\n",
        "    _loop_agent: LoopAgent = PrivateAttr()\n",
        "    _sequential_agent: SequentialAgent = PrivateAttr()\n",
        "\n",
        "    # --- Remove custom __init__ ---\n",
        "\n",
        "    def model_post_init(self, __context: Any) -> None:\n",
        "        \"\"\"\n",
        "        Runs after Pydantic initializes declared fields.\n",
        "        Used here to construct composite agents and set up children.\n",
        "        \"\"\"\n",
        "        super().model_post_init(__context) # Call parent's post_init\n",
        "\n",
        "        # Create the LoopAgent using the initialized fields\n",
        "        self._loop_agent = LoopAgent(\n",
        "            name=\"CriticReviserLoop\",\n",
        "            children=[self.critic, self.reviser], # Use fields initialized by Pydantic\n",
        "            max_iterations=1 # Keep loop short for demo\n",
        "        )\n",
        "\n",
        "        # Create the SequentialAgent\n",
        "        self._sequential_agent = SequentialAgent(\n",
        "            name=\"PostProcessing\",\n",
        "            children=[self.grammar_check, self.tone_check] # Use fields initialized by Pydantic\n",
        "        )\n",
        "\n",
        "        # Define the children list for the framework (important!)\n",
        "        # BaseAgent's model_post_init handles setting parent links for these children.\n",
        "        self.children = [\n",
        "            self.story_generator,\n",
        "            self._loop_agent,       # Use the private attribute\n",
        "            self._sequential_agent, # Use the private attribute\n",
        "        ]\n",
        "        # Ensure parent links are set for the privately managed agents too\n",
        "        self._loop_agent.parent_agent = self\n",
        "        self._sequential_agent.parent_agent = self\n",
        "\n",
        "        print(f\"Initialized {self.name} with children: {[c.name for c in self.children]}\")\n",
        "\n",
        "\n",
        "    @override\n",
        "    async def _run_async_impl(\n",
        "        self, ctx: InvocationContext\n",
        "    ) -> AsyncGenerator[Event, None]:\n",
        "        \"\"\"\n",
        "        Implements the custom orchestration logic for the story workflow.\n",
        "        Controls execution of child agents and implements conditional logic.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- [{self.name}] Starting Workflow ---\")\n",
        "        initial_topic = ctx.state.get('topic', 'a brave mouse')\n",
        "        print(f\"--- [{self.name}] Topic from state: '{initial_topic}' ---\")\n",
        "\n",
        "        # 1. Initial Story Generation\n",
        "        print(f\"\\n--- [{self.name}] Running StoryGenerator ---\")\n",
        "        async for event in self.story_generator.run_async(ctx):\n",
        "            yield event\n",
        "        # story_generator's output_key=\"current_story\" handles saving\n",
        "\n",
        "        print(f\"\\n--- [{self.name}] Current story (after generate): '{ctx.state.get('current_story', 'NOT SET')[:50]}...' ---\")\n",
        "\n",
        "        # 2. Critic-Reviser Loop (use the private attribute)\n",
        "        print(f\"\\n--- [{self.name}] Running CriticReviserLoop ---\")\n",
        "        async for event in self._loop_agent.run_async(ctx):\n",
        "            yield event\n",
        "        # Reviser's output_key=\"current_story\" handles saving the revision\n",
        "\n",
        "        print(f\"\\n--- [{self.name}] Current story (after loop): '{ctx.state.get('current_story', 'NOT SET')[:50]}...' ---\")\n",
        "        print(f\"--- [{self.name}] Criticism was: '{ctx.state.get('criticism', 'NOT SET')[:50]}...' ---\")\n",
        "\n",
        "\n",
        "        # 3. Sequential Post-Processing (use the private attribute)\n",
        "        print(f\"\\n--- [{self.name}] Running PostProcessing (Grammar/Tone) ---\")\n",
        "        async for event in self._sequential_agent.run_async(ctx):\n",
        "            yield event\n",
        "        # grammar_check and tone_check save their results via output_key\n",
        "\n",
        "        print(f\"\\n--- [{self.name}] Grammar suggestions: '{ctx.state.get('grammar_suggestions', 'NOT SET')[:50]}...' ---\")\n",
        "\n",
        "        # 4. Tone-Based Conditional Logic\n",
        "        print(f\"\\n--- [{self.name}] Checking Tone ---\")\n",
        "        # Access state via ctx.state\n",
        "        tone_check_result = ctx.state.get(\"tone_check_result\")\n",
        "        print(f\"--- [{self.name}] Tone result from state: {tone_check_result} ---\")\n",
        "\n",
        "        # Check if the tone string contains 'negative' (case-insensitive)\n",
        "        is_negative = isinstance(tone_check_result, str) and \"negative\" in tone_check_result.lower()\n",
        "\n",
        "        if is_negative:\n",
        "            print(f\"--- [{self.name}] Tone is negative. Regenerating story... ---\")\n",
        "            # Regenerate story\n",
        "            # Story_generator will overwrite state['current_story'] via output_key\n",
        "            async for event in self.story_generator.run_async(ctx):\n",
        "                yield event\n",
        "            print(f\"\\n--- [{self.name}] Regenerated story: '{ctx.state.get('current_story', 'NOT SET')[:50]}...' ---\")\n",
        "\n",
        "        else:\n",
        "            print(f\"--- [{self.name}] Tone is acceptable ('{tone_check_result}'). Workflow complete. ---\")\n",
        "\n",
        "        print(f\"\\n--- [{self.name}] Finishing Workflow ---\")\n",
        "\n",
        "\n",
        "# --- Define the individual LLM agents (as before) ---\n",
        "\n",
        "story_generator = LlmAgent(\n",
        "    name=\"StoryGenerator\", model=MODEL,\n",
        "    instruction=\"You are a story writer. Write a short story (2-3 sentences) about the topic provided in session state key 'topic'.\",\n",
        "    output_key=\"current_story\",\n",
        ")\n",
        "\n",
        "critic = LlmAgent(\n",
        "    name=\"Critic\", model=MODEL,\n",
        "    instruction=\"You are a story critic. Review the story in session state key 'current_story'. Provide one sentence of constructive criticism.\",\n",
        "    output_key=\"criticism\",\n",
        ")\n",
        "\n",
        "reviser = LlmAgent(\n",
        "    name=\"Reviser\", model=MODEL,\n",
        "    instruction=\"You are a story reviser. Revise the story in session state key 'current_story' based on the criticism in session state key 'criticism'. Output only the revised story (2-3 sentences).\",\n",
        "    output_key=\"current_story\", # Overwrites original/previous revision\n",
        ")\n",
        "\n",
        "grammar_check = LlmAgent(\n",
        "    name=\"GrammarCheck\", model=MODEL,\n",
        "    instruction=\"You are a grammar checker. Check the grammar of the story in session state key 'current_story'. Output only suggested corrections (max 1 sentence). If no corrections, output 'Grammar looks good!'.\",\n",
        "    output_key=\"grammar_suggestions\",\n",
        ")\n",
        "\n",
        "tone_check = LlmAgent(\n",
        "    name=\"ToneCheck\", model=MODEL,\n",
        "    instruction=\"You are a tone analyzer. Analyze the tone of the story in session state key 'current_story'. Output ONLY one word: 'positive', 'negative', or 'neutral'.\",\n",
        "    output_key=\"tone_check_result\",\n",
        ")\n",
        "\n",
        "# --- Main execution block ---\n",
        "async def main():\n",
        "    print(\"--- Setting up Story Workflow ---\")\n",
        "    # 1. Create the custom agent instance\n",
        "    # Pydantic automatically calls model_post_init after initializing the fields\n",
        "    story_flow_agent = StoryFlowAgent(\n",
        "        name=\"StoryFlowAgent\",\n",
        "        story_generator=story_generator,\n",
        "        critic=critic,\n",
        "        reviser=reviser,\n",
        "        grammar_check=grammar_check,\n",
        "        tone_check=tone_check,\n",
        "    )\n",
        "\n",
        "    # 2. Setup Session and Runner\n",
        "    session_service = InMemorySessionService()\n",
        "    initial_topic = \"a shy squirrel making a friend\" # Changed topic slightly\n",
        "    session = session_service.create_session(\n",
        "        app_name=APP_NAME,\n",
        "        user_id=USER_ID,\n",
        "        session_id=SESSION_ID,\n",
        "        state={\"topic\": initial_topic} # Provide initial topic\n",
        "    )\n",
        "    runner = Runner(\n",
        "        agent=story_flow_agent, # Use the custom orchestrator agent\n",
        "        app_name=APP_NAME,\n",
        "        session_service=session_service\n",
        "    )\n",
        "    print(f\"Session created (ID: {session.id}) with initial topic: '{initial_topic}'\")\n",
        "\n",
        "    # 3. Run the workflow\n",
        "    print(\"\\n--- Starting Agent Run ---\")\n",
        "    async for event in runner.run_async(\n",
        "        user_id=USER_ID,\n",
        "        session_id=SESSION_ID,\n",
        "        new_message=types.Content(parts=[types.Part(text=\"Start story workflow.\")]) # Dummy message to trigger run\n",
        "    ):\n",
        "        # Print events from child agents as they happen\n",
        "        if event.content and event.content.parts:\n",
        "             # Simple check to avoid printing empty/non-text parts if any\n",
        "             text_content = event.content.parts[0].text if event.content.parts[0].text else \"[Non-text content]\"\n",
        "             print(f\"  Event from [{event.author}]: {text_content.strip()}\")\n",
        "        elif event.actions.state_delta and event.author != story_flow_agent.name: # Avoid printing empty state deltas from parent\n",
        "             # Filter empty deltas unless you specifically want to see them\n",
        "             if event.actions.state_delta:\n",
        "                  print(f\"  State Delta from [{event.author}]: {event.actions.state_delta}\")\n",
        "\n",
        "\n",
        "    # 4. Show final state\n",
        "    print(\"\\n--- Workflow Finished. Final Session State: ---\")\n",
        "    final_session = session_service.get_session(APP_NAME, USER_ID, SESSION_ID)\n",
        "    final_story = final_session.state.get('current_story', 'N/A')\n",
        "    final_tone = final_session.state.get('tone_check_result', 'N/A')\n",
        "    final_grammar = final_session.state.get('grammar_suggestions', 'N/A')\n",
        "    print(f\"  Topic: {final_session.state.get('topic', 'N/A')}\")\n",
        "    print(f\"  Final Story: {final_story}\")\n",
        "    print(f\"  Criticism: {final_session.state.get('criticism', 'N/A')}\")\n",
        "    print(f\"  Grammar Suggs: {final_grammar}\")\n",
        "    print(f\"  Final Tone: {final_tone}\")\n",
        "    # print(f\" Full State: {final_session.state}\") # Optional: print entire state\n",
        "\n",
        "# --- Run the main function ---\n",
        "await main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
